{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dupireNN08082001.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rQdgNr7Lt_rn",
        "0v-MujhxbB9L",
        "7--0wSGKZYhl",
        "VljyG0NV1CID",
        "JB4SRjCP1Wtv",
        "OlfrW4Gn2N4B",
        "gTX3rV8ka02z",
        "vNDkFJlYdi7h",
        "-qgxVPguB_6P",
        "KWdzg9DEv7eH",
        "s192l-r-B6lD",
        "hUMtwNuguMRu",
        "r9EvOdg0kU2P",
        "_eJG7_513hNC",
        "MrvDySFQWK-D",
        "NZBPdHPuB0CX",
        "NB3_-zutBt2y",
        "5yP7BP6avj5q",
        "nl9Gq8s4o_DH",
        "pMVAzDEKXe2Z",
        "s8feoYmarxmj",
        "3n8evyIxsB2W",
        "8ysLo9wxsTOj",
        "_sdUIUb_MwVJ",
        "_RU2VfS9UcAN",
        "LIWdTVy8tf59",
        "z3Hrt5KOVt9S",
        "Pwupw4shWcqI",
        "Z45IZtNnWfek",
        "LtubjBVY3vGh",
        "hwG8y4zzVlLj",
        "3sie49ZXVqJA",
        "r1raybsuVwOU",
        "DabeiqM_V0Cp",
        "3a3HFfj9Oa6O",
        "q-FhHmEUAaIj",
        "dPQe7i1bQys_"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rQdgNr7Lt_rn"
      },
      "source": [
        "# Load modules and external files\n",
        "\n",
        "You need to import four python scripts for implied volatility calibration :\n",
        "- *newton.py*\n",
        "- *BSImplVol.py*\n",
        "- *BS.py*\n",
        "- *Bisect.py*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UbzsYb2nltYw",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.integrate as integrate\n",
        "from scipy import interpolate\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from io import StringIO\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "from mpl_toolkits import mplot3d\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import sklearn as skl\n",
        "from sklearn import preprocessing\n",
        "import importlib\n",
        "import scipy.stats as st\n",
        "import numpy as np\n",
        "import math\n",
        "import scipy.stats as st\n",
        "import matplotlib.ticker as mtick\n",
        "import time\n",
        "from scipy import interpolate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RXQOZxW3tp7-",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5zOz93riuTb0",
        "colab": {}
      },
      "source": [
        "#Load python files to google colaborative environment\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-APUi7EiuTS7",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KySbpTpKuTO3",
        "colab": {}
      },
      "source": [
        "from BS import bsformula\n",
        "from Bisect import bisect\n",
        "from newton import newton\n",
        "from BSImplVol import bsimpvol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ntAmGtpauTLK",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b8kSmrOxuTEw",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gdJ5B21juS5Y",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0v-MujhxbB9L"
      },
      "source": [
        "# Load data with google colab \n",
        "\n",
        "You will find in github repository six days of data.\n",
        "For each day you need to load six csv files :\n",
        "- *underlying.csv* for the stock value.\n",
        "- *locvol.csv* for the local volatility calibrated with tree pricing and tikhonov volatility (see Crépey (2002)).\n",
        "- *dividend.csv* for dividend extracted from put-call parity.\n",
        "- *discount.csv* for zero-coupon curve. \n",
        "- *dataTrain.csv* for prices and/or implied volatility used in training set.\n",
        "- *dataTest.csv* for prices and/or implied volatility used in testing set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNMh06JyZONo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load csv files to get data\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJh4umbkZOK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Read csv files as dataFrames\n",
        "zeroCouponCurve = pd.read_csv(\"discount.csv\",decimal=\".\").apply(pd.to_numeric)\n",
        "dividendCurve = pd.read_csv(\"dividend.csv\",decimal=\".\").apply(pd.to_numeric)\n",
        "trainingData = pd.read_csv(\"dataTrain.csv\",decimal=\".\").apply(pd.to_numeric)\n",
        "testingData = pd.read_csv(\"dataTest.csv\",decimal=\".\").apply(pd.to_numeric)\n",
        "underlyingNative = pd.read_csv(\"underlying.csv\",decimal=\".\").apply(pd.to_numeric)\n",
        "localVolatilityNative = pd.read_csv(\"locvol.csv\",decimal=\".\").apply(pd.to_numeric)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlyxIssgZOIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parseDatFile(fileName):\n",
        "  s = open(fileName).read()\n",
        "  \n",
        "  defPos=s.find(\"[option]\")\n",
        "  finPos=s.find(\"[dividend]\")\n",
        "  df = pd.read_csv(StringIO(s[defPos:finPos].replace(\"\\n\\n\",\";\").replace(\"\\n\",\",\").replace(\";\",\";\\n\")),decimal=\".\", sep=\",\", header=None)\n",
        "  \n",
        "  matC = pd.to_numeric(df[1].str.split(pat=\"= \", expand=True)[1]).round(3)\n",
        "  strikeC = pd.to_numeric(df[3].str.split(pat=\"= \", expand=True)[1]).round()\n",
        "  priceC = pd.to_numeric(df[4].str.replace(\";\",\"\").str.split(pat=\"= \", expand=True)[1])\n",
        "  typeC = pd.to_numeric(df[2].str.split(pat=\"= \", expand=True)[1])\n",
        "  formattedDat = pd.DataFrame([matC, strikeC, priceC, typeC], index = [\"Maturity\", \"Strike\", \"Price\", \"Type\"]).transpose().astype({\"Type\":\"int32\"})\n",
        "  \n",
        "  filteredDat = formattedDat[formattedDat[\"Type\"]==2]\n",
        "  return filteredDat\n",
        "filteredDat = parseDatFile(\"7_8_2001__filterdax.dat\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7--0wSGKZYhl",
        "colab_type": "text"
      },
      "source": [
        "#### From Dat files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzR7VJaRZOFM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parseModelParamDatFile(fileName):\n",
        "    s = open(fileName).read()\n",
        "    \n",
        "    parts = s.split(\"\\n\\n\")\n",
        "    number1 = parts[0]\n",
        "    repo = parts[1]\n",
        "    dates = parts[2]\n",
        "    interestRates = parts[3]\n",
        "    dividendRates = parts[4]\n",
        "    \n",
        "    number2 = parts[5]\n",
        "    number3 = parts[6]\n",
        "    \n",
        "    n = parts[7]\n",
        "    sigmaRef = parts[8]\n",
        "    h = parts[9]\n",
        "    sigmaMax = parts[10]\n",
        "    sigmaMin = parts[11]\n",
        "    \n",
        "    number4 = parts[12]\n",
        "    underlying = parts[13]\n",
        "    \n",
        "    def splitRow(row):\n",
        "        return np.array(row.split(\"\\t\")).astype(np.float)\n",
        "    \n",
        "    tree = (\"\\n\".join(parts[14:])).split(\"\\n\")\n",
        "    tree.remove(\"\")\n",
        "    formattedTree = np.reshape(np.array(list(map(splitRow, tree))), (-1,3))\n",
        "    \n",
        "    \n",
        "    return pd.DataFrame(formattedTree, columns = [\"date\", \"stock(%)\", \"vol\"])\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAHor6QrZODA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parseImpliedVolDatFile(fileName):\n",
        "    s = open(fileName).read()\n",
        "    \n",
        "    parts = s.split(\"\\n\\n\")\n",
        "    \n",
        "    def splitRow(row):\n",
        "        return np.array(row.split(\"\\t\")).astype(np.float)\n",
        "    \n",
        "    testGrid = (\"\\n\".join(parts)).split(\"\\n\")\n",
        "    testGrid.remove(\"\")\n",
        "    formattedTestGrid = np.reshape(np.array(list(map(splitRow, testGrid))), (-1,4))\n",
        "    \n",
        "    return pd.DataFrame(formattedTestGrid, columns=[\"Strike\",\"Maturity\",\"Implied vol.\",\"Option price\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MitIrOkaZOAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parseCalibrOutDatFile(fileName):\n",
        "    s = open(fileName).read()\n",
        "    \n",
        "    parts = s.split(\"\\n\")\n",
        "    \n",
        "    def splitRow(row):\n",
        "        return np.array(row.split(\"\\t\"))\n",
        "    def filterRow(row):\n",
        "        return len(row)==10\n",
        "    def formatRow(row):\n",
        "        return row.astype(np.float)\n",
        "    \n",
        "    #tree = (\"\\n\".join(parts)).split(\"\\n\")\n",
        "    #tree.remove(\"\")\n",
        "    filteredTrainingData = list(filter(filterRow , \n",
        "                                       list(map(splitRow, parts))))\n",
        "    formattedTrainingData = np.array(list(map(formatRow, filteredTrainingData)))\n",
        "    \n",
        "    colNames = [\"Active\", \"Option\\ntype\", \"Maturity\", \"Strike\", \"Moneyness\", \n",
        "                \"Option\\nprice\", \"Implied\\nvol.\", \"Calibrated\\nvol.\",\"Market vol. -\\nCalibrated vol.\"]\n",
        "    dfTrainingData = pd.DataFrame(formattedTrainingData[:,:-1], columns = colNames)\n",
        "    dfTrainingData[\"Active\"] = dfTrainingData[\"Active\"].astype(np.int) \n",
        "    dfTrainingData[\"Option\\ntype\"] = dfTrainingData[\"Option\\ntype\"].astype(np.int) \n",
        "    return dfTrainingData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg5chayfZN9u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def parseDatFiles(fileName):\n",
        "    s = open(fileName).read()\n",
        "    \n",
        "    posUnderlying = s.find(\"[underlying]\")\n",
        "    posZeroCoupon = s.find(\"[zero_coupon]\")\n",
        "    posOption = s.find(\"[option]\")\n",
        "    posDividend = s.find(\"[dividend]\")\n",
        "    \n",
        "    underlyingString = s[posUnderlying:posZeroCoupon]\n",
        "    zeroCouponString = s[posZeroCoupon:posOption]\n",
        "    optionString = s[posOption:posDividend]\n",
        "    dividendString = s[posDividend:-2] \n",
        "    \n",
        "    def extractData(subStr, tag):\n",
        "        parts = subStr.replace(tag + \"\\n\", \"\").split(\"\\n\\n\")\n",
        "        try :\n",
        "            parts.remove(\"\")\n",
        "        except ValueError:\n",
        "            #Not found, we continue\n",
        "            pass\n",
        "        \n",
        "        def parseRow(row):\n",
        "            return (int(row.split(\" = \")[1]) if (row.split(\" = \")[0] == \"type\") else float(row.split(\" = \")[1]))\n",
        "        \n",
        "        def splitRow(row):\n",
        "            table = np.array(row.split(\"\\n\"))\n",
        "            parseTable = np.array(list(map(parseRow, table)))\n",
        "            return np.reshape(parseTable, (-1))\n",
        "        \n",
        "        return np.array(list(map(splitRow, parts)))\n",
        "    \n",
        "    \n",
        "    underlying = pd.DataFrame(extractData(underlyingString, \"[underlying]\"), \n",
        "                              columns=[\"S\",\"Repo\"])\n",
        "    zeroCoupon = pd.DataFrame(extractData(zeroCouponString, \"[zero_coupon] \"), \n",
        "                              columns=[\"Maturity\",\"Price\"])\n",
        "    option = pd.DataFrame(extractData(optionString, \"[option] \"), \n",
        "                          columns=[\"Maturity\",\"Type\", \"Price\", \"Strike\"])\n",
        "    option[\"Type\"] = option[\"Type\"].astype(np.int) \n",
        "    dividend = pd.DataFrame(extractData(dividendString, \"[dividend] \"), \n",
        "                            columns=[\"Maturity\",\"Amount\"])\n",
        "    return underlying, zeroCoupon, dividend, option"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjkbaSnJZN7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "localVolatilityNative = parseModelParamDatFile(\"./esx/8_8_2001__filterdax.dat.modelparam.dat\")\n",
        "#localVolatilityNative = parseModelParamDatFile(\"./esx/30_11_1999__filteresx.dat.modelparam.dat\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPiBn9QFZN4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testingData = parseImpliedVolDatFile(\"./esx/8_8_2001__filterdax.dat.impliedvol.dat\")\n",
        "#testingData = parseImpliedVolDatFile(\"./esx/30_11_1999__filteresx.dat.impliedvol.dat\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pw2SpL7NZN2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainingData = parseCalibrOutDatFile(\"./esx/8_8_2001__filterdax.dat.calibr.out.dat\")\n",
        "#trainingData = parseCalibrOutDatFile(\"./esx/30_11_1999__filteresx.dat.calibr.out.dat\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZA0CSToBZN0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "underlyingNative, zeroCouponCurve, dividendCurve, filteredDat = parseDatFiles(\"./esx/8_8_2001__filterdax.dat\")\n",
        "#underlyingNative, zeroCouponCurve, dividendCurve, filteredDat = parseDatFiles(\"./esx/30_11_1999__filteresx.dat\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2OK4Zb0ZuMM",
        "colab_type": "text"
      },
      "source": [
        "#### Cleaning datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T52qwu4QZNxV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Format dividend curve as a Pandas series\n",
        "dividendDf = dividendCurve.set_index('Maturity').sort_index()\n",
        "dividendDf.loc[1.0] = 0.0\n",
        "dividendDf.sort_index(inplace=True)\n",
        "dividendDf.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_xa3RQjZNuw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Format zero coupon curve as a Pandas series\n",
        "rateCurveDf = zeroCouponCurve.set_index('Maturity').sort_index()\n",
        "# keep only rates expriring before 1 year\n",
        "rateCurveDf = rateCurveDf.loc[rateCurveDf.index <= 1.01]\n",
        "rateCurveDf.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iarO-REZNsN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "localVolatilityNative.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x906G5WuZNpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Format local volatility\n",
        "localVolatility = localVolatilityNative.dropna()\n",
        "localVolatility[\"Strike\"] = localVolatility[\"stock(%)\"] * underlyingNative[\"S\"].values\n",
        "localVolatility[\"date\"] = localVolatility[\"date\"].round(decimals=3)\n",
        "renameDict = {\"date\": \"Maturity\", \n",
        "              \"vol\" : \"LocalVolatility\", \n",
        "              \"stock(%)\" : \"StrikePercentage\"}\n",
        "localVolatility = localVolatility.rename(columns=renameDict).set_index([\"Strike\", \"Maturity\"])\n",
        "localVolatility.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R7vMJSnZNnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "localVolatility.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EJGDyjFZNko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "underlyingNative.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7ClwgvOZNiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testingData.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwDYUSbFZ7NG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Treatment for training data\n",
        "filteredTestingData = testingData[(testingData[\"Implied vol.\"] > 0) * (testingData[\"Option price\"] > 0)]\n",
        "filteredTestingData[\"Maturity\"] = filteredTestingData[\"Maturity\"].round(decimals=3)\n",
        "renameDict = {\"Implied vol.\": \"ImpliedVol\", \n",
        "              \"Option price\" : \"Price\", \n",
        "              \"Implied delta\" : \"ImpliedDelta\", \n",
        "              \"Implied gamma\" : \"ImpliedGamma\",\n",
        "              \"Implied theta\" : \"ImpliedTheta\",\n",
        "              \"Local delta\" : \"LocalDelta\",\n",
        "              \"Local gamma\" : \"LocalGamma\"}\n",
        "formattedTestingData = filteredTestingData.rename(columns=renameDict).set_index([\"Strike\", \"Maturity\"])[\"ImpliedVol\"]\n",
        "formattedTestingData.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo9dohxWZ7Kj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainingData.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-NRe2jKZ7IW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Treatment for testing data\n",
        "filteredTrainingData = trainingData[(trainingData[\"Calibrated\\nvol.\"] > 0) * (trainingData[\"Option\\nprice\"] > 0) * (trainingData[\"Option\\ntype\"] == 2)]\n",
        "filteredTrainingData[\"Maturity\"] = filteredTrainingData[\"Maturity\"].round(decimals=3)\n",
        "renameDict = {\"Option\\ntype\" : \"OptionType\", \n",
        "              \"Option\\nprice\" : \"Price\", \n",
        "              \"Calibrated\\nvol.\" : \"ImpliedVol\",#\"LocalImpliedVol\", \n",
        "              \"Implied\\nvol.\" : \"LocalImpliedVol\"}#\"ImpliedVol\"}\n",
        "formattedTrainingData = filteredTrainingData.drop([\"Active\", \"Market vol. -\\nCalibrated vol.\"],axis=1).rename(columns=renameDict).set_index([\"Strike\",\"Maturity\"])\n",
        "formattedTrainingData.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7Gm9kVJZ7Fs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "formattedTrainingData.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZgRcQ3nZ7BD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvw46AEDZ6-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbzmkhNwZ670",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uix1XUaU8sa6"
      },
      "source": [
        "# Formatting data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VljyG0NV1CID"
      },
      "source": [
        "### Boostsrapping Rate Curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wRo7rm_oS-Fm"
      },
      "source": [
        "\n",
        "- For bootstrapping short rate $r$ and dividend rate $q$, we assume piecewise constant short rate for risk free rate and dividend i.e. \n",
        "$\\exp{(-\\int_{0}^{T} r_t d_t)} = \\exp{(-\\sum_{i} r_i h)}$ and $\\exp{(\\int_{0}^{T} q_t d_t)} = \\exp{(\\sum_{i} q_i h)}$.\n",
        "- $\\forall i \\in \\{0,..,N\\}$ with $ t_0 = 0$ and $t_N = T$, we have that $\\frac{\\log{B(0,t_{i+1})} - \\log{B(0,t_i)}}{h} = r_i$ with $B(0,T_i)$ the price of a bond expiring at time $t_i$. \n",
        "- For dividend, we just to substitute $B(0,T_i)$ with with spot action price plus dividend cash flow received until time $T_i$ i.e. $S_{t_0} + \\sum\\limits_i Div_{t_i}$.\n",
        "- Then we linearly interpolate $r$ and $q$.\n",
        "-  Linear interpolation is also used for integrals $\\int_{0}^{T} q_t d_t$ and $\\int_{0}^{T} r_t d_t$ in order to obtain discount factor or dividend factor. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5qXMpZsnmeKv",
        "colab": {}
      },
      "source": [
        "#Compute the integral and return the linear interpolation function \n",
        "def interpIntegral(curve):\n",
        "    #curve is piece-wise constant\n",
        "    timeDelta = curve.index.to_series().diff().fillna(0)\n",
        "    timeStep = np.linspace(0,0.99,100)\n",
        "    integralStepWise = (curve * timeDelta).cumsum()\n",
        "    integralStepWise.loc[0] = 0.0\n",
        "    integralStepWise.sort_index(inplace=True)\n",
        "    integralSpline = interpolate.interp1d(integralStepWise.index,\n",
        "                                          integralStepWise, \n",
        "                                          fill_value= 'extrapolate', \n",
        "                                          kind ='linear')\n",
        "    return pd.Series(integralSpline(timeStep),index=timeStep), integralSpline\n",
        "\n",
        "def bootstrapZeroCoupon(curvePrice, name):\n",
        "    #Bootstrap short rate curve\n",
        "    def computeShortRate(curve) :\n",
        "      shortRateList = [] \n",
        "      for i in range(curve.size):\n",
        "        if i == 0 :\n",
        "          shortRateList.append(-(np.log(curve.iloc[i]))/(curve.index[i]))\n",
        "        else : \n",
        "          shortRateList.append(-(np.log(curve.iloc[i])-np.log(curve.iloc[i-1]))/(curve.index[i]-curve.index[i-1]))\n",
        "      return pd.Series(shortRateList,index = curve.index)\n",
        "    #For t=0 we take the first available point to ensure right continuity\n",
        "    riskFreeCurve = computeShortRate(curvePrice)\n",
        "    riskFreeCurve.loc[0.00] = riskFreeCurve.iloc[0]\n",
        "    riskFreeCurve = riskFreeCurve.sort_index()\n",
        "\n",
        "    #Bootstrap yield curve\n",
        "    def zeroYield(x):\n",
        "      if(float(x.name) < 1):\n",
        "        return (1/x - 1)/float(x.name)\n",
        "      else:\n",
        "        return (x**(-1/float(x.name)) - 1)\n",
        "    yieldCurve = curvePrice.apply(zeroYield, axis = 1)\n",
        "    yieldCurve.loc[0.00] = yieldCurve.iloc[0]\n",
        "    yieldCurve = yieldCurve.sort_index()\n",
        "\n",
        "    plt.plot(riskFreeCurve, label = \"Short rate\")\n",
        "\n",
        "    #Interpolate short rate curve and yield curve\n",
        "    timeStep = np.linspace(0,0.99,100)\n",
        "    riskCurvespline = interpolate.interp1d(riskFreeCurve.index,\n",
        "                                           riskFreeCurve,#riskFreeCurve[name],\n",
        "                                           fill_value= 'extrapolate',\n",
        "                                           kind ='next')\n",
        "    interpolatedCurve = pd.Series(riskCurvespline(timeStep),index=timeStep)\n",
        "    plt.plot(interpolatedCurve, label=\"Interpolated short rate\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(yieldCurve, label = \"Yield curve\")\n",
        "    yieldCurvespline = interpolate.interp1d(yieldCurve.index,\n",
        "                                            yieldCurve['Price'],\n",
        "                                            fill_value= 'extrapolate',\n",
        "                                            kind ='next')\n",
        "    interpolatedCurve = pd.Series(yieldCurvespline(timeStep),index=timeStep)\n",
        "    plt.plot(interpolatedCurve, label = \"Interpolated Yield curve\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    #Integrate short rate\n",
        "    interpolatedIntegral, riskFreeIntegral = interpIntegral(riskFreeCurve)\n",
        "    plt.plot(interpolatedIntegral)\n",
        "    plt.show()\n",
        "\n",
        "    return riskFreeCurve, riskCurvespline, yieldCurve, yieldCurvespline, interpolatedIntegral, riskFreeIntegral\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o-ahnsiAH9xh",
        "colab": {}
      },
      "source": [
        "riskFreeCurve, riskCurvespline, yieldCurve, yieldCurvespline, interpolatedIntegral, riskFreeIntegral = bootstrapZeroCoupon(rateCurveDf, \"Short rate\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tkDf8dPPOW3Y",
        "colab": {}
      },
      "source": [
        "riskFreeCurve"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yr0_fpL6OWdf",
        "colab": {}
      },
      "source": [
        "interpolatedIntegral"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JB4SRjCP1Wtv"
      },
      "source": [
        "### Boostraping dividend curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aawmDPX0EBMD",
        "colab": {}
      },
      "source": [
        "def bootstrapDividend(curvePrice, underlying, name):\n",
        "    #Compute cumulative sum of dividend plus spot price\n",
        "    priceEvolution = underlying['S'].iloc[0] - curvePrice['Amount'].cumsum()\n",
        "    priceEvolution.loc[0] = underlying['S'].iloc[0]\n",
        "    priceEvolution.sort_index(inplace=True)\n",
        "\n",
        "    #Bootstrap short rate for dividend\n",
        "    def computeShortRate(curve) :\n",
        "      shortRateList = [] \n",
        "      for i in range(curve.size):\n",
        "        if i == 0 :\n",
        "          shortRateList.append(-(np.log(curve.iloc[i+1])-np.log(curve.iloc[i]))/(curve.index[i+1]-curve.index[i]))\n",
        "        else : \n",
        "          shortRateList.append(-(np.log(curve.iloc[i])-np.log(curve.iloc[i-1]))/(curve.index[i]-curve.index[i-1]))\n",
        "      return pd.Series(shortRateList,index = curve.index).dropna()\n",
        "    logReturnDividendDf = computeShortRate(priceEvolution)\n",
        "\n",
        "    #Dividend yield curve\n",
        "    def divYield(x):\n",
        "      return ((priceEvolution[x]/priceEvolution.iloc[0])**(1/float(x)) - 1) #np.log(priceEvolution[x]/priceEvolution.iloc[0])/x\n",
        "    dividendYield = logReturnDividendDf.index.to_series().tail(-1).apply(divYield)\n",
        "    dividendYield.loc[0.00] = dividendYield.iloc[0]\n",
        "    dividendYield = dividendYield.sort_index()\n",
        "\n",
        "    plt.plot(logReturnDividendDf, label = \"Short rate\")\n",
        "\n",
        "    #Interpolate short rate curve and yield curve\n",
        "    timeStep = np.linspace(0,0.99,100)\n",
        "    logReturnDividendSpline = interpolate.interp1d(logReturnDividendDf.index,\n",
        "                                                   logReturnDividendDf,#logReturnDividendDf[name],\n",
        "                                                   fill_value= 'extrapolate',\n",
        "                                                   kind ='next')\n",
        "    interpolatedCurve = pd.Series(logReturnDividendSpline(timeStep),index=timeStep)\n",
        "    plt.plot(interpolatedCurve, label=\"Interpolated short rate\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(dividendYield, label = \"Yield curve\")\n",
        "    yieldCurvespline = interpolate.interp1d(dividendYield.index,\n",
        "                                            dividendYield.values,\n",
        "                                            fill_value= 'extrapolate',\n",
        "                                            kind ='next')\n",
        "    interpolatedCurve = pd.Series(yieldCurvespline(timeStep),index=timeStep)\n",
        "    plt.plot(interpolatedCurve, label = \"Interpolated Yield curve\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    #Integrate short rate\n",
        "    interpolatedIntegral, logReturnDividendIntegral = interpIntegral(logReturnDividendDf)#logReturnDividendDf[name])\n",
        "    plt.plot(interpolatedIntegral)\n",
        "    plt.show()\n",
        "\n",
        "    return logReturnDividendDf, logReturnDividendSpline, dividendYield, yieldCurvespline, interpolatedIntegral, logReturnDividendIntegral"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mgQ9b3rpbCBl",
        "colab": {}
      },
      "source": [
        "spreadDividend, divSpline, yieldDividend, divYieldSpline, interpolatedIntegral, divSpreadIntegral  = bootstrapDividend(dividendDf, underlyingNative, \"Spread\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KJonVnKvOmfN",
        "colab": {}
      },
      "source": [
        "spreadDividend"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-50wFPo7OmWP",
        "colab": {}
      },
      "source": [
        "interpolatedIntegral"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ucimXKUjVzDE",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OlfrW4Gn2N4B"
      },
      "source": [
        "### Pricing black-scholes price\n",
        "\n",
        "#### Change of variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J5RE5f5UaTUT"
      },
      "source": [
        "- In presence of dividend rate $d$ and risk free rate $r$ Dupire formula is :   $$\\sigma^2(T,K) = 2 \\frac{ \\partial_T P(T,K) + (r-q)\\partial_K P(T,K) + qP(T,K)}{K² \\partial_{K}^2 P(T,K)}$$ \n",
        "with Strike $K$, Maturity $T$, dividend rate $q$ and risk-free rate $r$, $P$ our pricing function. \n",
        "- We apply the following change of variable : $$ w(T,k) = \\exp{(\\int_{0}^{T} q_t dt)} P(T,K)$$ with $K = k \\exp{(\\int_{0}^{T} (r_t - q_t) dt)} $.\n",
        "\n",
        "- Then Dupire equation becomes :  $\\sigma^2(T,K) = 2 \\frac{ \\partial_T w(T,k)}{k² \\partial_{k}^2 w(T,k)}$. \n",
        "- If we learn the mapping $v$ with a neural network then we should obtain quickly by adjoint differentiation $\\partial_T w$ and $\\partial_{k²}^2 w$ and therefore $\\sigma$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8Cbfr7sT1jq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Linear interpolation combined with Nearest neighbor extrapolation\n",
        "def customInterpolator(interpolatedData, newStrike, newMaturity):\n",
        "  strikeRef = np.ravel(interpolatedData.index.get_level_values(\"Strike\").values)\n",
        "  maturityRef = np.ravel(interpolatedData.index.get_level_values(\"Maturity\").values)\n",
        "  xym = np.vstack((strikeRef, maturityRef)).T\n",
        "\n",
        "  fInterpolation = interpolate.griddata(xym,\n",
        "                                        interpolatedData.values.flatten(),\n",
        "                                        (newStrike, newMaturity),\n",
        "                                        method = 'linear',\n",
        "                                        rescale=True)\n",
        "\n",
        "  fExtrapolation =  interpolate.griddata(xym,\n",
        "                                         interpolatedData.values.flatten(),\n",
        "                                         (newStrike, newMaturity),\n",
        "                                         method = 'nearest',\n",
        "                                         rescale=True)\n",
        "    \n",
        "  return np.where(np.isnan(fInterpolation), fExtrapolation, fInterpolation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s5Zydj3k5Y-d",
        "colab": {}
      },
      "source": [
        "import scipy.stats as st\n",
        "#Density derivative\n",
        "def dpdf(x):\n",
        "    v = 1\n",
        "    return -x*np.exp(-x**2/(2.0*v**2))/(v**3*np.sqrt(2.0*np.pi))\n",
        "    \n",
        "\n",
        "def generalizedGreeks(cp, s, k, rf, t, v, div, rfInt, divInt):\n",
        "        \"\"\" Price an option using the Black-Scholes model.\n",
        "        cp: +1/-1 for call/put\n",
        "        s: initial stock price\n",
        "        k: strike price\n",
        "        t: expiration time\n",
        "        v: volatility\n",
        "        rf: risk-free rate at time t\n",
        "        div: dividend at time t\n",
        "        rfInt: deterministic risk-free rate integrated between 0 and t\n",
        "        divInt: deterministic dividend integrated between 0 and t\n",
        "        \"\"\"\n",
        "\n",
        "        d1 = (np.log(s/k)+(rfInt-divInt+0.5*v*v*t))/(v*np.sqrt(t))\n",
        "        d2 = d1 - v*np.sqrt(t)\n",
        "        \n",
        "        Nd1 = st.norm.cdf(cp*d1)\n",
        "        Nd2 = st.norm.cdf(cp*d2)\n",
        "\n",
        "        discountFactor = np.exp(-rfInt)\n",
        "        forwardFactor = np.exp(-divInt)\n",
        "        avgDiv = divInt/t\n",
        "        avgRf = rfInt/t\n",
        "\n",
        "        optprice = (cp*s*forwardFactor*Nd1) - (cp*k*discountFactor*Nd2)\n",
        "\n",
        "        delta = cp*Nd1\n",
        "        vega  = s*np.sqrt(t)*st.norm.pdf(d1)\n",
        "        delta_k = -s*forwardFactor*Nd1/(v*np.sqrt(t)*k) - cp*discountFactor*Nd2 + k*discountFactor*Nd2/(v*np.sqrt(t)*k)\n",
        "        \n",
        "        gamma_k = s*forwardFactor/((v*np.sqrt(t)*k)**2)*(Nd1*v*np.sqrt(t) + cp*dpdf(cp*d1)) - k*discountFactor/((v*np.sqrt(t)*k)**2)*(Nd2*v*np.sqrt(t) + cp*dpdf(cp*d2)) +  2.0*discountFactor*Nd2/(v*np.sqrt(t)*k)  \n",
        "\n",
        "        dd1_dt = (avgRf-avgDiv+0.5*v*v)/(v*np.sqrt(t)) - 0.5*(np.log(s/k)+(rfInt-divInt+0.5*v*v*t))/(v*v*t**(3/2))\n",
        "        dd2_dt = dd1_dt - 0.5*v/np.sqrt(t)\n",
        "        delta_T = avgRf*cp*k*discountFactor*Nd2 - avgDiv*cp*s*forwardFactor*Nd1 + s*forwardFactor*Nd1*dd1_dt- k*discountFactor*Nd2*dd2_dt\n",
        "        \n",
        "        return optprice, delta, vega, delta_k, gamma_k, delta_T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HbOR7gse5Y4x",
        "colab": {}
      },
      "source": [
        "S0 = underlyingNative[\"S\"].values\n",
        "#Change of variable for deterministic discount curve and dividend curve\n",
        "def changeOfVariable(s,t):\n",
        "  def qInterp(m):\n",
        "    return divSpreadIntegral(m).astype(np.float32)\n",
        "  q = qInterp(t)\n",
        "  \n",
        "  def rInterp(m):\n",
        "    return riskFreeIntegral(m).astype(np.float32)\n",
        "  r = rInterp(t)\n",
        "\n",
        "  factorPrice = np.exp( - q )\n",
        "\n",
        "  divSpread = q-r\n",
        "\n",
        "  factorStrike = np.exp( divSpread )\n",
        "  adjustedStrike = np.multiply(s, factorStrike)\n",
        "  return adjustedStrike, factorPrice\n",
        "\n",
        "#Change of variable for constant discount and dividend short rate \n",
        "def changeOfVariable_BS(s,t):\n",
        "  \n",
        "  factorPrice = np.exp( - q*t )\n",
        "\n",
        "  divSpread = (q-r)*t\n",
        "\n",
        "  factorStrike = np.exp( divSpread )\n",
        "  adjustedStrike = np.multiply(s, factorStrike)\n",
        "  return adjustedStrike, factorPrice"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPH_FwsHUznO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generate a proper dataset from implied volatility\n",
        "def generateData(impliedVol,\n",
        "                 S0,\n",
        "                 rIntegralSpline,\n",
        "                 qIntegralSpline,\n",
        "                 rSpline,\n",
        "                 qSpline,\n",
        "                 priceDf = None,\n",
        "                 spotValue = True):\n",
        "  #Get grid coordinates\n",
        "  if priceDf is None :\n",
        "    x_train = impliedVol.index.to_frame()\n",
        "    #Get implied vol by interpolating another grid\n",
        "    x_train[\"ImpliedVol\"] = impliedVol\n",
        "  else :\n",
        "    x_train = pd.MultiIndex.from_arrays([priceDf[\"Strike\"], priceDf[\"Maturity\"]], \n",
        "                                        names=('Strike', 'Maturity')).to_frame()\n",
        "    #Get implied vol by interpolating another grid\n",
        "    x_train[\"ImpliedVol\"] = customInterpolator(impliedVol, \n",
        "                                               x_train[\"Strike\"], \n",
        "                                               x_train[\"Maturity\"])\n",
        "  #Get sensitivities and prices\n",
        "  isPut = True\n",
        "  cp = -1 if isPut else 1\n",
        "  impliedPriceFunction = lambda x : generalizedGreeks(cp, \n",
        "                                                      S0, \n",
        "                                                      x[\"Strike\"] , \n",
        "                                                      rSpline(x[\"Maturity\"]), \n",
        "                                                      x[\"Maturity\"], \n",
        "                                                      x[\"ImpliedVol\"], \n",
        "                                                      qSpline(x[\"Maturity\"]), \n",
        "                                                      rIntegralSpline(x[\"Maturity\"]), \n",
        "                                                      qIntegralSpline(x[\"Maturity\"]))\n",
        "  \n",
        "  res = np.reshape(np.array(list(zip(x_train.apply(impliedPriceFunction,axis=1).values))),\n",
        "                   (x_train.shape[0], 6))  # put greeks\n",
        "  prices = res[:,0] if priceDf is None else priceDf[\"Price\"].values\n",
        "  deltas = res[:,1]\n",
        "  vegas = res[:,2]\n",
        "  delta_ks = res[:,3]\n",
        "  gamma_ks = res[:,4]\n",
        "  delta_Ts = res[:,5]\n",
        "  \n",
        "  #Vega for optional loss weighting\n",
        "  sigmaRef = 0.25\n",
        "  impliedPriceFunction = lambda x : generalizedGreeks(cp, \n",
        "                                                      S0, \n",
        "                                                      x[\"Strike\"] , \n",
        "                                                      rSpline(x[\"Maturity\"]), \n",
        "                                                      x[\"Maturity\"], \n",
        "                                                      sigmaRef, \n",
        "                                                      qSpline(x[\"Maturity\"]), \n",
        "                                                      rIntegralSpline(x[\"Maturity\"]), \n",
        "                                                      qIntegralSpline(x[\"Maturity\"]))\n",
        "  \n",
        "  res1 = np.reshape(np.array(list(zip(x_train.apply(impliedPriceFunction,axis=1).values))),\n",
        "                    (x_train.shape[0], 6))  # put greeks\n",
        "  \n",
        "  #Get adjusted strike for the change of variables\n",
        "  changedVar = changeOfVariable(x_train[\"Strike\"],x_train[\"Maturity\"])\n",
        "  \n",
        "  multiIndex = x_train[\"ImpliedVol\"].index\n",
        "\n",
        "  #Gather all data as a Dataframe \n",
        "  cols = [\"Price\", \"Delta\", \"Vega\", \"Delta Strike\", \"Gamma Strike\", \n",
        "          \"Theta\", \"ChangedStrike\", \"DividendFactor\", \"Strike\", \"Maturity\", \"ImpliedVol\", \"VegaRef\"]\n",
        "\n",
        "  dfData = np.vstack((prices, deltas, vegas, delta_ks, gamma_ks, delta_Ts) + \n",
        "                     changedVar + (x_train[\"Strike\"], x_train[\"Maturity\"], x_train[\"ImpliedVol\"], res1[:,2]))\n",
        "  \n",
        "  df = pd.DataFrame(dfData.T , columns=cols, index = multiIndex)\n",
        "\n",
        "  #Add pricing with spot delivery\n",
        "  if spotValue : \n",
        "    KAvailable = multiIndex.get_level_values(\"Strike\").unique()\n",
        "    TSpot = np.zeros_like(KAvailable)\n",
        "    priceSpot = np.maximum(KAvailable- S0[0],0)\n",
        "    deltaSpot = -np.sign(np.maximum(KAvailable- S0[0],0))\n",
        "    gammaSpot = np.zeros_like(deltaSpot)\n",
        "    vegasSpot = gammaSpot\n",
        "    deltaKSpot = np.sign(np.maximum(KAvailable- S0[0],0))\n",
        "    thetaSpot = 1000000 * deltaKSpot\n",
        "\n",
        "    #Ignore implied vol for T=0\n",
        "    impliedSpot = np.zeros_like(thetaSpot)\n",
        "\n",
        "    changedVarSpot = changeOfVariable(KAvailable,TSpot)\n",
        "    \n",
        "    dfDataSpot = np.vstack((priceSpot, deltaSpot, vegasSpot, deltaKSpot, gammaSpot, thetaSpot) +\n",
        "                          changedVarSpot + (KAvailable, TSpot, impliedSpot, vegasSpot))\n",
        "    indexSpot = pd.MultiIndex.from_arrays([np.array(KAvailable), TSpot], names=('Strike', 'Maturity'))\n",
        "    dfSpot = pd.DataFrame(dfDataSpot.T , columns=cols, index = indexSpot)\n",
        "    df = df.append(dfSpot).sort_index()\n",
        "\n",
        "  #Add forward logmoneyness if we want to calibrate local volatility from implied volatilities\n",
        "  df[\"logMoneyness\"] = np.log(df[\"ChangedStrike\"] / S0[0]) \n",
        "  df[\"impliedTotalVariance\"] = np.square(df[\"ImpliedVol\"]) #*  df[\"Maturity\"]\n",
        "\n",
        "  return df.sort_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKgC3tZXoVXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymMfIqK7oPkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "S0[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Navdr2beN2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "formattedTrainingData.sort_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpFHsuuEovZw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testingData.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3sQVozdp5Y1p",
        "colab": {}
      },
      "source": [
        "testingDataSet = generateData(formattedTestingData,\n",
        "                              S0,\n",
        "                              riskFreeIntegral,\n",
        "                              divSpreadIntegral,\n",
        "                              riskCurvespline,\n",
        "                              divSpline)\n",
        "testingDataSet.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c3Do8EBH5YnO",
        "colab": {}
      },
      "source": [
        "#Checking call put parity\n",
        "maturity = testingData.iloc[-4][\"Maturity\"]\n",
        "strike = testingData.iloc[-4][\"Strike\"]\n",
        "(S0 * np.exp(-divSpreadIntegral(maturity))  - np.exp(-riskFreeIntegral(maturity)) * strike) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aykFRj_xqKY_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jmAkv50Td1dJ",
        "colab": {}
      },
      "source": [
        "#Put call parity\n",
        "testingData.iloc[-4][\"Option price\"] - testingDataSet.loc[(testingData.iloc[-4][\"Strike\"],round(testingData.iloc[-4][\"Maturity\"],3))][\"Price\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h0ag0ChkGEkP",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "715238Hglz63",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Use all prices in dat files\n",
        "trainingDataSet = generateData(formattedTrainingData[\"ImpliedVol\"],\n",
        "                               S0,\n",
        "                               riskFreeIntegral,\n",
        "                               divSpreadIntegral,\n",
        "                               riskCurvespline,\n",
        "                               divSpline,\n",
        "                               priceDf = filteredDat)\n",
        "trainingDataSet.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KNvpFAU1FN78",
        "colab": {}
      },
      "source": [
        "#Use same prices as those for tikhonov calibration\n",
        "trainingDataSet = generateData(formattedTrainingData[\"ImpliedVol\"], \n",
        "                               S0, \n",
        "                               riskFreeIntegral, \n",
        "                               divSpreadIntegral, \n",
        "                               riskCurvespline, \n",
        "                               divSpline,\n",
        "                               priceDf = formattedTrainingData.reset_index())\n",
        "trainingDataSet.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0TDRVwgkFN4c",
        "colab": {}
      },
      "source": [
        "filteredTrainingData.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WCkjHBiSFN2D",
        "colab": {}
      },
      "source": [
        "filteredTrainingData[filteredTrainingData[\"Strike\"]==5900]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xj_4dnWpFNul",
        "colab": {}
      },
      "source": [
        "trainingDataSet[trainingDataSet[\"Strike\"]==5900]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z5u31XfRfe5s",
        "colab": {}
      },
      "source": [
        "filteredTrainingData[filteredTrainingData[\"Strike\"]==4000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1bGr3ZOPfev0",
        "colab": {}
      },
      "source": [
        "trainingDataSet[trainingDataSet[\"Strike\"]==4000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lC01cOOrB6kE",
        "colab": {}
      },
      "source": [
        "localVolatility.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5oBekB4C5YyD",
        "colab": {}
      },
      "source": [
        "testingData.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ttVRKskQEi4g",
        "colab": {}
      },
      "source": [
        "#Get local volatility from Crépey (2002) by nearest neighbour interpolation\n",
        "def interpolatedLocalVolatility(localVol, priceGrid):\n",
        "    \n",
        "    strikePrice = priceGrid.index.get_level_values(\"Strike\").values.flatten()\n",
        "    maturityPrice = priceGrid.index.get_level_values(\"Maturity\").values.flatten()\n",
        "    coordinates = customInterpolator(localVol[\"LocalVolatility\"], strikePrice, maturityPrice)\n",
        " \n",
        "\n",
        "    return pd.Series(coordinates, index = priceGrid.index)\n",
        "\n",
        "trainingDataSet[\"locvol\"] = interpolatedLocalVolatility(localVolatility, trainingDataSet[\"Price\"])\n",
        "testingDataSet[\"locvol\"] = interpolatedLocalVolatility(localVolatility, testingDataSet[\"Price\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wyOafgBuKk1d",
        "colab": {}
      },
      "source": [
        "localVolatility[localVolatility.index.get_level_values(\"Maturity\") <= 0.01]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QUxTjD2zY706",
        "colab": {}
      },
      "source": [
        "dataSet = trainingDataSet #Training set\n",
        "dataSetTest = testingDataSet #Testing set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAEIN8NVowfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Data for gaussian processes\n",
        "tGrid = np.linspace(0, 1, 101)\n",
        "exportedRiskFreeIntegral = riskFreeIntegral(tGrid),\n",
        "exportedDivSpreadIntegral = divSpreadIntegral(tGrid)\n",
        "exportedRRiskCurvespline = riskCurvespline(tGrid),\n",
        "exportedDivSpline = divSpline(tGrid)\n",
        "dfCurve = pd.DataFrame(np.vstack([exportedRiskFreeIntegral, exportedDivSpreadIntegral, exportedRRiskCurvespline, exportedDivSpline]).T,\n",
        "                       columns=[\"riskFreeIntegral\",\"divSpreadIntegral\",\"riskCurvespline\",\"divSpline\"], \n",
        "                       index = tGrid)\n",
        "#Discount and dividend curve\n",
        "dfCurve.to_csv(\"dfCurve.csv\")\n",
        "#Training dataset\n",
        "dataSet.to_csv(\"trainingDataSet.csv\")\n",
        "#Testing dataset\n",
        "dataSetTest.to_csv(\"testingDataSet.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XFKey5xg68I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataSetTest.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8HmA41Xg67P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsQ55bK5g66T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHGmJOe2g63_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v2I8qPzg6nc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RyOtl8iCziXl"
      },
      "source": [
        "# Neural network "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gTX3rV8ka02z"
      },
      "source": [
        "## Scaling methods\n",
        "\n",
        "Use min-max of scaling strike between 0 et 1 for improving stability of neural network training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BLqa33lNSXTB",
        "colab": {}
      },
      "source": [
        "def transformCustomMinMax(df, scaler):\n",
        "  return pd.DataFrame(scaler.transform(df),\n",
        "                      index = df.index, \n",
        "                      columns = df.columns)\n",
        "#Reverse operation min-max scaling\n",
        "def inverseTransformMinMax(df, scaler):\n",
        "  return pd.DataFrame(scaler.inverse_transform(df),\n",
        "                      index = df.index, \n",
        "                      columns = df.columns)\n",
        "#Same thing but for a particular column\n",
        "def inverseTransformColumnMinMax(originalDf, scaler, column):\n",
        "  colIndex = originalDf.columns.get_loc(column.name)\n",
        "  maxCol = scaler.data_max_[colIndex]\n",
        "  minCol = scaler.data_min_[colIndex]\n",
        "  return pd.Series(minCol + (maxCol - minCol) * column, index = column.index).rename(column.name)  \n",
        "#Reverse transform of min-max scaling but for greeks   \n",
        "def inverseTransformColumnGreeksMinMax(originalDf, \n",
        "                                       scaler,\n",
        "                                       columnDerivative,\n",
        "                                       columnFunctionName,\n",
        "                                       columnVariableName,\n",
        "                                       order = 1):\n",
        "  colFunctionIndex = originalDf.columns.get_loc(columnFunctionName)\n",
        "  maxColFunction = scaler.data_max_[colFunctionIndex]\n",
        "  minColFunction = scaler.data_min_[colFunctionIndex]\n",
        "  scaleFunction = (maxColFunction - minColFunction)\n",
        "  \n",
        "  colVariableIndex = originalDf.columns.get_loc(columnVariableName)\n",
        "  maxColVariable = scaler.data_max_[colVariableIndex]\n",
        "  minColVariable = scaler.data_min_[colVariableIndex]\n",
        "  scaleVariable = (maxColVariable - minColVariable) ** order\n",
        "\n",
        "  return pd.Series(scaleFunction * columnDerivative / scaleVariable , \n",
        "                   index = columnDerivative.index).rename(columnDerivative.name) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZFGpjMuoaJOh",
        "colab": {}
      },
      "source": [
        "#Tools functions for min-max scaling\n",
        "def transformCustomId(df, scaler):\n",
        "  return pd.DataFrame(df,\n",
        "                      index = df.index, \n",
        "                      columns = df.columns)\n",
        "def inverseTransformId(df, scaler):\n",
        "  return pd.DataFrame(df,\n",
        "                      index = df.index, \n",
        "                      columns = df.columns)\n",
        "def inverseTransformColumnId(originalDf, scaler, column):\n",
        "  return pd.Series(column, index = column.index).rename(column.name)  \n",
        "\n",
        "def inverseTransformColumnGreeksId(originalDf, scaler, \n",
        "                                 columnDerivative, \n",
        "                                 columnFunctionName, \n",
        "                                 columnVariableName,\n",
        "                                 order = 1):\n",
        "  return pd.Series(columnDerivative , index = columnDerivative.index).rename(columnDerivative.name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XkPnYeFSbWKt",
        "colab": {}
      },
      "source": [
        "activateScaling = False\n",
        "transformCustom = transformCustomMinMax if activateScaling else transformCustomId\n",
        "inverseTransform = inverseTransformMinMax if activateScaling else inverseTransformId\n",
        "inverseTransformColumn = inverseTransformColumnMinMax if activateScaling else inverseTransformColumnId\n",
        "inverseTransformColumnGreeks = inverseTransformColumnGreeksMinMax if activateScaling else inverseTransformColumnGreeksId"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "83ZlEZ7Ys0tA",
        "colab": {}
      },
      "source": [
        "scaler = skl.preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
        "scaler.fit(dataSet)\n",
        "scaledDataSet = transformCustom(dataSet, scaler)\n",
        "scaledDataSetTest = transformCustom(dataSetTest, scaler)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sLjz7YjBw7o5",
        "colab": {}
      },
      "source": [
        "scaledDataSet.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lAUxbEzOnLMh",
        "colab": {}
      },
      "source": [
        "#Search strike for ATM option\n",
        "midS0 = dataSet[dataSet.index.get_level_values(\"Strike\") >= S0[0]].index.get_level_values(\"Strike\").min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aVDBRx3BbcFW"
      },
      "source": [
        "## Plot functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6McrN8zK6WL2",
        "colab": {}
      },
      "source": [
        "#Plot loss for each epoch \n",
        "def plotEpochLoss(lossSerie):\n",
        "  fig = plt.figure(figsize=(20,10))\n",
        "  ax = fig.gca()\n",
        "  \n",
        "  ax.plot(lossSerie , \"-\", color=\"black\")\n",
        "  ax.set_xlabel(\"Epoch number\", fontsize=18, labelpad=20)\n",
        "  ax.set_ylabel(\"Logarithmic Loss\", fontsize=18, labelpad=20)\n",
        "  ax.set_title(\"Training Loss evolution\", fontsize=24)\n",
        "  ax.tick_params(labelsize=16)\n",
        "  ax.set_facecolor('white')\n",
        "  plt.show()\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DTXqhgHOSP7r",
        "colab": {}
      },
      "source": [
        "KMin = 0.7 * S0[0]\n",
        "KMax = 1.3 * S0[0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kDz4FSbcprg7",
        "colab": {}
      },
      "source": [
        "#Plot a surface as a superposition of curves\n",
        "def plotMultipleCurve(data,\n",
        "                      Title = 'True Price Surface',\n",
        "                      yMin = KMin,\n",
        "                      yMax = KMax,\n",
        "                      zAsPercent = False):\n",
        "  \n",
        "\n",
        "  dataCurve = data[(data.index.get_level_values(\"Strike\") <= yMax) * (data.index.get_level_values(\"Strike\") >= yMin)]\n",
        "\n",
        "  fig = plt.figure(figsize=(20,10))\n",
        "  ax = fig.gca()\n",
        "\n",
        "  for t in np.linspace(0,0.8,9) :\n",
        "    k = dataCurve[dataCurve.index.get_level_values(\"Maturity\") >= t].index.get_level_values(\"Maturity\").unique().min()\n",
        "    curveK = dataCurve[dataCurve.index.get_level_values(\"Maturity\")==k]\n",
        "    dataSerie = pd.Series(curveK.values * (100 if zAsPercent else 1) ,\n",
        "                          index = curveK.index.get_level_values(\"Strike\"))\n",
        "    ax.plot(dataSerie , \"--+\", label=str(k))\n",
        "  ax.legend()  \n",
        "  ax.set_xlabel(data.index.names[0], fontsize=18, labelpad=20)\n",
        "  ax.set_ylabel(data.name, fontsize=18, labelpad=20)\n",
        "  if zAsPercent :\n",
        "    ax.yaxis.set_major_formatter(mtick.PercentFormatter())\n",
        "  ax.set_title(Title, fontsize=24)\n",
        "  ax.tick_params(labelsize=16)\n",
        "  ax.set_facecolor('white')\n",
        "  plt.show()\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NrzucF2AQznx",
        "colab": {}
      },
      "source": [
        "plotMultipleCurve(localVolatility[\"LocalVolatility\"][localVolatility.index.get_level_values(\"Maturity\")>0.01],\n",
        "                  Title = 'Local Volatility Surface',\n",
        "                  yMin=0.7*S0[0],\n",
        "                  yMax=1.4*S0[0], \n",
        "                  zAsPercent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZYhSZTjM2Lqp",
        "colab": {}
      },
      "source": [
        "#Plotting function for surface\n",
        "#xTitle : title for x axis\n",
        "#yTitle : title for y axis\n",
        "#zTitle : title for z axis\n",
        "#Title : plot title\n",
        "#az : azimuth i.e. angle of view for surface\n",
        "#yMin : minimum value for y axis\n",
        "#yMax : maximum value for y axis\n",
        "#zAsPercent : boolean, if true format zaxis as percentage \n",
        "def plotGridCustom(coordinates, zValue,\n",
        "                   xTitle = \"Maturity\",\n",
        "                   yTitle = \"Strike\",\n",
        "                   zTitle = \"Price\",\n",
        "                   Title = 'True Price Surface', \n",
        "                   az=320, \n",
        "                   yMin = KMin,\n",
        "                   yMax = KMax,\n",
        "                   zAsPercent = False):\n",
        "  y = coordinates[:,0]\n",
        "  filteredValue = (y > yMin) & (y < yMax)\n",
        "  x = coordinates[:,1][filteredValue]\n",
        "  y = coordinates[:,0][filteredValue]\n",
        "  z = zValue[filteredValue].flatten()\n",
        "  \n",
        "  fig = plt.figure(figsize=(20,10))\n",
        "  ax = fig.gca(projection='3d')\n",
        "  \n",
        "  ax.set_xlabel(xTitle, fontsize=18, labelpad=20)\n",
        "  ax.set_ylabel(yTitle, fontsize=18, labelpad=20)\n",
        "  ax.set_zlabel(zTitle, fontsize=18, labelpad=10)\n",
        "  \n",
        "  cmap=plt.get_cmap(\"inferno\")\n",
        "  colors=cmap(z * 100 if zAsPercent else z)[np.newaxis, :, :3]\n",
        "  surf = ax.plot_trisurf(x, y,\n",
        "                         z * 100 if zAsPercent else z ,\n",
        "                         linewidth=1.0,\n",
        "                         antialiased=True, \n",
        "                         cmap = cmap,\n",
        "                         color=(0,0,0,0))\n",
        "  scaleEdgeValue = surf.to_rgba(surf.get_array())\n",
        "  surf.set_edgecolors(scaleEdgeValue) \n",
        "  surf.set_alpha(0)\n",
        "\n",
        "  if zAsPercent :\n",
        "    ax.zaxis.set_major_formatter(mtick.PercentFormatter())\n",
        "  ax.view_init(elev=10., azim=az)\n",
        "  ax.set_title(Title, fontsize=24)\n",
        "  ax.set_facecolor('white')\n",
        "\n",
        "  plt.tick_params(labelsize=16)\n",
        "\n",
        "  \n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tGvs9RI0bC3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE34jVfx0a2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cTNEnEsWbBOE",
        "colab": {}
      },
      "source": [
        "#Plotting function from a dataframe\n",
        "def plotSurface(data, \n",
        "                zName, \n",
        "                Title = 'True Price Surface', \n",
        "                az=320,\n",
        "                yMin = KMin,\n",
        "                yMax = KMax,\n",
        "                zAsPercent = False):\n",
        "  plotGridCustom(data.index.to_frame().values, \n",
        "                 data[zName].values,\n",
        "                 xTitle = data.index.names[1],\n",
        "                 yTitle = data.index.names[0],\n",
        "                 zTitle = zName,\n",
        "                 Title = Title, \n",
        "                 az=az, \n",
        "                 yMin = yMin, \n",
        "                 yMax = yMax, \n",
        "                 zAsPercent=zAsPercent)\n",
        "  return\n",
        "\n",
        "#Plotting function from a pandas series\n",
        "def plotSerie(data,\n",
        "              Title = 'True Price Surface',\n",
        "              az=320,\n",
        "              yMin = KMin,\n",
        "              yMax = KMax, \n",
        "              zAsPercent = False):\n",
        "  \n",
        "\n",
        "  plotGridCustom(data.index.to_frame().values, \n",
        "                 data.values,\n",
        "                 xTitle = data.index.names[1],\n",
        "                 yTitle = data.index.names[0],\n",
        "                 zTitle = data.name,\n",
        "                 Title = Title, \n",
        "                 az=az, \n",
        "                 yMin = yMin, \n",
        "                 yMax = yMax, \n",
        "                 zAsPercent = zAsPercent)\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1hknqmZHSKW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plotting function for surface\n",
        "#xTitle : title for x axis\n",
        "#yTitle : title for y axis\n",
        "#zTitle : title for z axis\n",
        "#Title : plot title\n",
        "#az : azimuth i.e. angle of view for surface\n",
        "#yMin : minimum value for y axis\n",
        "#yMax : maximum value for y axis\n",
        "#zAsPercent : boolean, if true format zaxis as percentage \n",
        "def plot2GridCustom(coordinates, zValue,\n",
        "                    coordinates2, zValue2,\n",
        "                    xTitle = \"Maturity\",\n",
        "                    yTitle = \"Strike\",\n",
        "                    zTitle = \"Price\",\n",
        "                    Title = 'True Price Surface', \n",
        "                    az=320, \n",
        "                    yMin = KMin,\n",
        "                    yMax = KMax,\n",
        "                    zAsPercent = False):\n",
        "  y = coordinates[:,0]\n",
        "  filteredValue = (y > yMin) & (y < yMax)\n",
        "  x = coordinates[:,1][filteredValue]\n",
        "  y = coordinates[:,0][filteredValue]\n",
        "  z = zValue[filteredValue].flatten()\n",
        "  \n",
        "  y2 = coordinates2[:,0]\n",
        "  filteredValue2 = (y2 > yMin) & (y2 < yMax)\n",
        "  x2 = coordinates2[:,1][filteredValue2]\n",
        "  y2 = coordinates2[:,0][filteredValue2]\n",
        "  z2 = zValue2[filteredValue2].flatten()\n",
        "  \n",
        "  fig = plt.figure(figsize=(20,10))\n",
        "  ax = fig.gca(projection='3d')\n",
        "  \n",
        "  ax.set_xlabel(xTitle, fontsize=18, labelpad=20)\n",
        "  ax.set_ylabel(yTitle, fontsize=18, labelpad=20)\n",
        "  ax.set_zlabel(zTitle, fontsize=18, labelpad=10)\n",
        "  \n",
        "  cmap=plt.get_cmap(\"inferno\")\n",
        "  colors=cmap(z * 100 if zAsPercent else z)[np.newaxis, :, :3]\n",
        "  ax.scatter(x2, y2, z2, marker='o', color=\"r\", alpha=1, s=40)\n",
        "  ax.scatter(x, y, z, marker='o', color=\"b\", alpha=1, s=40)\n",
        "  #surf = ax.plot_trisurf(x, y,\n",
        "  #                       z * 100 if zAsPercent else z ,\n",
        "  #                       linewidth=1.0,\n",
        "  #                       antialiased=True, \n",
        "  #                       cmap = cmap,\n",
        "  #                       color=(0,0,0,0))\n",
        "  #scaleEdgeValue = surf.to_rgba(surf.get_array())\n",
        "  #surf.set_edgecolors(scaleEdgeValue) \n",
        "  #surf.set_alpha(0)\n",
        "\n",
        "\n",
        "  if zAsPercent :\n",
        "    ax.zaxis.set_major_formatter(mtick.PercentFormatter())\n",
        "  ax.view_init(elev=10., azim=az)\n",
        "  #ax.set_title(Title, fontsize=24)\n",
        "  ax.set_facecolor('white')\n",
        "\n",
        "  plt.tick_params(labelsize=16)\n",
        "\n",
        "  \n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  return\n",
        "\n",
        "#Plotting function from a pandas series\n",
        "def plot2Series(data, \n",
        "                data2,\n",
        "                Title = 'True Price Surface',\n",
        "                az=320,\n",
        "                yMin = KMin,\n",
        "                yMax = KMax, \n",
        "                zAsPercent = False):\n",
        "  \n",
        "\n",
        "  plot2GridCustom(data.index.to_frame().values, \n",
        "                  data.values,\n",
        "                  data2.index.to_frame().values, \n",
        "                  data2.values,\n",
        "                  xTitle = data.index.names[1],\n",
        "                  yTitle = data.index.names[0],\n",
        "                  zTitle = data.name,\n",
        "                  Title = Title, \n",
        "                  az=az, \n",
        "                  yMin = yMin, \n",
        "                  yMax = yMax, \n",
        "                  zAsPercent = zAsPercent)\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Za78IddfhsqS",
        "colab": {}
      },
      "source": [
        "plt.get_cmap(\"plasma\")(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wUKTeMfH2IhD",
        "colab": {}
      },
      "source": [
        "plotSurface(dataSet, \"Price\", Title = 'True Price Surface')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YmQpNeyis0iV",
        "colab": {}
      },
      "source": [
        "inverseTransform(scaledDataSet, scaler).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af5ddhHu6rhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-zkCoxz7rvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convertToLogMoneyness(formerSerie):\n",
        "  maturity = formerSerie.index.get_level_values(\"Maturity\")\n",
        "  logMoneyness = np.log(S0[0] / formerSerie.index.get_level_values(\"Strike\"))\n",
        "  newIndex = pd.MultiIndex.from_arrays([np.array(logMoneyness.values), np.array(maturity.values)], names=('LogMoneyness', 'Maturity'))\n",
        "  if type(formerSerie) == type(pd.Series()) :\n",
        "    return pd.Series(formerSerie.values , index=newIndex)\n",
        "  return pd.DataFrame(formerSerie.values, index = newIndex, columns= formerSerie.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh8qUCd11Uyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "REWz2Qinm4iZ",
        "colab": {}
      },
      "source": [
        "#Plot predicted value, benchmark value, absoluate error and relative error\n",
        "#It also compute RMSE between predValue and refValue\n",
        "#predValue : approximated value \n",
        "#refValue : benchamrk value\n",
        "#quantityName : name for approximated quantity\n",
        "#az : azimuth i.e. angle of view for surface\n",
        "#yMin : minimum value for y axis\n",
        "#yMax : maximum value for y axis\n",
        "def predictionDiagnosis(predValue, \n",
        "                        refValue, \n",
        "                        quantityName, \n",
        "                        az=320,\n",
        "                        yMin = KMin,\n",
        "                        yMax = KMax):\n",
        "  \n",
        "  predValueFiltered = predValue[predValue.index.get_level_values(\"Maturity\") > 0.001]\n",
        "  refValueFiltered = refValue[refValue.index.get_level_values(\"Maturity\") > 0.001]\n",
        "  title = \"Predicted \" + quantityName + \" surface\"\n",
        "  plotSerie(predValueFiltered.rename(quantityName), \n",
        "            Title = title, \n",
        "            az=az,\n",
        "            yMin = yMin,\n",
        "            yMax = yMax)\n",
        "  \n",
        "  title = \"True \" + quantityName + \" surface\"\n",
        "  plotSerie(refValueFiltered.rename(quantityName), \n",
        "            Title = title, \n",
        "            az=az,\n",
        "            yMin = yMin,\n",
        "            yMax = yMax)\n",
        "  \n",
        "  title = quantityName + \" surface error\"\n",
        "  absoluteError = np.abs(predValueFiltered - refValueFiltered) \n",
        "  plotSerie(absoluteError.rename(quantityName + \" Absolute Error\"),\n",
        "            Title = title,\n",
        "            az=az,\n",
        "            yMin = yMin,\n",
        "            yMax = yMax)\n",
        "  \n",
        "  title = quantityName + \" surface error\"\n",
        "  relativeError = np.abs(predValueFiltered - refValueFiltered) / refValueFiltered\n",
        "  plotSerie(relativeError.rename(quantityName + \" Relative Error (%)\"),\n",
        "            Title = title,\n",
        "            az=az,\n",
        "            yMin = yMin,\n",
        "            yMax = yMax, \n",
        "            zAsPercent = True)\n",
        "  \n",
        "  print(\"RMSE : \", np.sqrt(np.mean(np.square(absoluteError))) )\n",
        "  \n",
        "  return\n",
        "\n",
        "#Diagnose Price, theta, gamma and local volatility\n",
        "def modelSummary(price, \n",
        "                 volLocale, \n",
        "                 delta_T, \n",
        "                 gamma_K, \n",
        "                 benchDataset,\n",
        "                 sigma=0.3, \n",
        "                 az=40,\n",
        "                 yMin = KMin,\n",
        "                 yMax = KMax,\n",
        "                 logMoneynessScale = False):\n",
        "  nbArbitrageViolations = ((delta_T<0) + (gamma_K<0)).sum()\n",
        "  print(\"Number of static arbitrage violations : \", nbArbitrageViolations)\n",
        "  if logMoneynessScale : \n",
        "    pricePred = convertToLogMoneyness(price)\n",
        "    volLocalePred = convertToLogMoneyness(volLocale)\n",
        "    delta_TPred = convertToLogMoneyness(delta_T)\n",
        "    gKRefPred = convertToLogMoneyness(gamma_K)\n",
        "    benchDatasetScaled = convertToLogMoneyness(benchDataset)\n",
        "    yMinScaled = np.log(S0[0]/yMax)\n",
        "    yMaxScaled = np.log(S0[0]/yMin)\n",
        "    azimutIncrement = 180\n",
        "  else : \n",
        "    pricePred = price\n",
        "    volLocalePred = volLocale\n",
        "    delta_TPred = delta_T\n",
        "    gKRefPred = gamma_K\n",
        "    benchDatasetScaled = benchDataset\n",
        "    yMinScaled = yMin\n",
        "    yMaxScaled = yMax\n",
        "    azimutIncrement = 0\n",
        "  \n",
        "  priceRef = benchDatasetScaled[\"Price\"]\n",
        "  predictionDiagnosis(pricePred, \n",
        "                      priceRef, \n",
        "                      \"Price\",\n",
        "                      az=320 + azimutIncrement,\n",
        "                      yMin = yMinScaled,\n",
        "                      yMax = yMaxScaled)\n",
        "  \n",
        "  \n",
        "  volLocaleRef = benchDatasetScaled[\"locvol\"]\n",
        "  predictionDiagnosis(volLocalePred, \n",
        "                      volLocaleRef, \n",
        "                      \"Local volatility\",\n",
        "                      az= az + azimutIncrement,\n",
        "                      yMin = yMinScaled,\n",
        "                      yMax = yMaxScaled)\n",
        "  \n",
        "  \n",
        "  dTRef = benchDatasetScaled[\"Theta\"]\n",
        "  predictionDiagnosis(delta_TPred, \n",
        "                      dTRef, \n",
        "                      \"Theta\",\n",
        "                      az=340 + azimutIncrement,\n",
        "                      yMin = yMinScaled,\n",
        "                      yMax = yMaxScaled)\n",
        "  \n",
        "  \n",
        "  gKRef = benchDatasetScaled[\"Gamma Strike\"]\n",
        "  predictionDiagnosis(gKRefPred, \n",
        "                      gKRef, \n",
        "                      \"Gamma Strike\",\n",
        "                      az=340 + azimutIncrement,\n",
        "                      yMin = yMinScaled,\n",
        "                      yMax = yMaxScaled)\n",
        "  return\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jQ8OwgHF1I-n"
      },
      "source": [
        "### Implied volatility function calibration by bissection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aL1OsFOh5h--",
        "colab": {}
      },
      "source": [
        "def bs_price(cp, s, k, rf, t, v, div):\n",
        "        \"\"\" Price an option using the Black-Scholes model.\n",
        "        cp: +1/-1 for call/put\n",
        "        s: initial stock price\n",
        "        k: strike price\n",
        "        t: expiration time\n",
        "        v: volatility\n",
        "        rf: risk-free rate\n",
        "        div: dividend\n",
        "        \"\"\"\n",
        "    \n",
        "        d1 = (np.log(s/k)+(rf-div+0.5*v*v)*t)/(v*np.sqrt(t))\n",
        "        d2 = d1 - v*np.sqrt(t)\n",
        "\n",
        "        optprice = (cp*s*np.exp(-div*t)*st.norm.cdf(cp*d1)) - (cp*k*np.exp(-rf*t)*st.norm.cdf(cp*d2))\n",
        "        \n",
        "        return optprice\n",
        "\n",
        "def bissectionMethod(S_0, r, q, implied_vol0, maturity, Strike, refPrice, epsilon):\n",
        "    calibratedSigma = implied_vol0\n",
        "    #Call black-scholes price function for initial value\n",
        "    priceBS = bs_price(-1 ,S0, Strike, r, maturity, calibratedSigma, q)\n",
        "    sigmaUp = 2.0\n",
        "    sigmaInf = epsilon\n",
        "    lossSerie = []\n",
        "    \n",
        "    priceMax = bs_price(-1 ,S0, Strike, r, maturity, sigmaUp, q)\n",
        "    if priceMax < refPrice:\n",
        "        return priceMax, sigmaUp, pd.Series(lossSerie)\n",
        "    \n",
        "    priceMin = bs_price(-1 ,S0, Strike, r, maturity, sigmaInf, q)\n",
        "    if priceMin > refPrice:\n",
        "        return priceMin, sigmaInf, pd.Series(lossSerie) \n",
        "\n",
        "    #Stop the optimization when the error is less than epsilon\n",
        "    while(abs(priceBS - refPrice) > epsilon):\n",
        "        #Update the upper bound or the lower bound \n",
        "        #by comparing calibrated price and the target price \n",
        "        if priceBS < refPrice : \n",
        "            sigmaInf = calibratedSigma\n",
        "        else :\n",
        "            sigmaUp = calibratedSigma\n",
        "        #Update calibratedSigma\n",
        "        calibratedSigma = (sigmaUp + sigmaInf) / 2\n",
        "        #Update calibrated price\n",
        "        priceBS = bs_price(-1 ,S0, Strike, r, maturity, calibratedSigma, q)\n",
        "        #Record the calibration error for this step\n",
        "        lossSerie.append(abs(priceBS - refPrice)) \n",
        "        \n",
        "    return priceBS, calibratedSigma, pd.Series(lossSerie)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qGu1Iz5I1DoF",
        "colab": {}
      },
      "source": [
        "#Execute calibration of implied volatility from estimated price and benchmark price\n",
        "#Then plot esitmated implied vol, absolute and relative error\n",
        "def plotImpliedVol(priceSurface, \n",
        "                   refImpliedVol, \n",
        "                   rIntegralSpline = None, \n",
        "                   qIntegralSpline = None, \n",
        "                   az=40,\n",
        "                   yMin = KMin,\n",
        "                   yMax = KMax,\n",
        "                   relativeErrorVolMax = 1000,\n",
        "                   logMoneynessScale = False):\n",
        "    return plotImpliedVolConcrete(priceSurface[priceSurface.index.get_level_values(\"Maturity\") > 0.001],\n",
        "                                  refImpliedVol[refImpliedVol.index.get_level_values(\"Maturity\") > 0.001],\n",
        "                                  rIntegralSpline = rIntegralSpline,\n",
        "                                  qIntegralSpline = qIntegralSpline,\n",
        "                                  az=az,\n",
        "                                  yMin = yMin,\n",
        "                                  yMax = yMax,\n",
        "                                  relativeErrorVolMax = relativeErrorVolMax, \n",
        "                                  logMoneynessScale = logMoneynessScale)\n",
        "\n",
        "def plotImpliedVolConcrete(priceSurface,\n",
        "                           refImpliedVol,\n",
        "                           rIntegralSpline = None,\n",
        "                           qIntegralSpline = None,\n",
        "                           az=40,\n",
        "                           yMin = KMin,\n",
        "                           yMax = KMax,\n",
        "                           relativeErrorVolMax = 10,\n",
        "                           logMoneynessScale = False):\n",
        "    priceSurfaceScaled = convertToLogMoneyness(priceSurface) if logMoneynessScale else priceSurface\n",
        "    refImpliedVolScaled = convertToLogMoneyness(refImpliedVol) if logMoneynessScale else refImpliedVol\n",
        "    df = priceSurfaceScaled.index.to_frame()\n",
        "    df[\"Price\"] = priceSurfaceScaled\n",
        "    df[\"Strike\"] = convertToLogMoneyness(priceSurface.index.to_frame()[\"Strike\"]) if logMoneynessScale else priceSurface.index.to_frame()[\"Strike\"]\n",
        "    scaledYMin = np.log(S0[0]/yMax) if logMoneynessScale else yMin\n",
        "    scaledYMax = np.log(S0[0]/yMin) if logMoneynessScale else yMax\n",
        "    azimutIncrement = 180 if logMoneynessScale else 0\n",
        "\n",
        "\n",
        "    epsilon = 1e-9\n",
        "    calibrationFunction = lambda x : bissectionMethod(S0, \n",
        "                                                      rIntegralSpline(x[\"Maturity\"])/x[\"Maturity\"] if (rIntegralSpline is not None) else r, \n",
        "                                                      qIntegralSpline(x[\"Maturity\"])/x[\"Maturity\"] if (qIntegralSpline is not None) else q, \n",
        "                                                      0.2, \n",
        "                                                      x[\"Maturity\"], \n",
        "                                                      x[\"Strike\"], \n",
        "                                                      x[\"Price\"], \n",
        "                                                      epsilon)[1]\n",
        "\n",
        "    impliedVol = df.apply(calibrationFunction, axis = 1).rename(\"Implied Volatility\")\n",
        "    impliedVolError = np.abs(impliedVol-refImpliedVolScaled).rename('Absolute Error')\n",
        "    relativeImpliedVolError = (impliedVolError / refImpliedVolScaled).rename(\"Relative error (%)\")\n",
        "    \n",
        "    plotSerie(impliedVol, \n",
        "              Title = 'Implied volatility surface', \n",
        "              az=az + azimutIncrement,\n",
        "              yMin = scaledYMin,\n",
        "              yMax = scaledYMax)\n",
        "\n",
        "    plotSerie(impliedVolError, \n",
        "              Title = 'Implied volatility error', \n",
        "              az=az + azimutIncrement,\n",
        "              yMin = scaledYMin,\n",
        "              yMax = scaledYMax)\n",
        "    \n",
        "    plotSerie(relativeImpliedVolError.clip(0,relativeErrorVolMax / 100.0), \n",
        "              Title = 'Implied volatility relative error', \n",
        "              az=az + azimutIncrement,\n",
        "              yMin = scaledYMin,\n",
        "              yMax = scaledYMax,\n",
        "              zAsPercent = True)\n",
        "  \n",
        "    print(\"Implied volalitity RMSE : \", np.sqrt(np.mean(np.square(impliedVolError))) )\n",
        "\n",
        "    return impliedVol"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0m2TzPY7TL_m",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jl7xMrJRRiVk",
        "colab": {}
      },
      "source": [
        "plotSerie(localVolatility[\"LocalVolatility\"],\n",
        "          Title = 'Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.7*S0,\n",
        "          yMax=1.4*S0, zAsPercent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ6EELvlQJJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plotSerie(dataSet[\"locvol\"],\n",
        "          Title = 'Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.7*S0,\n",
        "          yMax=1.4*S0, zAsPercent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MhLrX-8Y1DeM",
        "colab": {}
      },
      "source": [
        "dataSet[\"locvol\"].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UpF5ELRnoS73"
      },
      "source": [
        "## Learning Price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Rwa7gLppaEdi",
        "colab": {}
      },
      "source": [
        "#Import tensorflow for 1.x version \n",
        "from keras.layers import Dense, Input\n",
        "from keras import Model\n",
        "import keras.backend as K\n",
        "import keras.activations as Act\n",
        "from functools import partial\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vGkkvShljABd",
        "colab": {}
      },
      "source": [
        "#Deactivate warning messages\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KqtRsQxRZ9bT",
        "colab": {}
      },
      "source": [
        "hyperparameters = {}\n",
        "#penalization coefficient\n",
        "hyperparameters[\"lambdaLocVol\"] = 100\n",
        "hyperparameters[\"lambdaSoft\"] = 100 \n",
        "hyperparameters[\"lambdaGamma\"] = 10000\n",
        "\n",
        "#Derivative soft constraints parameters\n",
        "hyperparameters[\"lowerBoundTheta\"] = 0.01\n",
        "hyperparameters[\"lowerBoundGamma\"] = 0.00001\n",
        "\n",
        "#Local variance parameters\n",
        "hyperparameters[\"DupireVarCap\"] = 10\n",
        "hyperparameters[\"DupireVolLowerBound\"] = 0.05\n",
        "hyperparameters[\"DupireVolUpperBound\"] = 0.40\n",
        "\n",
        "#Learning scheduler coefficient\n",
        "hyperparameters[\"LearningRateStart\"] = 0.1\n",
        "hyperparameters[\"Patience\"] = 100\n",
        "hyperparameters[\"batchSize\"] = 50\n",
        "hyperparameters[\"FinalLearningRate\"] = 1e-6\n",
        "hyperparameters[\"FixedLearningRate\"] = False\n",
        "\n",
        "#Training parameters\n",
        "hyperparameters[\"nbUnits\"] = 200 #number of units for hidden layers\n",
        "hyperparameters[\"maxEpoch\"] = 10000 #maximum number of epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vNDkFJlYdi7h"
      },
      "source": [
        "### Learning scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-4ZrujaHeD0L",
        "colab": {}
      },
      "source": [
        "#Format result from training step\n",
        "def evalAndFormatResult(price, loss, dataSet):\n",
        "\n",
        "    scaledPredPrice = pd.Series(price.flatten(), index = dataSet.index).rename(\"Price\")\n",
        "    predPrice = inverseTransformColumn(dataSet, scaler, scaledPredPrice)\n",
        "    \n",
        "    return predPrice, pd.Series(loss)\n",
        "\n",
        "#Format result from training step when local volatility is computed\n",
        "def evalAndFormatDupireResult(price, volDupire, theta, gamma, dupireVar, loss, dataSet):\n",
        "    predPrice, lossEpoch = evalAndFormatResult(price, loss, dataSet)\n",
        "\n",
        "    predDupire = pd.Series(volDupire.flatten(), index = dataSet.index).rename(\"Dupire\")\n",
        "    \n",
        "    scaledTheta = pd.Series(theta.flatten(), index = dataSet.index).rename(\"Theta\")\n",
        "    predTheta = inverseTransformColumnGreeks(dataSet, scaler, scaledTheta, \n",
        "                                             \"Price\", \"Maturity\")\n",
        "    \n",
        "    scaledGammaK = pd.Series(gamma.flatten(), index = dataSet.index).rename(\"GammaK\")\n",
        "    predGammaK = inverseTransformColumnGreeks(dataSet, scaler, scaledGammaK, \n",
        "                                              \"Price\", \"ChangedStrike\", order = 2)\n",
        "    \n",
        "    return predPrice, predDupire, predTheta, predGammaK, lossEpoch\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B1iXQCMZRIGV",
        "colab": {}
      },
      "source": [
        "#Penalization for pseudo local volatility\n",
        "def intervalRegularization(localVariance, vegaRef, hyperParameters):\n",
        "  lowerVolBound = hyperParameters[\"DupireVolLowerBound\"]\n",
        "  upperVolBound = hyperParameters[\"DupireVolUpperBound\"]\n",
        "  no_nans = tf.clip_by_value(localVariance, 0, hyperParameters[\"DupireVarCap\"])\n",
        "  reg = tf.nn.relu(tf.square(lowerVolBound) - no_nans) + tf.nn.relu(no_nans - tf.square(upperVolBound))\n",
        "  lambdas = hyperParameters[\"lambdaLocVol\"] / tf.reduce_mean(vegaRef)\n",
        "  return lambdas * tf.reduce_mean(tf.boolean_mask(reg, tf.is_finite(reg)))\n",
        "\n",
        "#Add above regularization to the list of penalization\n",
        "def addDupireRegularisation(priceTensor, tensorList, penalizationList, formattingResultFunction, vegaRef, hyperParameters):\n",
        "    updatedPenalizationList = penalizationList + [intervalRegularization(tensorList[-1], vegaRef, hyperParameters)]\n",
        "    return priceTensor, tensorList, updatedPenalizationList, formattingResultFunction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZUe6Ixrf8Nk-",
        "colab": {}
      },
      "source": [
        "#Mini-batch sampling methods for large datasets\n",
        "def selectMiniBatchWithoutReplacement(dataSet, batch_size):\n",
        "    nbObs = dataSet.shape[0]\n",
        "    idx = np.arange(nbObs) \n",
        "    np.random.shuffle(idx) \n",
        "    nbBatches = int(np.ceil(nbObs/batch_size))\n",
        "    xBatchList = []\n",
        "    lastBatchIndex = 0\n",
        "    for i in range(nbBatches):\n",
        "        firstBatchIndex = i*batch_size\n",
        "        lastBatchIndex = (i+1)*batch_size\n",
        "        xBatchList.append(dataSet.iloc[idx[firstBatchIndex:lastBatchIndex],:])\n",
        "    xBatchList.append(dataSet.iloc[idx[lastBatchIndex:],:])\n",
        "    return xBatchList\n",
        "\n",
        "def selectMiniBatchWithReplacement(dataSet, batch_size):\n",
        "    nbObs = dataSet.shape[0] \n",
        "    nbBatches = int(np.ceil(nbObs/batch_size)) + 1\n",
        "    xBatchList = []\n",
        "    lastBatchIndex = 0\n",
        "    for i in range(nbBatches):\n",
        "        idx = np.random.randint(nbObs, size = batch_size)\n",
        "        xBatchList.append(dataSet.iloc[idx,:])\n",
        "    return xBatchList\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y4oKCKdCL2fI",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZXP0y5kveFPg",
        "colab": {}
      },
      "source": [
        "#Train neural network with a decreasing rule for learning rate\n",
        "#NNFactory :  function creating the architecture\n",
        "#dataSet : training data\n",
        "#activateRegularization : boolean, if true add bound penalization to dupire variance\n",
        "#hyperparameters : dictionnary containing various hyperparameters\n",
        "#modelName : name under which tensorflow model is saved\n",
        "def create_train_model(NNFactory, \n",
        "                       dataSet, \n",
        "                       activateRegularization, \n",
        "                       hyperparameters,\n",
        "                       modelName = \"bestModel\"):\n",
        "    hidden_nodes = hyperparameters[\"nbUnits\"] \n",
        "    nbEpoch = hyperparameters[\"maxEpoch\"] \n",
        "    fixedLearningRate = (None if hyperparameters[\"FixedLearningRate\"] else hyperparameters[\"LearningRateStart\"])\n",
        "    patience = hyperparameters[\"Patience\"]\n",
        "    \n",
        "    # Go through num_iters iterations (ignoring mini-batching)\n",
        "    activateLearningDecrease = (~ hyperparameters[\"FixedLearningRate\"])\n",
        "    learningRate = hyperparameters[\"LearningRateStart\"]\n",
        "    learningRateEpoch = 0\n",
        "    finalLearningRate = hyperparameters[\"FinalLearningRate\"]\n",
        "\n",
        "    batch_size = hyperparameters[\"batchSize\"]\n",
        "\n",
        "    start = time.time()\n",
        "    # Reset the graph\n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    # Placeholders for input and output data   \n",
        "    Strike = tf.placeholder(tf.float32,[None,1])\n",
        "    Maturity = tf.placeholder(tf.float32,[None,1])\n",
        "    factorPrice = tf.placeholder(tf.float32,[None,1])\n",
        "    y = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='y')\n",
        "    vegaRef = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='vegaRef')\n",
        "    learningRateTensor = tf.placeholder(tf.float32,[])\n",
        "    \n",
        "    #Get scaling for strike\n",
        "    colStrikeIndex = dataSet.columns.get_loc(\"ChangedStrike\")\n",
        "    maxColFunction = scaler.data_max_[colStrikeIndex]\n",
        "    minColFunction = scaler.data_min_[colStrikeIndex]\n",
        "    scF = (maxColFunction - minColFunction) \n",
        "    scaleTensor = tf.constant(scF, dtype=tf.float32)\n",
        "    strikeMinTensor = tf.constant(minColFunction, dtype=tf.float32)\n",
        "\n",
        "    price_pred_tensor = None\n",
        "    TensorList = None\n",
        "    penalizationList = None \n",
        "    formattingFunction = None\n",
        "    if activateRegularization : #Add pseudo local volatility regularisation\n",
        "        price_pred_tensor, TensorList, penalizationList, formattingFunction = addDupireRegularisation( *NNFactory(hidden_nodes,\n",
        "                                                                                                                  Strike,\n",
        "                                                                                                                  Maturity, \n",
        "                                                                                                                  scaleTensor, \n",
        "                                                                                                                  strikeMinTensor, \n",
        "                                                                                                                  vegaRef, \n",
        "                                                                                                                  hyperparameters) ,\n",
        "                                                                                                      vegaRef, \n",
        "                                                                                                      hyperparameters)\n",
        "    else :\n",
        "        price_pred_tensor, TensorList, penalizationList, formattingFunction = NNFactory(hidden_nodes,\n",
        "                                                                                        Strike, \n",
        "                                                                                        Maturity, \n",
        "                                                                                        scaleTensor, \n",
        "                                                                                        strikeMinTensor, \n",
        "                                                                                        vegaRef, \n",
        "                                                                                        hyperparameters)\n",
        "\n",
        "    price_pred_tensor_sc= tf.multiply( factorPrice, price_pred_tensor)\n",
        "    TensorList[0] = price_pred_tensor_sc\n",
        "    \n",
        "    # Define a loss function\n",
        "    pointwiseError = tf.reduce_mean(tf.abs(price_pred_tensor_sc - y) / vegaRef)\n",
        "    errors = tf.add_n([pointwiseError] + penalizationList) \n",
        "    loss = tf.log(tf.reduce_mean(errors))\n",
        "\n",
        "\n",
        "\n",
        "    # Define a train operation to minimize the loss\n",
        "    lr = learningRate\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRateTensor)\n",
        "    train = optimizer.minimize(loss)\n",
        "\n",
        "    # Initialize variables and run session\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver()\n",
        "    sess = tf.Session()\n",
        "    sess.run(init)\n",
        "    n = dataSet.shape[0]\n",
        "    scaledInput = transformCustomMinMax(dataSet, scaler)\n",
        "\n",
        "    \n",
        "    maturity = dataSet[\"Maturity\"].values.reshape(n,1)\n",
        "    loss_serie = []\n",
        "\n",
        "    def createFeedDict(batch):\n",
        "        batchSize = batch.shape[0]\n",
        "        feedDict = {Strike : scaledInput[\"ChangedStrike\"].loc[batch.index].values.reshape(batchSize,1),\n",
        "                    Maturity : batch[\"Maturity\"].values.reshape(batchSize,1), \n",
        "                    y : batch[\"Price\"].values.reshape(batchSize,1),\n",
        "                    factorPrice : batch[\"DividendFactor\"].values.reshape(batchSize,1), \n",
        "                    learningRateTensor : learningRate,\n",
        "                    vegaRef : np.ones_like(batch[\"VegaRef\"].values.reshape(batchSize,1))}\n",
        "        return feedDict\n",
        "\n",
        "    #Learning rate is divided by 10 if no imporvement is observed for training loss after \"patience\" epochs\n",
        "    def updateLearningRate(iterNumber, lr, lrEpoch):\n",
        "        if not activateLearningDecrease :\n",
        "            print(\"Constant learning rate, stop training\")\n",
        "            return False, lr, lrEpoch\n",
        "        if learningRate > finalLearningRate :\n",
        "            lr *= 0.1\n",
        "            lrEpoch = iterNumber\n",
        "            saver.restore(sess, modelName)\n",
        "            print(\"Iteration : \", lrEpoch, \"new learning rate : \", lr)\n",
        "        else :\n",
        "          print(\"Last Iteration : \", lrEpoch, \"final learning rate : \", lr)\n",
        "          return False, lr, lrEpoch\n",
        "        return True, lr, lrEpoch\n",
        "    \n",
        "    epochFeedDict = createFeedDict(dataSet)\n",
        "\n",
        "    def evalBestModel():\n",
        "        if not activateLearningDecrease :\n",
        "            print(\"Learning rate : \", learningRate, \" final loss : \", min(loss_serie))\n",
        "        currentBestLoss = sess.run(loss, feed_dict=epochFeedDict)\n",
        "        currentBestPenalizations = sess.run([pointwiseError, penalizationList], feed_dict=epochFeedDict)\n",
        "        print(\"Best loss (hidden nodes: %d, iterations: %d): %.2f\" % (hidden_nodes, len(loss_serie), currentBestLoss))\n",
        "        print(\"Best Penalization : \", currentBestPenalizations)\n",
        "        return\n",
        "    \n",
        "    for i in range(nbEpoch):\n",
        "        miniBatchList = [dataSet]\n",
        "        penalizationResult = sess.run(penalizationList, feed_dict=epochFeedDict)\n",
        "        lossResult = sess.run(pointwiseError, feed_dict=epochFeedDict)\n",
        "\n",
        "        #miniBatchList = selectMiniBatchWithoutReplacement(dataSet, batch_size)\n",
        "        for k in range(len(miniBatchList)) :\n",
        "            batchFeedDict = createFeedDict(miniBatchList[k])\n",
        "            sess.run(train, feed_dict=batchFeedDict)\n",
        "        \n",
        "        \n",
        "        loss_serie.append(sess.run(loss, feed_dict=epochFeedDict))\n",
        "\n",
        "        if (len(loss_serie) < 2) or (loss_serie[-1] <= min(loss_serie)):\n",
        "          #Save model as model is improved\n",
        "          saver.save(sess, modelName)\n",
        "        if (np.isnan(loss_serie[-1]) or  #Unstable model\n",
        "            ( (i-learningRateEpoch >= patience) and (min(loss_serie[-patience:]) > min(loss_serie)) ) ) : #No improvement for training loss during the latest 100 iterations\n",
        "          continueTraining, learningRate, learningRateEpoch = updateLearningRate(i, learningRate, learningRateEpoch)\n",
        "          if continueTraining :\n",
        "            evalBestModel()\n",
        "          else :\n",
        "            break\n",
        "    saver.restore(sess, modelName)  \n",
        "    \n",
        "    evalBestModel()\n",
        "\n",
        "    evalList  = sess.run(TensorList, feed_dict=epochFeedDict)\n",
        "    \n",
        "    sess.close()\n",
        "    end = time.time()\n",
        "    print(\"Training Time : \", end - start)\n",
        "    \n",
        "    return formattingFunction(*evalList, loss_serie, dataSet) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vn3rCCVOeCyX",
        "colab": {}
      },
      "source": [
        "#Evaluate neural network without training, it restores parameters obtained from a pretrained model \n",
        "#NNFactory :  function creating the neural architecture\n",
        "#dataSet : dataset on which neural network is evaluated \n",
        "#activateRegularization : boolean, if true add bound penalization for dupire variance\n",
        "#hyperparameters : dictionnary containing various hyperparameters\n",
        "#modelName : name of tensorflow model to restore\n",
        "def create_eval_model(NNFactory, \n",
        "                      dataSet, \n",
        "                      activateRegularization, \n",
        "                      hyperparameters,\n",
        "                      modelName = \"bestModel\"):\n",
        "    hidden_nodes = hyperparameters[\"nbUnits\"] \n",
        "    \n",
        "    # Go through num_iters iterations (ignoring mini-batching)\n",
        "    activateLearningDecrease = (~ hyperparameters[\"FixedLearningRate\"])\n",
        "    learningRate = hyperparameters[\"LearningRateStart\"]\n",
        "\n",
        "    # Reset the graph\n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    # Placeholders for input and output data   \n",
        "    Strike = tf.placeholder(tf.float32,[None,1])\n",
        "    Maturity = tf.placeholder(tf.float32,[None,1])\n",
        "    factorPrice = tf.placeholder(tf.float32,[None,1])\n",
        "    y = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='y')\n",
        "    vegaRef = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='vegaRef')\n",
        "    learningRateTensor = tf.placeholder(tf.float32,[])\n",
        "    \n",
        "    #Get scaling for strike\n",
        "    colStrikeIndex = dataSet.columns.get_loc(\"ChangedStrike\")\n",
        "    maxColFunction = scaler.data_max_[colStrikeIndex]\n",
        "    minColFunction = scaler.data_min_[colStrikeIndex]\n",
        "    scF = (maxColFunction - minColFunction) \n",
        "    scaleTensor = tf.constant(scF, dtype=tf.float32)\n",
        "    strikeMinTensor = tf.constant(minColFunction, dtype=tf.float32)\n",
        "\n",
        "    price_pred_tensor = None\n",
        "    TensorList = None\n",
        "    penalizationList = None \n",
        "    formattingFunction = None\n",
        "    if activateRegularization : \n",
        "        price_pred_tensor, TensorList, penalizationList, formattingFunction = addDupireRegularisation( *NNFactory(hidden_nodes,\n",
        "                                                                                                                  Strike,\n",
        "                                                                                                                  Maturity, \n",
        "                                                                                                                  scaleTensor, \n",
        "                                                                                                                  strikeMinTensor, \n",
        "                                                                                                                  vegaRef,\n",
        "                                                                                                                  hyperparameters,\n",
        "                                                                                                                  IsTraining=False), \n",
        "                                                                                                      vegaRef,\n",
        "                                                                                                      hyperparameters )\n",
        "    else :\n",
        "        price_pred_tensor, TensorList, penalizationList, formattingFunction = NNFactory(hidden_nodes,\n",
        "                                                                                        Strike, \n",
        "                                                                                        Maturity, \n",
        "                                                                                        scaleTensor, \n",
        "                                                                                        strikeMinTensor,\n",
        "                                                                                        vegaRef,\n",
        "                                                                                        hyperparameters,\n",
        "                                                                                        IsTraining=False)\n",
        "\n",
        "    price_pred_tensor_sc= tf.multiply(factorPrice,price_pred_tensor)\n",
        "    TensorList[0] = price_pred_tensor_sc\n",
        "    \n",
        "    # Define a loss function\n",
        "    pointwiseError = tf.reduce_mean(tf.abs(price_pred_tensor_sc - y) / vegaRef)\n",
        "    errors = tf.add_n([pointwiseError] + penalizationList)\n",
        "    loss = tf.log(tf.reduce_mean(errors))\n",
        "\n",
        "\n",
        "    # Define a train operation to minimize the loss\n",
        "    lr = learningRate \n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRateTensor)\n",
        "    train = optimizer.minimize(loss)\n",
        "\n",
        "    # Initialize variables and run session\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver()\n",
        "    sess = tf.Session()\n",
        "    sess.run(init)\n",
        "    n = dataSet.shape[0]\n",
        "    scaledInput = transformCustomMinMax(dataSet, scaler)\n",
        "\n",
        "    \n",
        "    maturity = dataSet[\"Maturity\"].values.reshape(n,1)\n",
        "    loss_serie = []\n",
        "\n",
        "    def createFeedDict(batch):\n",
        "        batchSize = batch.shape[0]\n",
        "        feedDict = {Strike : scaledInput[\"ChangedStrike\"].loc[batch.index].values.reshape(batchSize,1),\n",
        "                    Maturity : batch[\"Maturity\"].values.reshape(batchSize,1), \n",
        "                    y : batch[\"Price\"].values.reshape(batchSize,1),\n",
        "                    factorPrice : batch[\"DividendFactor\"].values.reshape(batchSize,1), \n",
        "                    learningRateTensor : learningRate,\n",
        "                    vegaRef : np.ones_like(batch[\"VegaRef\"].values.reshape(batchSize,1))}\n",
        "        return feedDict\n",
        "    \n",
        "    epochFeedDict = createFeedDict(dataSet)\n",
        "\n",
        "    def evalBestModel():\n",
        "        if not activateLearningDecrease :\n",
        "            print(\"Learning rate : \", learningRate, \" final loss : \", min(loss_serie))\n",
        "        currentBestLoss = sess.run(loss, feed_dict=epochFeedDict)\n",
        "        currentBestPenalizations = sess.run([pointwiseError, penalizationList], feed_dict=epochFeedDict)\n",
        "        print(\"Best loss (hidden nodes: %d, iterations: %d): %.2f\" % (hidden_nodes, len(loss_serie), currentBestLoss))\n",
        "        print(\"Best Penalization : \", currentBestPenalizations)\n",
        "        return\n",
        "    \n",
        "    saver.restore(sess, modelName)  \n",
        "    \n",
        "    evalBestModel()\n",
        "\n",
        "    evalList  = sess.run(TensorList, feed_dict=epochFeedDict)\n",
        "    \n",
        "    sess.close()\n",
        "    \n",
        "    return formattingFunction(*evalList, [0], dataSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-qgxVPguB_6P"
      },
      "source": [
        "### Convex architecture (Price only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JKjTFU6ioS76",
        "colab": {}
      },
      "source": [
        "\n",
        "#Soft constraints for strike convexity and strike/maturity monotonicity  \n",
        "def arbitragePenalties(priceTensor, strikeTensor, maturityTensor, scaleTensor, vegaRef, hyperparameters):\n",
        "\n",
        "    dK = tf.gradients(priceTensor, strikeTensor, name=\"dK\")\n",
        "    hK = tf.gradients(dK[0], strikeTensor, name=\"hK\") / tf.square(scaleTensor)\n",
        "    theta = tf.gradients(priceTensor,maturityTensor,name=\"dT\")\n",
        "    \n",
        "    lambdas = hyperparameters[\"lambdaSoft\"]  / tf.reduce_mean(vegaRef) \n",
        "    lowerBoundTheta = tf.constant(hyperparameters[\"lowerBoundTheta\"])\n",
        "    lowerBoundGamma = tf.constant(hyperparameters[\"lowerBoundGamma\"])\n",
        "    grad_penalty = lambdas * tf.reduce_mean(tf.nn.relu(-theta[0] + lowerBoundTheta ))\n",
        "    hessian_penalty = lambdas * hyperparameters[\"lowerBoundGamma\"] * tf.reduce_mean(tf.nn.relu(-hK[0] + lowerBoundGamma ))\n",
        "    \n",
        "    return [grad_penalty, hessian_penalty]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "njypUjlPLFeK",
        "colab": {}
      },
      "source": [
        "#Tools function for Neural network architecture\n",
        "\n",
        "#Initilize weights as positive\n",
        "def positiveKernelInitializer(shape, \n",
        "                              dtype=None, \n",
        "                              partition_info=None):\n",
        "  return tf.abs(tf.keras.initializers.normal()(shape,dtype=dtype, partition_info=partition_info))\n",
        "\n",
        "#Soft convex layer\n",
        "def convexLayer(n_units, tensor, isTraining, name, isNonDecreasing = True):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
        "                            units=n_units,\n",
        "                            kernel_initializer=tf.keras.initializers.glorot_normal())\n",
        "    \n",
        "    \n",
        "    return tf.nn.softplus(layer)\n",
        "\n",
        "#Soft monotonic layer\n",
        "def monotonicLayer(n_units,  tensor, isTraining, name):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor, \n",
        "                            units=n_units,\n",
        "                            kernel_initializer=tf.keras.initializers.glorot_normal())\n",
        "    \n",
        "    \n",
        "    \n",
        "    return tf.nn.sigmoid(layer)\n",
        "\n",
        "#Soft convex layer followed by output layer for regression \n",
        "def convexOutputLayer(n_units, tensor, isTraining, name, isNonDecreasing = True):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
        "                            units=2*n_units,\n",
        "                            kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
        "                            activation = 'softplus')\n",
        "    \n",
        "     \n",
        "    layer = tf.layers.dense(layer, \n",
        "                            units=1,\n",
        "                            kernel_initializer=positiveKernelInitializer,\n",
        "                            activation = 'softplus')\n",
        "    \n",
        "    return layer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Neural network factory for Hybrid approach : splitted network with soft contraints\n",
        "def NNArchitectureConstrained(n_units, \n",
        "                              strikeTensor,\n",
        "                              maturityTensor, \n",
        "                              scaleTensor, \n",
        "                              strikeMinTensor, \n",
        "                              vegaRef, \n",
        "                              hyperparameters,\n",
        "                              IsTraining=True):\n",
        "  #First splitted layer\n",
        "  hidden1S = convexLayer(n_units = n_units,\n",
        "                         tensor = strikeTensor,\n",
        "                         isTraining=IsTraining, \n",
        "                         name = \"Hidden1S\")\n",
        "  \n",
        "  hidden1M = monotonicLayer(n_units = n_units,\n",
        "                            tensor = maturityTensor, \n",
        "                            isTraining = IsTraining, \n",
        "                            name = \"Hidden1M\")\n",
        "  \n",
        "  hidden1 = tf.concat([hidden1S, hidden1M], axis=-1)\n",
        "  \n",
        "  #Second and output layer\n",
        "  out = convexOutputLayer(n_units = n_units,\n",
        "                          tensor = hidden1,\n",
        "                          isTraining = IsTraining,\n",
        "                          name = \"Output\")\n",
        "  #Soft constraints\n",
        "  penaltyList = arbitragePenalties(out, strikeTensor, \n",
        "                                   maturityTensor, \n",
        "                                   scaleTensor, \n",
        "                                   vegaRef, \n",
        "                                   hyperparameters)\n",
        "  \n",
        "  return out, [out], penaltyList, evalAndFormatResult"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6i9lEOpGoS79",
        "colab": {}
      },
      "source": [
        "plt.plot(dataSet.index.get_level_values(\"Strike\"), dataSet[\"Price\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uT7gq2LRSXTI",
        "colab": {}
      },
      "source": [
        "y_pred0, lossSerie0 = create_train_model(NNArchitectureConstrained, scaledDataSet, False, hyperparameters, modelName = \"softConvexHybridModel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-fv_bsyPSXTK",
        "colab": {}
      },
      "source": [
        "print(\"Minimum error : \",lossSerie0.min())\n",
        "plotEpochLoss(lossSerie0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Kwi7u3dELnws",
        "colab": {}
      },
      "source": [
        "lossSerie0.iloc[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MUEXO111oS8G",
        "colab": {}
      },
      "source": [
        "y_pred0, lossSerie0 = create_eval_model(NNArchitectureConstrained, \n",
        "                                        scaledDataSet, \n",
        "                                        False, \n",
        "                                        hyperparameters, \n",
        "                                        modelName = \"softConvexHybridModel\")\n",
        "predictionDiagnosis(y_pred0, dataSet[\"Price\"], \" Price \")\n",
        "impV0 = plotImpliedVol(y_pred0, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L-mNSLXHhRfm",
        "colab": {}
      },
      "source": [
        "y_pred0.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2STQwalmhoyR",
        "colab": {}
      },
      "source": [
        "y_pred0.loc[(midS0,slice(None))].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V9tKlu5Ji1YF",
        "colab": {}
      },
      "source": [
        "dataSet[\"Price\"].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oRI-8R9ezLLa",
        "colab": {}
      },
      "source": [
        "y_pred0Test, lossSerie0Test = create_eval_model(NNArchitectureConstrained, \n",
        "                                                scaledDataSetTest, \n",
        "                                                False, \n",
        "                                                hyperparameters, \n",
        "                                                modelName = \"softConvexHybridModel\")\n",
        "predictionDiagnosis(y_pred0Test, dataSetTest[\"Price\"], \" Price \")\n",
        "impV0Test = plotImpliedVol(y_pred0Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nes-RItOzLIM",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bQT4PIYWzK81",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KWdzg9DEv7eH"
      },
      "source": [
        "### Unconstrained neural network (Price only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HnG0q3QisrMt",
        "colab": {}
      },
      "source": [
        "#Unconstrained dense layer\n",
        "def unconstrainedLayer(n_units,  tensor, isTraining, name, activation = K.softplus):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor, \n",
        "                            units=n_units,\n",
        "                            activation=activation,  \n",
        "                            kernel_initializer=tf.keras.initializers.he_normal())\n",
        "    return layer\n",
        "\n",
        "#Factory for unconstrained network\n",
        "def NNArchitectureUnconstrained(n_units, \n",
        "                                strikeTensor,\n",
        "                                maturityTensor, \n",
        "                                scaleTensor, \n",
        "                                strikeMinTensor, \n",
        "                                vegaRef,\n",
        "                                hyperparameters,\n",
        "                                IsTraining=True):\n",
        "  \n",
        "  inputLayer = tf.concat([strikeTensor,maturityTensor], axis=-1)\n",
        "  \n",
        "  #First layer\n",
        "  hidden1 = unconstrainedLayer(n_units = n_units,\n",
        "                               tensor = inputLayer,\n",
        "                               isTraining=IsTraining, \n",
        "                               name = \"Hidden1\")\n",
        "  \n",
        "  #Second layer\n",
        "  hidden2 = unconstrainedLayer(n_units = n_units,\n",
        "                               tensor = hidden1,\n",
        "                               isTraining=IsTraining, \n",
        "                               name = \"Hidden2\")\n",
        "  #Output layer \n",
        "  out = unconstrainedLayer(n_units = 1,\n",
        "                           tensor = hidden2,\n",
        "                           isTraining=IsTraining, \n",
        "                           name = \"Output\",\n",
        "                           activation = None)\n",
        "  \n",
        "  return out, [out], [], evalAndFormatResult\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_YZGyKcpsrMv",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WdWudyC2srMw",
        "colab": {}
      },
      "source": [
        "y_pred1, lossSerie1 = create_train_model(NNArchitectureUnconstrained, \n",
        "                                         scaledDataSet, \n",
        "                                         False, \n",
        "                                         hyperparameters,\n",
        "                                         modelName = \"unconstrainedModel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AzjHIzRUsrMx",
        "colab": {}
      },
      "source": [
        "print(\"Minimum error : \",lossSerie1.min())\n",
        "plotEpochLoss(lossSerie1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TArgJP5fsrMy",
        "colab": {}
      },
      "source": [
        "lossSerie1.iloc[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IzlURNdisrM0",
        "colab": {}
      },
      "source": [
        "y_pred1, lossSerie1 = create_eval_model(NNArchitectureUnconstrained, \n",
        "                                        scaledDataSet, \n",
        "                                        False, \n",
        "                                        hyperparameters,\n",
        "                                        modelName = \"unconstrainedModel\")\n",
        "predictionDiagnosis(y_pred1, dataSet[\"Price\"], \" Price \")\n",
        "impV1 = plotImpliedVol(y_pred1, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J5fP6NJ4srM1",
        "colab": {}
      },
      "source": [
        "y_pred1.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_pMZlMbdsrM5",
        "colab": {}
      },
      "source": [
        "y_pred1.loc[(midS0,slice(None))].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lU1LIqnPsrM9",
        "colab": {}
      },
      "source": [
        "dataSet[\"Price\"].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bPrMgqTtsrNB",
        "colab": {}
      },
      "source": [
        "y_pred1Test, lossSerie1Test = create_eval_model(NNArchitectureUnconstrained, \n",
        "                                                scaledDataSetTest, \n",
        "                                                False, \n",
        "                                                hyperparameters,\n",
        "                                                modelName = \"unconstrainedModel\")\n",
        "predictionDiagnosis(y_pred1Test, dataSetTest[\"Price\"], \" Price \")\n",
        "impV1Test = plotImpliedVol(y_pred1Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7JCdZib-C7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfnzM06b--q0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFoYMDiI_DHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ONoMCWOCXWO-"
      },
      "source": [
        "## Dupire formula implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QWZzojhoDJRn",
        "colab": {}
      },
      "source": [
        "#Dupire formula from exact derivative computation\n",
        "def dupireFormula(HessianStrike, \n",
        "                  GradMaturity, \n",
        "                  Strike,\n",
        "                  scaleTensor,\n",
        "                  strikeMinTensor,\n",
        "                  IsTraining=True):\n",
        "  twoConstant = tf.constant(2.0)\n",
        "  dupireVar = tf.math.divide(tf.math.divide(tf.math.scalar_mul(twoConstant,GradMaturity), \n",
        "                                            HessianStrike), \n",
        "                             tf.square(Strike + strikeMinTensor / scaleTensor))\n",
        "  #Initial weights of neural network can be random which lead to negative dupireVar\n",
        "  dupireVolTensor = tf.sqrt(dupireVar) \n",
        "  return dupireVolTensor, dupireVar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o59zKQXO8LKU",
        "colab": {}
      },
      "source": [
        "#Dupire formula with derivative obtained from native tensorflow algorithmic differentiation\n",
        "def rawDupireFormula(priceTensor, \n",
        "                     adjustedStrikeTensor, \n",
        "                     maturityTensor,\n",
        "                     scaleTensor,\n",
        "                     strikeMinTensor,\n",
        "                     IsTraining=True):\n",
        "  batchSize = tf.shape(adjustedStrikeTensor)[0]\n",
        "  dK = tf.reshape(tf.gradients(priceTensor, adjustedStrikeTensor, name=\"dK\")[0], shape=[batchSize,-1])\n",
        "  hK = tf.reshape(tf.gradients(dK, adjustedStrikeTensor, name=\"hK\")[0], shape=[batchSize,-1])\n",
        "  dupireDenominator = tf.square(adjustedStrikeTensor + strikeMinTensor / scaleTensor) * hK\n",
        "\n",
        "  dT = tf.reshape(tf.gradients(priceTensor,maturityTensor,name=\"dT\")[0], shape=[batchSize,-1])\n",
        "\n",
        "  #Initial weights of neural network can be random which lead to negative dupireVar\n",
        "  dupireVar = 2 * dT / dupireDenominator\n",
        "  dupireVol = tf.sqrt(dupireVar) \n",
        "  return  dupireVol, dT, hK / tf.square(scaleTensor), dupireVar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B3YTL72Tw_FV",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s192l-r-B6lD"
      },
      "source": [
        "### Hybrid architecture (Exact derivatives)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1mtTLFrQC36u",
        "colab": {}
      },
      "source": [
        "def exact_derivatives(Strike, Maturity):\n",
        "    w1K = tf.get_default_graph().get_tensor_by_name( 'dense/kernel:0')\n",
        "    w1T = tf.get_default_graph().get_tensor_by_name( 'dense_1/kernel:0')\n",
        "    w2 = tf.get_default_graph().get_tensor_by_name( 'dense_2/kernel:0')\n",
        "    w3 = tf.get_default_graph().get_tensor_by_name( 'dense_3/kernel:0')\n",
        "\n",
        "    b1K = tf.get_default_graph().get_tensor_by_name( 'dense/bias:0')\n",
        "    b1T = tf.get_default_graph().get_tensor_by_name( 'dense_1/bias:0')\n",
        "    b2 = tf.get_default_graph().get_tensor_by_name( 'dense_2/bias:0')\n",
        "    b3 = tf.get_default_graph().get_tensor_by_name( 'dense_3/bias:0')\n",
        "\n",
        "    Z1K= tf.nn.softplus(tf.matmul(Strike, w1K) + b1K)\n",
        "    Z1T= tf.nn.sigmoid(tf.matmul(Maturity, w1T) + b1T)\n",
        "\n",
        "    Z= tf.concat([Z1K, Z1T], axis=-1)\n",
        "    I2=tf.matmul(Z, w2) + b2\n",
        "    Z2=tf.nn.softplus(I2)\n",
        "    I3=tf.matmul(Z2, w3) + b3\n",
        "    F=tf.nn.softplus(I3)\n",
        "\n",
        "    D1K= tf.nn.sigmoid(tf.matmul(Strike, w1K) + b1K)\n",
        "    I2K=tf.multiply(D1K, w1K)\n",
        "    Z2K = tf.concat([I2K, tf.scalar_mul(tf.constant(0.0),I2K)],axis=-1)\n",
        "   \n",
        "    dI2dK=tf.matmul(Z2K, w2)\n",
        "    Z2w3=tf.multiply(tf.nn.sigmoid(I2),dI2dK)\n",
        "    dI3dK=tf.matmul(Z2w3, w3)\n",
        "    dF_dK=tf.multiply(tf.nn.sigmoid(I3),dI3dK)\n",
        "    \n",
        "    D1T= sigmoidGradient(tf.matmul(Maturity,w1T) + b1T)\n",
        "    I2T=tf.multiply(D1T, w1T)\n",
        "    Z2T = tf.concat([tf.scalar_mul(tf.constant(0.0),I2T), I2T],axis=-1)\n",
        "   \n",
        "    dI2dT=tf.matmul(Z2T, w2)\n",
        "    Z2w3=tf.multiply(tf.nn.sigmoid(I2),dI2dT)\n",
        "    dI3dT=tf.matmul(Z2w3, w3)\n",
        "    dF_dT=tf.multiply(tf.nn.sigmoid(I3),dI3dT)\n",
        "    \n",
        "    \n",
        "    d2F_dK2=tf.multiply(sigmoidGradient(I3),tf.square(dI3dK))\n",
        "    DD1K=sigmoidGradient(tf.matmul(Strike, w1K) + b1K)\n",
        "    w1K2=tf.multiply(w1K,w1K)\n",
        "    ID2K=tf.multiply(DD1K,w1K2)\n",
        "    ZD2K = tf.concat([ID2K, tf.scalar_mul(tf.constant(0.0),ID2K)],axis=-1)\n",
        "   \n",
        "    d2I2_dK2=tf.matmul(ZD2K, w2)\n",
        "    \n",
        "    ZD2=tf.multiply(sigmoidGradient(I2), tf.square(dI2dK)) \n",
        "    ZD2+=tf.multiply(tf.nn.sigmoid(I2),d2I2_dK2)\n",
        "    d2I3dK2=tf.matmul(ZD2, w3)\n",
        "    \n",
        "    d2F_dK2+=tf.multiply(tf.nn.sigmoid(I3),d2I3dK2)\n",
        "    \n",
        "    return dF_dT, dF_dK, d2F_dK2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "g_fkoZMVXWO-",
        "colab": {}
      },
      "source": [
        "#Tools functions for neural architecture\n",
        "def positiveKernelInitializer(shape, \n",
        "                              dtype=None, \n",
        "                              partition_info=None):\n",
        "  return tf.abs(tf.keras.initializers.normal()(shape,dtype=dtype, partition_info=partition_info))\n",
        "\n",
        "\n",
        "#Neural network architecture\n",
        "def convexLayer1(n_units, tensor, isTraining, name, isNonDecreasing = True):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
        "                            units=n_units,\n",
        "                            kernel_initializer=tf.keras.initializers.glorot_normal())\n",
        "    \n",
        "    \n",
        "    return tf.nn.softplus(layer), layer\n",
        "\n",
        "def monotonicLayer1(n_units,  tensor, isTraining, name):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor, \n",
        "                            units=n_units,\n",
        "                            kernel_initializer=tf.keras.initializers.glorot_normal())\n",
        "    \n",
        "    \n",
        "    \n",
        "    return tf.nn.sigmoid(layer),layer\n",
        "\n",
        "def convexOutputLayer1(n_units, tensor, isTraining, name, isNonDecreasing = True):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
        "                            units=2*n_units,\n",
        "                            kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
        "                            activation = 'softplus') \n",
        "    \n",
        "     \n",
        "    layer = tf.layers.dense(layer, \n",
        "                            units=1,\n",
        "                            kernel_initializer=positiveKernelInitializer, \n",
        "                            activation = 'softplus')\n",
        "    \n",
        "    return layer, layer \n",
        "  \n",
        "\n",
        "def convexLayerHybrid1(n_units, \n",
        "                      tensor, \n",
        "                      isTraining, \n",
        "                      name, \n",
        "                      activationFunction2 = Act.softplus,\n",
        "                      activationFunction1 = Act.exponential,\n",
        "                      isNonDecreasing = True):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
        "                            units=n_units,\n",
        "                            kernel_initializer=positiveKernelInitializer)\n",
        "    l1,l2 = tf.split(layer,2,1)\n",
        "    output = tf.concat([activationFunction1(l1),activationFunction2(l2)],axis=-1)\n",
        "    return output , layer\n",
        "\n",
        "def sigmoidGradient(inputTensor):\n",
        "  return tf.nn.sigmoid(inputTensor) * ( 1 - tf.nn.sigmoid(inputTensor) )\n",
        "\n",
        "def sigmoidHessian(inputTensor) :\n",
        "  return (tf.square(1 - tf.nn.sigmoid(inputTensor)) -\n",
        "          tf.nn.sigmoid(inputTensor) * (1 - tf.nn.sigmoid(inputTensor)))\n",
        "\n",
        "  \n",
        "def NNArchitectureConstrainedDupire(n_units, \n",
        "                                    strikeTensor,\n",
        "                                    maturityTensor, \n",
        "                                    scaleTensor, \n",
        "                                    strikeMinTensor,\n",
        "                                    vegaRef, \n",
        "                                    hyperparameters,\n",
        "                                    IsTraining=True):\n",
        "  #First splitted layer\n",
        "  hidden1S, layer1S = convexLayer1(n_units = n_units,\n",
        "                                   tensor = strikeTensor,\n",
        "                                   isTraining=IsTraining,\n",
        "                                   name = \"Hidden1S\")\n",
        "  \n",
        "  hidden1M,layer1M = monotonicLayer1(n_units = n_units,\n",
        "                                     tensor = maturityTensor,\n",
        "                                     isTraining = IsTraining,\n",
        "                                     name = \"Hidden1M\")\n",
        "  \n",
        "  hidden1 = tf.concat([hidden1S, hidden1M], axis=-1)\n",
        "  \n",
        "  #Second layer and output layer\n",
        "  out, layer = convexOutputLayer1(n_units = n_units,\n",
        "                                  tensor = hidden1,\n",
        "                                  isTraining = IsTraining,\n",
        "                                  name = \"Output\")\n",
        "  \n",
        "  \n",
        "  dT, dS, HS = exact_derivatives(strikeTensor, maturityTensor)\n",
        "  \n",
        "  \n",
        "  \n",
        "  #Local volatility\n",
        "  dupireVol, dupireVar = dupireFormula(HS, dT, \n",
        "                                       strikeTensor,\n",
        "                                       scaleTensor,\n",
        "                                       strikeMinTensor, \n",
        "                                       IsTraining=IsTraining)\n",
        "  \n",
        "  #Soft constraints on price\n",
        "  lambdas = hyperparameters[\"lambdaSoft\"]\n",
        "  lowerBoundTheta = tf.constant(hyperparameters[\"lowerBoundTheta\"])\n",
        "  lowerBoundGamma = tf.constant(hyperparameters[\"lowerBoundGamma\"])\n",
        "  grad_penalty = lambdas * tf.reduce_mean(tf.nn.relu(-dT + lowerBoundTheta) / vegaRef)\n",
        "  HSScaled = HS / tf.square(scaleTensor)\n",
        "  hessian_penalty = lambdas * hyperparameters[\"lambdaGamma\"] * tf.reduce_mean(tf.nn.relu(- HSScaled + lowerBoundGamma) / vegaRef)\n",
        "  \n",
        "  return out, [out, dupireVol, dT, HSScaled, dupireVar], [grad_penalty, hessian_penalty], evalAndFormatDupireResult\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SHfggURehdvS",
        "colab": {}
      },
      "source": [
        "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yRIe-lSD8A0H",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8NX751W-BNad",
        "colab": {}
      },
      "source": [
        "y_pred2, volLocale2, dNN_T2, gNN_K2, lossSerie2 = create_train_model(NNArchitectureConstrainedDupire,\n",
        "                                                                     scaledDataSet,\n",
        "                                                                     False, \n",
        "                                                                     hyperparameters,\n",
        "                                                                     modelName = \"convexHybridMatthewDupireVolModel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XnX-RKpgBO0y",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerie2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "puVhFTMTBcQ5",
        "colab": {}
      },
      "source": [
        "lossSerie2.iloc[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x5k1RAfyearf",
        "colab": {}
      },
      "source": [
        "y_pred2, volLocale2, dNN_T2, gNN_K2, lossSerie2 = create_eval_model(NNArchitectureConstrainedDupire, \n",
        "                                                                    scaledDataSet, \n",
        "                                                                    False, \n",
        "                                                                    hyperparameters,\n",
        "                                                                    modelName = \"convexHybridMatthewDupireVolModel\")\n",
        "modelSummary(y_pred2, volLocale2, dNN_T2, gNN_K2, dataSet)\n",
        "impV2 = plotImpliedVol(y_pred2, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xj1pPdXLZHrk",
        "colab": {}
      },
      "source": [
        "volLocale2.loc[(midS0,slice(None))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R0jZy5U7JqVt",
        "colab": {}
      },
      "source": [
        "y_pred2Test, volLocale2Test, dNN_T2Test, gNN_K2Test, lossSerie2Test = create_eval_model(NNArchitectureConstrainedDupire, \n",
        "                                                                                        scaledDataSetTest, \n",
        "                                                                                        False, \n",
        "                                                                                        hyperparameters,\n",
        "                                                                                        modelName = \"convexHybridMatthewDupireVolModel\")\n",
        "modelSummary(y_pred2Test, volLocale2Test, dNN_T2Test, gNN_K2Test, dataSetTest)\n",
        "impV2Test = plotImpliedVol(y_pred2Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "de9dKmAAHI2m",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hUMtwNuguMRu"
      },
      "source": [
        "### Hybrid Network (Derivatives from algorithmic differentiation) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YOiA45eZJqSb",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j8m0kMz_udFn",
        "colab": {}
      },
      "source": [
        "\n",
        "def NNArchitectureConstrainedRawDupire(n_units, \n",
        "                                       strikeTensor,\n",
        "                                       maturityTensor,\n",
        "                                       scaleTensor,\n",
        "                                       strikeMinTensor, \n",
        "                                       vegaRef, \n",
        "                                       hyperparameters,\n",
        "                                       IsTraining=True):\n",
        "  #First splitted layer\n",
        "  hidden1S = convexLayer(n_units = n_units,\n",
        "                         tensor = strikeTensor,\n",
        "                         isTraining=IsTraining, \n",
        "                         name = \"Hidden1S\")\n",
        "  \n",
        "  hidden1M = monotonicLayer(n_units = n_units,\n",
        "                            tensor = maturityTensor, \n",
        "                            isTraining = IsTraining, \n",
        "                            name = \"Hidden1M\")\n",
        "  \n",
        "  hidden1 = tf.concat([hidden1S, hidden1M], axis=-1)\n",
        "  \n",
        "  #Second hidden layer and output layer\n",
        "  out = convexOutputLayer(n_units = n_units,\n",
        "                          tensor = hidden1,\n",
        "                          isTraining = IsTraining,\n",
        "                          name = \"Output\")\n",
        "  \n",
        "  #Compute local volatility\n",
        "  dupireVol, theta, hK, dupireVar = rawDupireFormula(out, strikeTensor, \n",
        "                                                     maturityTensor, \n",
        "                                                     scaleTensor, \n",
        "                                                     strikeMinTensor,\n",
        "                                                     IsTraining=IsTraining)\n",
        "\n",
        "  #Soft constraints for no-arbitrage\n",
        "  lambdas = hyperparameters[\"lambdaSoft\"] \n",
        "  lowerBoundTheta = tf.constant(hyperparameters[\"lowerBoundTheta\"])\n",
        "  lowerBoundGamma = tf.constant(hyperparameters[\"lowerBoundGamma\"])\n",
        "  grad_penalty = lambdas * tf.reduce_mean(tf.nn.relu(-theta + lowerBoundTheta) / vegaRef)\n",
        "  hessian_penalty = lambdas * hyperparameters[\"lambdaGamma\"] * tf.reduce_mean(tf.nn.relu(-hK + lowerBoundGamma) / vegaRef)\n",
        "  \n",
        "  return out, [out, dupireVol, theta, hK, dupireVar], [grad_penalty, hessian_penalty], evalAndFormatDupireResult"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GntRmtJkudCz",
        "colab": {}
      },
      "source": [
        "y_pred3, volLocale3, dNN_T3, gNN_K3, lossSerie3 = create_train_model(NNArchitectureConstrainedRawDupire,\n",
        "                                                                     scaledDataSet,\n",
        "                                                                     False, \n",
        "                                                                     hyperparameters,\n",
        "                                                                     modelName = \"convexHybridDupireVolModel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2lkQVBdnuc_-",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerie3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bfEKwrHCuc9B",
        "colab": {}
      },
      "source": [
        "lossSerie3.iloc[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MA24IOZoucqj",
        "colab": {}
      },
      "source": [
        "y_pred3, volLocale3, dNN_T3, gNN_K3, lossSerie3 = create_eval_model(NNArchitectureConstrainedRawDupire, \n",
        "                                                                    scaledDataSet, \n",
        "                                                                    False,\n",
        "                                                                    hyperparameters,\n",
        "                                                                    modelName = \"convexHybridDupireVolModel\")\n",
        "modelSummary(y_pred3, volLocale3, dNN_T3, gNN_K3, dataSet)\n",
        "impV3 = plotImpliedVol(y_pred3, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R5BCRxbMuc0p",
        "colab": {}
      },
      "source": [
        "volLocale3.loc[(midS0,slice(None))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7bS4fYI7ucjO",
        "colab": {}
      },
      "source": [
        "y_pred3Test, volLocale3Test, dNN_T3Test, gNN_K3Test, lossSerie3Test = create_eval_model(NNArchitectureConstrainedRawDupire, \n",
        "                                                                                        scaledDataSetTest, \n",
        "                                                                                        False, \n",
        "                                                                                        hyperparameters,\n",
        "                                                                                        modelName = \"convexHybridDupireVolModel\")\n",
        "modelSummary(y_pred3Test, volLocale3Test, dNN_T3Test, gNN_K3Test, dataSetTest)\n",
        "impV3Test = plotImpliedVol(y_pred3Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Y02WGdGVucZ5",
        "colab": {}
      },
      "source": [
        "\n",
        "dNN_T3Test[dNN_T3Test<=0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIki8Q3nfygr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelSummary(y_pred3, \n",
        "             volLocale3, \n",
        "             dNN_T3, \n",
        "             gNN_K3, \n",
        "             dataSet,\n",
        "             logMoneynessScale = True)\n",
        "impV3 = plotImpliedVol(y_pred3, \n",
        "                       dataSet[\"ImpliedVol\"], \n",
        "                       rIntegralSpline=riskFreeIntegral, \n",
        "                       qIntegralSpline=divSpreadIntegral,\n",
        "                       logMoneynessScale = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sixvBhaGfyYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelSummary(y_pred3Test, \n",
        "             volLocale3Test, \n",
        "             dNN_T3Test, \n",
        "             gNN_K3Test, \n",
        "             dataSetTest,\n",
        "             logMoneynessScale = True)\n",
        "impV3Test = plotImpliedVol(y_pred3Test, \n",
        "                           dataSetTest[\"ImpliedVol\"], \n",
        "                           rIntegralSpline=riskFreeIntegral, \n",
        "                           qIntegralSpline=divSpreadIntegral,\n",
        "                           logMoneynessScale = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r9EvOdg0kU2P"
      },
      "source": [
        "### Standard network with soft constraints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O1UXinUrkUm_",
        "colab": {}
      },
      "source": [
        "def NNArchitectureVanillaSoftDupire(n_units, strikeTensor,\n",
        "                                    maturityTensor,\n",
        "                                    scaleTensor,\n",
        "                                    strikeMinTensor,\n",
        "                                    vegaRef,\n",
        "                                    hyperparameters,\n",
        "                                    IsTraining=True):\n",
        "  \n",
        "  inputLayer = tf.concat([strikeTensor,maturityTensor], axis=-1)\n",
        "  #First layer\n",
        "  hidden1 = unconstrainedLayer(n_units = n_units,\n",
        "                               tensor = inputLayer,\n",
        "                               isTraining=IsTraining, \n",
        "                               name = \"Hidden1\")\n",
        "  #Second layer\n",
        "  hidden2 = unconstrainedLayer(n_units = n_units,\n",
        "                               tensor = hidden1,\n",
        "                               isTraining=IsTraining, \n",
        "                               name = \"Hidden2\")\n",
        "  #Output layer\n",
        "  out = unconstrainedLayer(n_units = 1,\n",
        "                           tensor = hidden2,\n",
        "                           isTraining=IsTraining, \n",
        "                           name = \"Output\",\n",
        "                           activation = None)\n",
        "  #Local volatility \n",
        "  dupireVol, theta, hK, dupireVar = rawDupireFormula(out, strikeTensor,\n",
        "                                                     maturityTensor,\n",
        "                                                     scaleTensor,\n",
        "                                                     strikeMinTensor,\n",
        "                                                     IsTraining=IsTraining)\n",
        "  #Soft constraints for no arbitrage\n",
        "  lambdas = hyperparameters[\"lambdaSoft\"] \n",
        "  lowerBoundTheta = tf.constant(hyperparameters[\"lowerBoundTheta\"])\n",
        "  lowerBoundGamma = tf.constant(hyperparameters[\"lowerBoundGamma\"])\n",
        "  grad_penalty = lambdas * tf.reduce_mean(tf.nn.relu(-theta + lowerBoundTheta) / vegaRef)\n",
        "  hessian_penalty = lambdas * hyperparameters[\"lambdaGamma\"] * tf.reduce_mean(tf.nn.relu(-hK + lowerBoundGamma) / vegaRef)\n",
        "  \n",
        "  return out, [out, dupireVol, theta, hK, dupireVar], [grad_penalty, hessian_penalty], evalAndFormatDupireResult"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k-FDViM6keCA",
        "colab": {}
      },
      "source": [
        "y_pred4, volLocale4, dNN_T4, gNN_K4, lossSerie4 = create_train_model(NNArchitectureVanillaSoftDupire,\n",
        "                                                                     scaledDataSet,\n",
        "                                                                     False, \n",
        "                                                                     hyperparameters,\n",
        "                                                                     modelName = \"convexSoftDupireVolModel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YqrPGUcHkd_a",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerie4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PBoUrx83kd9W",
        "colab": {}
      },
      "source": [
        "lossSerie4.iloc[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI7jsuqPYvI1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred4, volLocale4, dNN_T4, gNN_K4, lossSerie4 = create_eval_model(NNArchitectureVanillaSoftDupire,\n",
        "                                                                    scaledDataSet, \n",
        "                                                                    False, \n",
        "                                                                    hyperparameters,\n",
        "                                                                    modelName = \"convexSoftDupireVolModel\")\n",
        "modelSummary(y_pred4, volLocale4, dNN_T4, gNN_K4, dataSet)\n",
        "impV4 = plotImpliedVol(y_pred4, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rwpPXhszkd6Z",
        "colab": {}
      },
      "source": [
        "volLocale4.loc[(midS0,slice(None))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xAfSOpREkdzc",
        "colab": {}
      },
      "source": [
        "y_pred4Test, volLocale4Test, dNN_T4Test, gNN_K4Test, lossSerie4Test = create_eval_model(NNArchitectureVanillaSoftDupire, \n",
        "                                                                                        scaledDataSetTest, \n",
        "                                                                                        False, \n",
        "                                                                                        hyperparameters,\n",
        "                                                                                        modelName = \"convexSoftDupireVolModel\")\n",
        "modelSummary(y_pred4Test, volLocale4Test, dNN_T4Test, gNN_K4Test, dataSetTest)\n",
        "impV4Test = plotImpliedVol(y_pred4Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OCZXZWCIkdxR",
        "colab": {}
      },
      "source": [
        "modelSummary(y_pred4, \n",
        "             volLocale4, \n",
        "             dNN_T4, \n",
        "             gNN_K4, \n",
        "             dataSet,\n",
        "             logMoneynessScale = True)\n",
        "impV4 = plotImpliedVol(y_pred4, \n",
        "                       dataSet[\"ImpliedVol\"], \n",
        "                       rIntegralSpline=riskFreeIntegral, \n",
        "                       qIntegralSpline=divSpreadIntegral,\n",
        "                       logMoneynessScale = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0SsMFMnokdu-",
        "colab": {}
      },
      "source": [
        "modelSummary(y_pred4Test, \n",
        "             volLocale4Test, \n",
        "             dNN_T4Test, \n",
        "             gNN_K4Test, \n",
        "             dataSetTest,\n",
        "             logMoneynessScale = True)\n",
        "impV4Test = plotImpliedVol(y_pred4Test, \n",
        "                           dataSetTest[\"ImpliedVol\"], \n",
        "                           rIntegralSpline=riskFreeIntegral, \n",
        "                           qIntegralSpline=divSpreadIntegral,\n",
        "                           logMoneynessScale = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9LHmCnZikdr1",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Atqa9I8Gkdn-",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_eJG7_513hNC"
      },
      "source": [
        "### Unconstrained standard network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6skBo-i-JqOx",
        "colab": {}
      },
      "source": [
        "def NNArchitectureUnconstrainedDupire(n_units, strikeTensor,\n",
        "                                      maturityTensor,\n",
        "                                      scaleTensor,\n",
        "                                      strikeMinTensor, \n",
        "                                      vegaRef,\n",
        "                                      hyperparameters,\n",
        "                                      IsTraining=True):\n",
        "  \n",
        "  inputLayer = tf.concat([strikeTensor,maturityTensor], axis=-1)\n",
        "  \n",
        "  #First layer\n",
        "  hidden1 = unconstrainedLayer(n_units = n_units,\n",
        "                               tensor = inputLayer,\n",
        "                               isTraining=IsTraining, \n",
        "                               name = \"Hidden1\")\n",
        "  #Second layer\n",
        "  hidden2 = unconstrainedLayer(n_units = n_units,\n",
        "                               tensor = hidden1,\n",
        "                               isTraining=IsTraining, \n",
        "                               name = \"Hidden2\")\n",
        "  #Ouput layer\n",
        "  out = unconstrainedLayer(n_units = 1,\n",
        "                           tensor = hidden2,\n",
        "                           isTraining=IsTraining, \n",
        "                           name = \"Output\",\n",
        "                           activation = None)\n",
        "  #Local volatility\n",
        "  dupireVol, theta, hK, dupireVar = rawDupireFormula(out, strikeTensor,\n",
        "                                                     maturityTensor,\n",
        "                                                     scaleTensor,\n",
        "                                                     strikeMinTensor,\n",
        "                                                     IsTraining=IsTraining)\n",
        "  \n",
        "  return out, [out, dupireVol, theta, hK, dupireVar], [], evalAndFormatDupireResult"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cIEIgBTp3mFn",
        "colab": {}
      },
      "source": [
        "y_pred5, volLocale5, dNN_T5, gNN_K5, lossSerie5 = create_train_model(NNArchitectureUnconstrainedDupire,\n",
        "                                                                     scaledDataSet,\n",
        "                                                                     False, \n",
        "                                                                     hyperparameters,\n",
        "                                                                     modelName = \"unconstrainedDupireVolModel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "evKBtgfB3mBj",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerie5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X8qTeMJm3l-i",
        "colab": {}
      },
      "source": [
        "lossSerie5.iloc[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odwXtVHNYvJH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred5, volLocale5, dNN_T5, gNN_K5, lossSerie5 = create_eval_model(NNArchitectureUnconstrainedDupire,\n",
        "                                                                    scaledDataSet,\n",
        "                                                                    False,\n",
        "                                                                    hyperparameters,\n",
        "                                                                    modelName = \"unconstrainedDupireVolModel\")\n",
        "modelSummary(y_pred5, volLocale5, dNN_T5, gNN_K5, dataSet)\n",
        "impV5 = plotImpliedVol(y_pred5, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a306_WrY3l7R",
        "colab": {}
      },
      "source": [
        "volLocale5.loc[(midS0,slice(None))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1vsHhlAW3lpu",
        "colab": {}
      },
      "source": [
        "y_pred5Test, volLocale5Test, dNN_T5Test, gNN_K5Test, lossSerie5Test = create_eval_model(NNArchitectureUnconstrainedDupire, \n",
        "                                                                                        scaledDataSetTest, \n",
        "                                                                                        False, \n",
        "                                                                                        hyperparameters,\n",
        "                                                                                        modelName = \"unconstrainedDupireVolModel\")\n",
        "modelSummary(y_pred5Test, volLocale5Test, dNN_T5Test, gNN_K5Test, dataSetTest)\n",
        "impV5Test = plotImpliedVol(y_pred5Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cUB3Y62YJpwo",
        "colab": {}
      },
      "source": [
        "modelSummary(y_pred5, \n",
        "             volLocale5, \n",
        "             dNN_T5, \n",
        "             gNN_K5, \n",
        "             dataSet,\n",
        "             logMoneynessScale = True)\n",
        "impV5 = plotImpliedVol(y_pred5, \n",
        "                       dataSet[\"ImpliedVol\"], \n",
        "                       rIntegralSpline=riskFreeIntegral, \n",
        "                       qIntegralSpline=divSpreadIntegral,\n",
        "                       logMoneynessScale = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kzo0lHB9e8pN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelSummary(y_pred5Test, \n",
        "             volLocale5Test, \n",
        "             dNN_T5Test, \n",
        "             gNN_K5Test, \n",
        "             dataSetTest,\n",
        "             logMoneynessScale = True)\n",
        "impV5Test = plotImpliedVol(y_pred5Test, \n",
        "                           dataSetTest[\"ImpliedVol\"], \n",
        "                           rIntegralSpline=riskFreeIntegral, \n",
        "                           qIntegralSpline=divSpreadIntegral,\n",
        "                           logMoneynessScale = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MrvDySFQWK-D"
      },
      "source": [
        "### Hard constrained architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1R2iPlotWQoH",
        "colab": {}
      },
      "source": [
        "#Tools functions for hard constrained neural architecture\n",
        "\n",
        "def convexLayerHard(n_units, tensor, isTraining, name, isNonDecreasing = True):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
        "                            units=n_units,\n",
        "                            kernel_constraint = tf.keras.constraints.NonNeg(), \n",
        "                            kernel_initializer=tf.keras.initializers.glorot_normal())\n",
        "    \n",
        "    \n",
        "    return tf.nn.softplus(layer), layer \n",
        "\n",
        "def monotonicLayerHard(n_units,  tensor, isTraining, name):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor, \n",
        "                            units=n_units,\n",
        "                            kernel_constraint = tf.keras.constraints.NonNeg(), \n",
        "                            kernel_initializer=tf.keras.initializers.glorot_normal())\n",
        "    \n",
        "    \n",
        "    \n",
        "    return tf.nn.sigmoid(layer),layer\n",
        "\n",
        "def convexOutputLayerHard(n_units, tensor, isTraining, name, isNonDecreasing = True):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
        "                            units=2*n_units,\n",
        "                            kernel_constraint = tf.keras.constraints.NonNeg(), \n",
        "                            kernel_initializer=tf.keras.initializers.glorot_normal(),\n",
        "                            activation = 'softplus') \n",
        "    \n",
        "     \n",
        "    layer = tf.layers.dense(layer, \n",
        "                            units=1,\n",
        "                            kernel_constraint = tf.keras.constraints.NonNeg(), \n",
        "                            kernel_initializer=positiveKernelInitializer, \n",
        "                            activation = 'softplus')\n",
        "    \n",
        "    return layer, layer \n",
        "  \n",
        "\n",
        "def convexLayerHybridHard(n_units,\n",
        "                          tensor,\n",
        "                          isTraining,\n",
        "                          name,\n",
        "                          activationFunction2 = Act.softplus,\n",
        "                          activationFunction1 = Act.exponential,\n",
        "                          isNonDecreasing = True):\n",
        "  with tf.name_scope(name):\n",
        "    layer = tf.layers.dense(tensor if isNonDecreasing else (- tensor), \n",
        "                            units=n_units,\n",
        "                            kernel_constraint = tf.keras.constraints.NonNeg(), \n",
        "                            kernel_initializer=positiveKernelInitializer)\n",
        "    l1,l2 = tf.split(layer,2,1)\n",
        "    output = tf.concat([activationFunction1(l1),activationFunction2(l2)],axis=-1)\n",
        "    return output , layer\n",
        "\n",
        "def sigmoidGradientHard(inputTensor):\n",
        "  return tf.nn.sigmoid(inputTensor) * ( 1 - tf.nn.sigmoid(inputTensor) )\n",
        "\n",
        "def sigmoidHessianHard(inputTensor) :\n",
        "  return (tf.square(1 - tf.nn.sigmoid(inputTensor)) -\n",
        "          tf.nn.sigmoid(inputTensor) * (1 - tf.nn.sigmoid(inputTensor)))\n",
        "  \n",
        "\n",
        "\n",
        "  \n",
        "def NNArchitectureHardConstrainedDupire(n_units, strikeTensor, \n",
        "                                        maturityTensor,\n",
        "                                        scaleTensor,\n",
        "                                        strikeMinTensor, \n",
        "                                        vegaRef,\n",
        "                                        hyperparameters,\n",
        "                                        IsTraining=True):\n",
        "  #First layer\n",
        "  hidden1S, layer1S = convexLayerHard(n_units = n_units,\n",
        "                                      tensor = strikeTensor,\n",
        "                                      isTraining=IsTraining,\n",
        "                                      name = \"Hidden1S\")\n",
        "  \n",
        "  hidden1M,layer1M = monotonicLayerHard(n_units = n_units,\n",
        "                                        tensor = maturityTensor,\n",
        "                                        isTraining = IsTraining,\n",
        "                                        name = \"Hidden1M\")\n",
        "  \n",
        "  hidden1 = tf.concat([hidden1S, hidden1M], axis=-1)\n",
        "  \n",
        "  #Second layer and output layer\n",
        "  out, layer = convexOutputLayerHard(n_units = n_units,\n",
        "                                     tensor = hidden1,\n",
        "                                     isTraining = IsTraining,\n",
        "                                     name = \"Output\")\n",
        "  #Local volatility\n",
        "  dupireVol, theta, hK, dupireVar = rawDupireFormula(out, strikeTensor,\n",
        "                                                     maturityTensor,\n",
        "                                                     scaleTensor,\n",
        "                                                     strikeMinTensor,\n",
        "                                                     IsTraining=IsTraining)\n",
        "  \n",
        "  return out, [out, dupireVol, theta, hK, dupireVar], [], evalAndFormatDupireResult"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ruwRhj_-WQlh",
        "colab": {}
      },
      "source": [
        "y_pred6, volLocale6, dNN_T6, gNN_K6, lossSerie6 = create_train_model(NNArchitectureHardConstrainedDupire,\n",
        "                                                                     scaledDataSet,\n",
        "                                                                     False, \n",
        "                                                                     hyperparameters,\n",
        "                                                                     modelName = \"convexHardDupireVolModel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W0GD3xYRWQif",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerie6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XF5Iq9ZYWQf6",
        "colab": {}
      },
      "source": [
        "lossSerie6.iloc[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRpdkOKMYvJQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred6, volLocale6, dNN_T6, gNN_K6, lossSerie6 = create_eval_model(NNArchitectureHardConstrainedDupire, \n",
        "                                                                    scaledDataSet, \n",
        "                                                                    False, \n",
        "                                                                    hyperparameters,\n",
        "                                                                    modelName = \"convexHardDupireVolModel\")\n",
        "modelSummary(y_pred6, volLocale6, dNN_T6, gNN_K6, dataSet)\n",
        "impV6 = plotImpliedVol(y_pred6, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-oZmufhXWQdk",
        "colab": {}
      },
      "source": [
        "volLocale6.loc[(midS0,slice(None))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_NKwHrxdWQWG",
        "colab": {}
      },
      "source": [
        "y_pred6Test, volLocale6Test, dNN_T6Test, gNN_K6Test, lossSerie6Test = create_eval_model(NNArchitectureHardConstrainedDupire, \n",
        "                                                                                        scaledDataSetTest, \n",
        "                                                                                        False, \n",
        "                                                                                        hyperparameters,\n",
        "                                                                                        modelName = \"convexHardDupireVolModel\")\n",
        "modelSummary(y_pred6Test, volLocale6Test, dNN_T6Test, gNN_K6Test, dataSetTest)\n",
        "impV6Test = plotImpliedVol(y_pred6Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8W6gaoaKWQTS",
        "colab": {}
      },
      "source": [
        "modelSummary(y_pred6, \n",
        "             volLocale6, \n",
        "             dNN_T6, \n",
        "             gNN_K6, \n",
        "             dataSet)\n",
        "impV6 = plotImpliedVol(y_pred6, \n",
        "                       dataSet[\"ImpliedVol\"], \n",
        "                       rIntegralSpline=riskFreeIntegral, \n",
        "                       qIntegralSpline=divSpreadIntegral,\n",
        "                       logMoneynessScale = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTPL26K4dYLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelSummary(y_pred6Test, \n",
        "             volLocale6Test, \n",
        "             dNN_T6Test, \n",
        "             gNN_K6Test, \n",
        "             dataSetTest)\n",
        "impV6Test = plotImpliedVol(y_pred6Test, \n",
        "                           dataSetTest[\"ImpliedVol\"], \n",
        "                           rIntegralSpline=riskFreeIntegral, \n",
        "                           qIntegralSpline=divSpreadIntegral,\n",
        "                           logMoneynessScale = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n_iWhfZjZJt7"
      },
      "source": [
        "## Dupire regularization \n",
        "\n",
        "Same lines as above except that dupire regularization is now activated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NZBPdHPuB0CX"
      },
      "source": [
        "### Hybrid architecture (Exact derivatives)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hmbg_HciZNaW",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CMQDARTbZTM8",
        "colab": {}
      },
      "source": [
        "y_pred8, volLocale8, dNN_T8, gNN_K8, lossSerie8 = create_train_model(NNArchitectureConstrainedDupire,\n",
        "                                                                     scaledDataSet,\n",
        "                                                                     True, \n",
        "                                                                     hyperparameters,\n",
        "                                                                     modelName = \"regularizedConvexHybridMatthewDupireVolModel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "phELMC43ZS-4",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerie8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "elgx3ip3ZS24",
        "colab": {}
      },
      "source": [
        "lossSerie8.iloc[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5kIlaICuZSzV",
        "colab": {}
      },
      "source": [
        "y_pred8, volLocale8, dNN_T8, gNN_K8, lossSerie8 = create_eval_model(NNArchitectureConstrainedDupire, \n",
        "                                                                    scaledDataSet, \n",
        "                                                                    True, \n",
        "                                                                    hyperparameters,\n",
        "                                                                    modelName = \"regularizedConvexHybridMatthewDupireVolModel\")\n",
        "modelSummary(y_pred8, volLocale8, dNN_T8, gNN_K8, dataSet)\n",
        "impV8 = plotImpliedVol(y_pred8, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "njoK6xVa1ReM",
        "colab": {}
      },
      "source": [
        "y_pred8Test, volLocale8Test, dNN_T8Test, gNN_K8Test, lossSerie8Test = create_eval_model(NNArchitectureConstrainedDupire, \n",
        "                                                                                        scaledDataSetTest, \n",
        "                                                                                        True, \n",
        "                                                                                        hyperparameters,\n",
        "                                                                                        modelName = \"regularizedConvexHybridMatthewDupireVolModel\")\n",
        "modelSummary(y_pred8Test, volLocale8Test, dNN_T8Test, gNN_K8Test, dataSetTest)\n",
        "impV8Test = plotImpliedVol(y_pred8Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NB3_-zutBt2y"
      },
      "source": [
        "### Unconstrained standard network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xYrYMhOZZ3Pd",
        "colab": {}
      },
      "source": [
        "y_pred9, volLocale9, dNN_T9, gNN_K9, lossSerie9 = create_train_model(NNArchitectureUnconstrainedDupire,\n",
        "                                                                     scaledDataSet,\n",
        "                                                                     True, \n",
        "                                                                     hyperparameters,\n",
        "                                                                     modelName = \"regularizedUnconstrainedDupireVolModel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wHi2doJQBbZO",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerie9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "76G0tLeSBdeD",
        "colab": {}
      },
      "source": [
        "lossSerie9.iloc[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AYzkOXUABdXV",
        "colab": {}
      },
      "source": [
        "y_pred9, volLocale9, dNN_T9, gNN_K9, lossSerie9 = create_eval_model(NNArchitectureUnconstrainedDupire, \n",
        "                                                                    scaledDataSet, \n",
        "                                                                    True, \n",
        "                                                                    hyperparameters,\n",
        "                                                                    modelName = \"regularizedUnconstrainedDupireVolModel\")\n",
        "modelSummary(y_pred9, volLocale9, dNN_T9, gNN_K9, dataSet)\n",
        "impV9 = plotImpliedVol(y_pred9, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t9Zvk6DAbAUn",
        "colab": {}
      },
      "source": [
        "y_pred9Test, volLocale9Test, dNN_T9Test, gNN_K9Test, lossSerie9Test = create_eval_model(NNArchitectureUnconstrainedDupire, \n",
        "                                                                                        scaledDataSetTest, \n",
        "                                                                                        True, \n",
        "                                                                                        hyperparameters,\n",
        "                                                                                        modelName = \"regularizedUnconstrainedDupireVolModel\")\n",
        "modelSummary(y_pred9Test, volLocale9Test, dNN_T9Test, gNN_K9Test, dataSetTest)\n",
        "impV9Test = plotImpliedVol(y_pred9Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "--dUCOKu4c1v",
        "colab": {}
      },
      "source": [
        "modelSummary(y_pred9, \n",
        "             volLocale9, \n",
        "             dNN_T9, \n",
        "             gNN_K9, \n",
        "             dataSet,\n",
        "             logMoneynessScale = True)\n",
        "impV9 = plotImpliedVol(y_pred9, \n",
        "                       dataSet[\"ImpliedVol\"], \n",
        "                       rIntegralSpline=riskFreeIntegral, \n",
        "                       qIntegralSpline=divSpreadIntegral,\n",
        "                       logMoneynessScale = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNkdNccRcanb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelSummary(y_pred9Test, \n",
        "             volLocale9Test, \n",
        "             dNN_T9Test, \n",
        "             gNN_K9Test, \n",
        "             dataSetTest,\n",
        "             logMoneynessScale = True)\n",
        "impV9Test = plotImpliedVol(y_pred9Test, \n",
        "                           dataSetTest[\"ImpliedVol\"], \n",
        "                           rIntegralSpline=riskFreeIntegral, \n",
        "                           qIntegralSpline=divSpreadIntegral,\n",
        "                           logMoneynessScale = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5yP7BP6avj5q"
      },
      "source": [
        "### Hybrid Network (Derivatives from algorithmic differentiation) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9u0zPfA6vpA2",
        "colab": {}
      },
      "source": [
        "y_pred10, volLocale10, dNN_T10, gNN_K10, lossSerie10 = create_train_model(NNArchitectureConstrainedRawDupire,\n",
        "                                                                          scaledDataSet,\n",
        "                                                                          True,\n",
        "                                                                          hyperparameters,\n",
        "                                                                          modelName = \"regularizedConvexHybridDupireVolModel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9t_BmpjTvpnU",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerie10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sKSBvFrzvqe8",
        "colab": {}
      },
      "source": [
        "lossSerie10.iloc[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh00YJyVYvJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred10, volLocale10, dNN_T10, gNN_K10, lossSerie10 = create_eval_model(NNArchitectureConstrainedRawDupire,\n",
        "                                                                         scaledDataSet,\n",
        "                                                                         True,\n",
        "                                                                         hyperparameters,\n",
        "                                                                         modelName = \"regularizedConvexHybridDupireVolModel\")\n",
        "modelSummary(y_pred10, volLocale10, dNN_T10, gNN_K10, dataSet)\n",
        "impV10 = plotImpliedVol(y_pred10, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VADXbcv2vqBp",
        "colab": {}
      },
      "source": [
        "y_pred10Test, volLocale10Test, dNN_T10Test, gNN_K10Test, lossSerie10Test = create_eval_model(NNArchitectureConstrainedRawDupire,\n",
        "                                                                                             scaledDataSetTest,\n",
        "                                                                                             True,\n",
        "                                                                                             hyperparameters,\n",
        "                                                                                             modelName = \"regularizedConvexHybridDupireVolModel\")\n",
        "modelSummary(y_pred10Test, volLocale10Test, dNN_T10Test, gNN_K10Test, dataSetTest)\n",
        "impV10Test = plotImpliedVol(y_pred10Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2bBB7MRVXmli",
        "colab": {}
      },
      "source": [
        "modelSummary(y_pred10, \n",
        "             volLocale10, \n",
        "             dNN_T10, \n",
        "             gNN_K10, \n",
        "             dataSet,\n",
        "             logMoneynessScale = True)\n",
        "impV10 = plotImpliedVol(y_pred10, \n",
        "                        dataSet[\"ImpliedVol\"], \n",
        "                        rIntegralSpline=riskFreeIntegral, \n",
        "                        qIntegralSpline=divSpreadIntegral,\n",
        "                        logMoneynessScale = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjfVGDKab29-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelSummary(y_pred10Test, \n",
        "             volLocale10Test, \n",
        "             dNN_T10Test, \n",
        "             gNN_K10Test, \n",
        "             dataSetTest,\n",
        "             logMoneynessScale = True)\n",
        "impV10Test = plotImpliedVol(y_pred10Test, \n",
        "                            dataSetTest[\"ImpliedVol\"], \n",
        "                            rIntegralSpline=riskFreeIntegral, \n",
        "                            qIntegralSpline=divSpreadIntegral,\n",
        "                            logMoneynessScale = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nl9Gq8s4o_DH"
      },
      "source": [
        "### Standard network with soft constraints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ck5is8QXicGb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "soE7FV4GpEFV",
        "colab": {}
      },
      "source": [
        "y_pred11, volLocale11, dNN_T11, gNN_K11, lossSerie11 = create_train_model(NNArchitectureVanillaSoftDupire,\n",
        "                                                                          scaledDataSet,\n",
        "                                                                          True,\n",
        "                                                                          hyperparameters,\n",
        "                                                                          modelName = \"regularizedConvexSoftDupireVolModel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "999cJ67npECl",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerie11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KRVyq4jRpEAJ",
        "colab": {}
      },
      "source": [
        "lossSerie11.iloc[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkCAmIkx9eSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fbu9gPcdYvJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred11, volLocale11, dNN_T11, gNN_K11, lossSerie11 = create_eval_model(NNArchitectureVanillaSoftDupire,\n",
        "                                                                         scaledDataSet,\n",
        "                                                                         True,\n",
        "                                                                         hyperparameters,\n",
        "                                                                         modelName = \"regularizedConvexSoftDupireVolModel\")\n",
        "modelSummary(y_pred11, volLocale11, dNN_T11, gNN_K11, dataSet)\n",
        "impV11 = plotImpliedVol(y_pred11, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLcb8QJdFdMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred11Test, volLocale11Test, dNN_T11Test, gNN_K11Test, lossSerie11Test = create_eval_model(NNArchitectureVanillaSoftDupire,\n",
        "                                                                                             scaledDataSetTest,\n",
        "                                                                                             True,\n",
        "                                                                                             hyperparameters,\n",
        "                                                                                             modelName = \"regularizedConvexSoftDupireVolModel\")\n",
        "modelSummary(y_pred11Test, volLocale11Test, dNN_T11Test, gNN_K11Test, dataSetTest)\n",
        "impV11Test = plotImpliedVol(y_pred11Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaHXPun1ibbS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelSummary(y_pred11, \n",
        "             volLocale11, \n",
        "             dNN_T11, \n",
        "             gNN_K11, \n",
        "             dataSet,\n",
        "             logMoneynessScale = True)\n",
        "impV11 = plotImpliedVol(y_pred11, \n",
        "                        dataSet[\"ImpliedVol\"], \n",
        "                        rIntegralSpline=riskFreeIntegral, \n",
        "                        qIntegralSpline=divSpreadIntegral,\n",
        "                        logMoneynessScale = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbXvzI1BXlRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelSummary(y_pred11Test, \n",
        "             volLocale11Test, \n",
        "             dNN_T11Test, \n",
        "             gNN_K11Test, \n",
        "             dataSetTest,\n",
        "             logMoneynessScale = True)\n",
        "impV11Test = plotImpliedVol(y_pred11Test, \n",
        "                            dataSetTest[\"ImpliedVol\"], \n",
        "                            rIntegralSpline=riskFreeIntegral, \n",
        "                            qIntegralSpline=divSpreadIntegral,\n",
        "                            logMoneynessScale = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MO8Iv9_cH6fS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "priceTrain = convertToLogMoneyness(dataSet[dataSet.Maturity > 0])[\"Price\"]\n",
        "plot2Series(convertToLogMoneyness(dataSetTest[dataSetTest.Maturity > 0])[\"Price\"], \n",
        "            priceTrain[priceTrain < 1500],\n",
        "            Title = \"Reference Price Surfaces\",\n",
        "            yMin = -1000,\n",
        "            az=140)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkukSDCmqTjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "priceTrain = convertToLogMoneyness(dataSet[dataSet.Maturity > 0])[\"Price\"]\n",
        "plotSerie(convertToLogMoneyness(dataSet[dataSet.Maturity > 0])[\"Price\"],\n",
        "          Title = \"Reference Price Surfaces\",\n",
        "          yMin = -1000,\n",
        "          az=140)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8BwviLCrfMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "priceTrain = convertToLogMoneyness(dataSet[dataSet.Maturity > 0])[\"Price\"]\n",
        "plot2Series(convertToLogMoneyness(dataSetTest[dataSetTest.Maturity > 0])[\"Price\"], \n",
        "            priceTrain,\n",
        "            Title = \"\",\n",
        "            yMin = -1000,\n",
        "            az=140)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OV7a-tVI2fK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "priceTrain = convertToLogMoneyness(y_pred11[y_pred11.index.get_level_values(\"Maturity\") > 0]) \n",
        "plot2Series(convertToLogMoneyness(y_pred11Test[y_pred11Test.index.get_level_values(\"Maturity\") > 0]), \n",
        "            priceTrain,\n",
        "            Title = '',\n",
        "            yMin = -1000,\n",
        "            az=140)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnFQih4ZaYZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "priceTrain = convertToLogMoneyness(dataSet)[\"Price\"]\n",
        "plot2Series(convertToLogMoneyness(y_pred11Test), \n",
        "            priceTrain,\n",
        "            Title = '',\n",
        "            yMin = -1000,\n",
        "            az=140)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAhDVOHRJDum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "volTrain = (impV11[impV11.index.get_level_values(\"Maturity\") > 0])\n",
        "plot2Series((impV11Test[impV11Test.index.get_level_values(\"Maturity\") > 0])[impV11Test > 0.05], \n",
        "            volTrain[volTrain < 0.3][volTrain > 0.05],\n",
        "            Title = \"Dense Soft Implied volatility Surfaces\",\n",
        "            yMin = -1000,\n",
        "            az=230)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MoFYYHsKJDlf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "volTrain = convertToLogMoneyness((dataSet[\"ImpliedVol\"][dataSet[\"ImpliedVol\"].index.get_level_values(\"Maturity\") > 0]))\n",
        "plot2Series((impV11Test[impV11Test.index.get_level_values(\"Maturity\") > 0])[impV11Test > 0.05], #convertToLogMoneyness((dataSetTest[\"ImpliedVol\"][dataSetTest[\"ImpliedVol\"].index.get_level_values(\"Maturity\") > 0])), \n",
        "            volTrain[volTrain < 0.3],\n",
        "            Title = \"Dense Soft Implied volatility Surfaces\",\n",
        "            yMin = -1000,\n",
        "            az=230)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pMVAzDEKXe2Z"
      },
      "source": [
        "### Hard constrained architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hLkTjQUOXkOj",
        "colab": {}
      },
      "source": [
        "y_pred12, volLocale12, dNN_T12, gNN_K12, lossSerie12 = create_train_model(NNArchitectureHardConstrainedDupire,\n",
        "                                                                          scaledDataSet,\n",
        "                                                                          True,\n",
        "                                                                          hyperparameters,\n",
        "                                                                          modelName = \"regularizedConvexHardDupireVolModel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "n9EwiocWXkLX",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerie12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JQzB6KPmXkJB",
        "colab": {}
      },
      "source": [
        "lossSerie12.iloc[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDI9ydh3YvJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred12, volLocale12, dNN_T12, gNN_K12, lossSerie12 = create_eval_model(NNArchitectureHardConstrainedDupire,\n",
        "                                                                         scaledDataSet,\n",
        "                                                                         True,\n",
        "                                                                         hyperparameters,\n",
        "                                                                         modelName = \"regularizedConvexHardDupireVolModel\")\n",
        "modelSummary(y_pred12, volLocale12, dNN_T12, gNN_K12, dataSet)\n",
        "impV12 = plotImpliedVol(y_pred12, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2548YwGSXj68",
        "colab": {}
      },
      "source": [
        "y_pred12Test, volLocale12Test, dNN_T12Test, gNN_K12Test, lossSerie12Test = create_eval_model(NNArchitectureHardConstrainedDupire,\n",
        "                                                                                             scaledDataSetTest,\n",
        "                                                                                             True,\n",
        "                                                                                             hyperparameters,\n",
        "                                                                                             modelName = \"regularizedConvexHardDupireVolModel\")\n",
        "modelSummary(y_pred12Test, volLocale12Test, dNN_T12Test, gNN_K12Test, dataSetTest)\n",
        "impV12Test = plotImpliedVol(y_pred12Test, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xccdFPbTXj3n",
        "colab": {}
      },
      "source": [
        "modelSummary(y_pred12, \n",
        "             volLocale12, \n",
        "             dNN_T12, \n",
        "             gNN_K12, \n",
        "             dataSet,\n",
        "             logMoneynessScale = True)\n",
        "impV12 = plotImpliedVol(y_pred12, \n",
        "                        dataSet[\"ImpliedVol\"], \n",
        "                        rIntegralSpline=riskFreeIntegral, \n",
        "                        qIntegralSpline=divSpreadIntegral,\n",
        "                        logMoneynessScale = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Nj3ve-crXj1B",
        "colab": {}
      },
      "source": [
        "modelSummary(y_pred12Test, \n",
        "             volLocale12Test, \n",
        "             dNN_T12Test, \n",
        "             gNN_K12Test, \n",
        "             dataSetTest,\n",
        "             logMoneynessScale = True)\n",
        "impV12Test = plotImpliedVol(y_pred12Test, \n",
        "                            dataSetTest[\"ImpliedVol\"], \n",
        "                            rIntegralSpline=riskFreeIntegral, \n",
        "                            qIntegralSpline=divSpreadIntegral,\n",
        "                            logMoneynessScale = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mu8QA9CmXjyg",
        "colab": {}
      },
      "source": [
        "priceTrain = convertToLogMoneyness(y_pred12[y_pred12.index.get_level_values(\"Maturity\") > 0]) \n",
        "plot2Series(convertToLogMoneyness(y_pred12Test[y_pred12Test.index.get_level_values(\"Maturity\") > 0]), \n",
        "            priceTrain,\n",
        "            Title = '',\n",
        "            yMin = -1000,\n",
        "            az=140)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gr7G5r7JXjrJ",
        "colab": {}
      },
      "source": [
        "volTrain = (impV12[impV12.index.get_level_values(\"Maturity\") > 0])\n",
        "plot2Series((impV12Test[impV12Test.index.get_level_values(\"Maturity\") > 0]), \n",
        "            volTrain[volTrain < 0.3],\n",
        "            Title = \"Hard Implied volatility Surfaces\",\n",
        "            yMin = -1000,\n",
        "            az=140)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TCvO25GfA2op",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DvLBkdlWjwzg"
      },
      "source": [
        "## Monte Carlo pricing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "s8feoYmarxmj"
      },
      "source": [
        "### Monte Carlo with implied vol"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N9KS6Vkro1Rg",
        "colab": {}
      },
      "source": [
        "nbTimeStep = 100\n",
        "nbPaths = 100000\n",
        "def MonteCarloPricerImplicit(S,\n",
        "                             Strike,\n",
        "                             Maturity,\n",
        "                             rSpline,\n",
        "                             divSpline,\n",
        "                             nbPaths,\n",
        "                             nbTimeStep,\n",
        "                             impliedVol):\n",
        "  time_grid = np.linspace(0, Maturity, int(nbTimeStep + 1))\n",
        "  timeStep = Maturity / nbTimeStep\n",
        "  gaussianNoise = np.random.normal(scale = np.sqrt(timeStep), size=(nbTimeStep, nbPaths))\n",
        "\n",
        "  logReturn = np.zeros((nbTimeStep + 1, nbPaths))\n",
        "  logReturn[0,:] = 0\n",
        "\n",
        "  for i in range(nbTimeStep) :\n",
        "      t = time_grid[i]\n",
        "\n",
        "      St = S0 * np.exp(logReturn[i,:])\n",
        "      volLocale = impliedVol\n",
        "\n",
        "      mu = rSpline(t) - divSpline(t)\n",
        "      drift = np.ones(nbPaths) * (mu - np.square(volLocale) / 2.0) \n",
        "      logReturn[i + 1, :] = logReturn[i,:] + drift * timeStep + gaussianNoise[i,:] * volLocale\n",
        "  SFinal = S0 * np.exp(logReturn[-1, :])\n",
        "  return np.mean(np.maximum(Strike - SFinal, 0))\n",
        "\n",
        "def MonteCarloPricerVectorizedImplicit(S,\n",
        "                                       dataSet,\n",
        "                                       rSpline,\n",
        "                                       divSpline,\n",
        "                                       nbPaths,\n",
        "                                       nbTimeStep):\n",
        "  func = lambda x : MonteCarloPricerImplicit(S, x[\"Strike\"], x[\"Maturity\"], riskCurvespline, divSpline, nbPaths, nbTimeStep, x[\"ImpliedVol\"])\n",
        "  return dataSet.apply(func, axis=1) * np.exp(-riskFreeIntegral(dataSet.index.get_level_values(\"Maturity\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q1E6lHkPpYXD",
        "colab": {}
      },
      "source": [
        "mcResRef = MonteCarloPricerVectorizedImplicit(S0[0],\n",
        "                                              dataSet,\n",
        "                                              riskCurvespline,\n",
        "                                              divSpline,\n",
        "                                              nbPaths,\n",
        "                                              nbTimeStep)\n",
        "mcResRef.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iKWRzDyTpYTh",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResRef, dataSet[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H8MJ9XlYr2My"
      },
      "source": [
        "### Monte Carlo local volatility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3n8evyIxsB2W"
      },
      "source": [
        "#### Constant local volatility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qnZBKekABhBm",
        "colab": {}
      },
      "source": [
        "def volLocaleTest(S, T):\n",
        "  return np.ones_like(S) * 0.23"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JdkQpONK4k7H",
        "colab": {}
      },
      "source": [
        "nbTimeStep = 100\n",
        "nbPaths = 100000\n",
        "def MonteCarloPricer(S, \n",
        "                     Strike, \n",
        "                     Maturity, \n",
        "                     rSpline, \n",
        "                     divSpline, \n",
        "                     nbPaths, \n",
        "                     nbTimeStep, \n",
        "                     volLocaleFunction):\n",
        "  time_grid = np.linspace(0, Maturity, int(nbTimeStep + 1))\n",
        "  timeStep = Maturity / nbTimeStep\n",
        "  gaussianNoise = np.random.normal(scale = np.sqrt(timeStep), size=(nbTimeStep, nbPaths))\n",
        "\n",
        "  logReturn = np.zeros((nbTimeStep + 1, nbPaths))\n",
        "  logReturn[0,:] = 0\n",
        "\n",
        "  for i in range(nbTimeStep) :\n",
        "      t = time_grid[i]\n",
        "\n",
        "      St = S0 * np.exp(logReturn[i,:])\n",
        "      volLocale = volLocaleFunction(St, np.ones(nbPaths) * t)\n",
        "\n",
        "      mu = rSpline(t) - divSpline(t)\n",
        "      drift = np.ones(nbPaths) * (mu - np.square(volLocale) / 2.0) \n",
        "      logReturn[i + 1, :] = logReturn[i,:] + drift * timeStep + gaussianNoise[i,:] * volLocale\n",
        "  SFinal = S0 * np.exp(logReturn[-1, :])\n",
        "  return np.mean(np.maximum(Strike - SFinal, 0))\n",
        "\n",
        "def MonteCarloPricerVectorized(S, \n",
        "                               dataSet,\n",
        "                               rSpline, \n",
        "                               divSpline, \n",
        "                               nbPaths, \n",
        "                               nbTimeStep, \n",
        "                               volLocaleFunction):\n",
        "  func = lambda x : MonteCarloPricer(S, x[\"Strike\"], x[\"Maturity\"], riskCurvespline, divSpline, nbPaths, nbTimeStep, volLocaleFunction)\n",
        "  return dataSet.apply(func, axis=1) * np.exp(-riskFreeIntegral(dataSet.index.get_level_values(\"Maturity\")))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mYHvsquue2JO",
        "colab": {}
      },
      "source": [
        "mcResSigmaRef = MonteCarloPricerVectorized(S0[0],\n",
        "                                           dataSetTest,\n",
        "                                           riskCurvespline,\n",
        "                                           divSpline,\n",
        "                                           nbPaths,\n",
        "                                           nbTimeStep,\n",
        "                                           volLocaleTest)\n",
        "mcResSigmaRef.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jJR0sXsUmCm2",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResSigmaRef, dataSetTest[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltaanlj5-uTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mcResSigmaRef.to_csv(\"mcResSigmaRef.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8ysLo9wxsTOj"
      },
      "source": [
        "#### Extracting neural local volatility\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MV4SmwctjbjS",
        "colab": {}
      },
      "source": [
        "def evalVolLocale(NNFactory,\n",
        "                  strikes,\n",
        "                  maturities,\n",
        "                  dataSet,\n",
        "                  hyperParameters,\n",
        "                  modelName = \"bestModel\"):\n",
        "    \n",
        "    hidden_nodes = hyperParameters[\"nbUnits\"] \n",
        "\n",
        "    # Reset the graph\n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    # Placeholders for input and output data   \n",
        "    Strike = tf.placeholder(tf.float32,[None,1])\n",
        "    Maturity = tf.placeholder(tf.float32,[None,1])\n",
        "    factorPrice = tf.placeholder(tf.float32,[None,1])\n",
        "    y = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='y')\n",
        "    vegaRef = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='vegaRef')\n",
        "    learningRateTensor = tf.placeholder(tf.float32,[])\n",
        "    \n",
        "    #Get scaling for strike\n",
        "    colStrikeIndex = dataSet.columns.get_loc(\"ChangedStrike\")\n",
        "    maxColFunction = scaler.data_max_[colStrikeIndex]\n",
        "    minColFunction = scaler.data_min_[colStrikeIndex]\n",
        "    scF = (maxColFunction - minColFunction) \n",
        "    scaleTensor = tf.constant(scF, dtype=tf.float32)\n",
        "    strikeMinTensor = tf.constant(minColFunction, dtype=tf.float32)\n",
        "\n",
        "    price_pred_tensor = None\n",
        "    TensorList = None\n",
        "    penalizationList = None \n",
        "    formattingFunction = None\n",
        "    price_pred_tensor, TensorList, penalizationList, formattingFunction = NNFactory(hidden_nodes,\n",
        "                                                                                    Strike,\n",
        "                                                                                    Maturity,\n",
        "                                                                                    scaleTensor,\n",
        "                                                                                    strikeMinTensor,\n",
        "                                                                                    vegaRef,\n",
        "                                                                                    hyperParameters,\n",
        "                                                                                    IsTraining=False)# one hidden layer\n",
        "\n",
        "\n",
        "    price_pred_tensor_sc= tf.multiply(factorPrice,price_pred_tensor)\n",
        "    TensorList[0] = price_pred_tensor_sc\n",
        "    \n",
        "    # Define a loss function\n",
        "    pointwiseError = tf.reduce_mean(tf.abs(price_pred_tensor_sc - y) / vegaRef)\n",
        "    errors = tf.add_n([pointwiseError] + penalizationList) \n",
        "    loss = tf.log(tf.reduce_mean(errors))\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRateTensor)\n",
        "    train = optimizer.minimize(loss)\n",
        "\n",
        "    # Initialize variables and run session\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver()\n",
        "    sess = tf.Session()\n",
        "    sess.run(init)\n",
        "    n = strikes.shape[0]\n",
        "    changedVar = changeOfVariable(strikes, maturities)\n",
        "    scaledStrike = (changedVar[0]-minColFunction)/scF\n",
        "    dividendFactor = changedVar[1]\n",
        "\n",
        "    def createFeedDict(s, t, d):\n",
        "        batchSize = s.shape[0]\n",
        "        feedDict = {Strike : np.reshape(s, (batchSize,1)), \n",
        "                    Maturity : np.reshape(t, (batchSize,1)) ,  \n",
        "                    factorPrice : np.reshape(d, (batchSize,1)), \n",
        "                    vegaRef : np.ones((batchSize,1))}\n",
        "        return feedDict\n",
        "    \n",
        "    epochFeedDict = createFeedDict(scaledStrike, maturities, dividendFactor)\n",
        "    \n",
        "    saver.restore(sess, modelName)  \n",
        "\n",
        "    evalList = sess.run(TensorList, feed_dict=epochFeedDict)\n",
        "    \n",
        "    sess.close()\n",
        "    \n",
        "    return pd.Series(evalList[1].flatten(), index = pd.MultiIndex.from_arrays([strikes, maturities], names=('Strike', 'Maturity')))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bG_opQ7DynZC",
        "colab": {}
      },
      "source": [
        "strikeLow = dataSet[\"Strike\"].min()#min(dataSet[\"Strike\"].min(),dataSetTest[\"Strike\"].min())\n",
        "strikeUp = dataSet[\"Strike\"].max()#max(dataSet[\"Strike\"].max(),dataSetTest[\"Strike\"].max())\n",
        "strikeGrid = np.linspace(strikeLow, strikeUp, 100)\n",
        "matLow = dataSet[\"Maturity\"].min()#min(dataSet[\"Maturity\"].min(),dataSetTest[\"Maturity\"].min())\n",
        "matUp = dataSet[\"Maturity\"].max()#max(dataSet[\"Maturity\"].max(),dataSetTest[\"Maturity\"].max())\n",
        "matGrid = np.linspace(matLow, matUp, 100)\n",
        "volLocaleGrid = np.meshgrid(strikeGrid, matGrid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4Yd7wL-ftbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oawxG7p4oGUA",
        "colab": {}
      },
      "source": [
        "def interpolatedMCLocalVolatility(localVol, strikes, maturities):\n",
        "    coordinates =  np.array( customInterpolator(localVol, strikes, maturities) ).flatten()  \n",
        "    return pd.Series(coordinates, index = pd.MultiIndex.from_arrays([strikes, maturities], names=('Strike', 'Maturity')))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rRwNc1rgM28W"
      },
      "source": [
        "##### Standard network, soft constraints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "QBw5bnz6QKTQ",
        "colab": {}
      },
      "source": [
        "def neuralVolLocale(s,t):\n",
        "  vLoc = evalVolLocale(NNArchitectureVanillaSoftDupire,\n",
        "                       s, t,\n",
        "                       dataSetTest,\n",
        "                       hyperparameters,\n",
        "                       modelName = \"convexSoftDupireVolModel\")\n",
        "  return vLoc.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UZwCXR8lzaQa",
        "colab": {}
      },
      "source": [
        "volLocalInterp = neuralVolLocale(volLocaleGrid[0].flatten(), \n",
        "                                 volLocaleGrid[1].flatten())\n",
        "volLocalInterp.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q0UQuuH3OHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "volLocalInterp.to_csv(\"Dense08082001VolLocalGrid.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OUVTy-tl9HBE",
        "colab": {}
      },
      "source": [
        "volLocalInterp2 = neuralVolLocale(dataSetTest.index.get_level_values(\"Strike\").values.flatten(), \n",
        "                                  dataSetTest.index.get_level_values(\"Maturity\").values.flatten())\n",
        "volLocalInterp2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8dutn-w3b0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "volLocalInterp.to_csv(\"Dense08082001dataSetTest.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m5rPNAOtNLGn",
        "colab": {}
      },
      "source": [
        "nnVolLocale = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp, x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9Amwp9SY-D1o",
        "colab": {}
      },
      "source": [
        "nnVolLocale2 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp2, x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HPeQZ53f0L46",
        "colab": {}
      },
      "source": [
        "plotSerie(volLocalInterp,\n",
        "          Title = 'Interpolated Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8OZjFTHezNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "plotSerie(nnVolLocale2(volLocaleGrid[0].flatten(), volLocaleGrid[1].flatten()),\n",
        "          Title = 'Interpolated Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-VTQGuG49sv_",
        "colab": {}
      },
      "source": [
        "plotSerie(volLocalInterp2,\n",
        "          Title = 'Testing Interpolated Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L_osDqp2mJbz",
        "colab": {}
      },
      "source": [
        "plotSerie(dataSetTest[\"locvol\"],\n",
        "          Title = 'Testing Tikhonov Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zAPjVEiEJk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plotSerie(dataSet[\"locvol\"],\n",
        "          Title = 'Tikohnov Train Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_xd_f6kpj6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plotSerie(neuralVolLocale(dataSet.index.get_level_values(\"Strike\").values.flatten(), dataSet.index.get_level_values(\"Maturity\").values.flatten()),\n",
        "          Title = 'Training Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cd5HBrfIgdR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plotSerie(localVolatility[\"LocalVolatility\"],\n",
        "          Title = 'Complete tikhonov Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB--pSuegkg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "localVolatility.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_sdUIUb_MwVJ"
      },
      "source": [
        "##### Hard constraint Regularized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4WESqBRzRGyt",
        "colab": {}
      },
      "source": [
        "def neuralVolLocaleHardReg(s,t):\n",
        "  vLoc = evalVolLocale(NNArchitectureHardConstrainedDupire,\n",
        "                       s, t,\n",
        "                       dataSet,\n",
        "                       hyperparameters,\n",
        "                       modelName = \"regularizedConvexHardDupireVolModel\")\n",
        "  return vLoc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "coZ0NmLduoR7",
        "colab": {}
      },
      "source": [
        "volLocalInterp3 = neuralVolLocaleHardReg(volLocaleGrid[0].flatten(),\n",
        "                                         volLocaleGrid[1].flatten())\n",
        "volLocalInterp3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rlLZq1p6RXc-",
        "colab": {}
      },
      "source": [
        "volLocalInterp4 = neuralVolLocaleHardReg(dataSetTest.index.get_level_values(\"Strike\").values.flatten(),\n",
        "                                         dataSetTest.index.get_level_values(\"Maturity\").values.flatten())\n",
        "volLocalInterp4.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bHQgKjjMRXZe",
        "colab": {}
      },
      "source": [
        "nnVolLocale3 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp3, x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eMy6YZzmRXXX",
        "colab": {}
      },
      "source": [
        "nnVolLocale4 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp4, x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bl72KtNERXT_",
        "colab": {}
      },
      "source": [
        "plotSerie(volLocalInterp3,\n",
        "          Title = 'Interpolated Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ILzOqJ0SRXOE",
        "colab": {}
      },
      "source": [
        "plotSerie(volLocalInterp4,\n",
        "          Title = 'Interpolated Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S9167YBhRpIY",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_RU2VfS9UcAN"
      },
      "source": [
        "##### Hard constraint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_fdam7WpUcAS",
        "colab": {}
      },
      "source": [
        "def neuralVolLocaleHard(s,t):\n",
        "  vLoc = evalVolLocale(NNArchitectureHardConstrainedDupire,\n",
        "                       s, t,\n",
        "                       dataSet,\n",
        "                       hyperparameters,\n",
        "                       modelName = \"convexHardDupireVolModel\")\n",
        "  return vLoc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "42NAiQ7OUcAd",
        "colab": {}
      },
      "source": [
        "volLocalInterp5 = neuralVolLocaleHard(volLocaleGrid[0].flatten(),\n",
        "                                      volLocaleGrid[1].flatten())\n",
        "volLocalInterp5.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1NZ9rNwyUcAj",
        "colab": {}
      },
      "source": [
        "volLocalInterp6 = neuralVolLocaleHard(dataSetTest.index.get_level_values(\"Strike\").values.flatten(),\n",
        "                                      dataSetTest.index.get_level_values(\"Maturity\").values.flatten())\n",
        "volLocalInterp6.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UjY8_upSUcAn",
        "colab": {}
      },
      "source": [
        "nnVolLocale5 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp5, x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A2uShCIhUcAr",
        "colab": {}
      },
      "source": [
        "nnVolLocale6 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp6, x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yDLVxVCsUcAu",
        "colab": {}
      },
      "source": [
        "plotSerie(volLocalInterp5,\n",
        "          Title = 'Interpolated Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4F5jBRtdUcAw",
        "colab": {}
      },
      "source": [
        "plotSerie(volLocalInterp6,\n",
        "          Title = 'Interpolated Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZojUS1lnUcAz",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CslgrmvEUcA1",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w96mzVNxRpFw",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LIWdTVy8tf59"
      },
      "source": [
        "#### Tikhonov local volatility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "odpuxjDbtlV0",
        "colab": {}
      },
      "source": [
        "nnTikhonov = lambda x,y : interpolatedMCLocalVolatility(localVolatility[\"LocalVolatility\"], x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cP0LG52vtlQ5",
        "colab": {}
      },
      "source": [
        "mcResTikhonov = MonteCarloPricerVectorized(S0[0],\n",
        "                                           dataSetTest,\n",
        "                                           riskCurvespline,\n",
        "                                           divSpline,\n",
        "                                           nbPaths,\n",
        "                                           nbTimeStep,\n",
        "                                           nnTikhonov)\n",
        "mcResTikhonov.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uH2OxQ0OtlNd",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResTikhonov, dataSetTest[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t65wX7-xtlKU",
        "colab": {}
      },
      "source": [
        "mcResTikhonov.to_csv(\"mcResTikhonov.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BSe4AmUJsLo-"
      },
      "source": [
        "#### Neural local Volatility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "z3Hrt5KOVt9S"
      },
      "source": [
        "##### Standard Network soft constraint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T0DdvwGtdKPZ",
        "colab": {}
      },
      "source": [
        "mcResVolLocale = MonteCarloPricerVectorized(S0[0],\n",
        "                                            dataSetTest,\n",
        "                                            riskCurvespline,\n",
        "                                            divSpline,\n",
        "                                            nbPaths,\n",
        "                                            nbTimeStep,\n",
        "                                            nnVolLocale)\n",
        "mcResVolLocale.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2NqMe9KEb4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mcResVolLocale.to_csv(\"mcResVolLocale.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aAslenyljg_w",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResVolLocale, dataSetTet[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "38OJqpkCfYaI",
        "colab": {}
      },
      "source": [
        "dataSetTest.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0eW1ICq6-RZ3",
        "colab": {}
      },
      "source": [
        "mcResVolLocale2 = MonteCarloPricerVectorized(S0[0],\n",
        "                                            dataSetTest,\n",
        "                                            riskCurvespline,\n",
        "                                            divSpline,\n",
        "                                            nbPaths,\n",
        "                                            nbTimeStep,\n",
        "                                            nnVolLocale2)\n",
        "mcResVolLocale2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jNmkLtZE-RPA",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResVolLocale2, dataSetTest[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "demJXGlD6V3U",
        "colab": {}
      },
      "source": [
        "mcResVolLocale2.to_csv(\"mcResVolLocale2.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pwupw4shWcqI"
      },
      "source": [
        "##### Hard constraint Regularized"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HtFZpARdWcqN",
        "colab": {}
      },
      "source": [
        "mcResVolLocale3 = MonteCarloPricerVectorized(S0[0],\n",
        "                                             dataSetTest,\n",
        "                                             riskCurvespline,\n",
        "                                             divSpline,\n",
        "                                             nbPaths,\n",
        "                                             nbTimeStep,\n",
        "                                             nnVolLocale3)\n",
        "mcResVolLocale3.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4JQvapLCWcqS",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResVolLocale3, dataSetTest[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FoR4evV3Wcqd",
        "colab": {}
      },
      "source": [
        "mcResVolLocale3.to_csv(\"mcResVolLocale3.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RbgkiRPeWcqY",
        "colab": {}
      },
      "source": [
        "mcResVolLocale4 = MonteCarloPricerVectorized(S0[0],\n",
        "                                             dataSetTest,\n",
        "                                             riskCurvespline,\n",
        "                                             divSpline,\n",
        "                                             nbPaths,\n",
        "                                             nbTimeStep,\n",
        "                                             nnVolLocale4)\n",
        "mcResVolLocale4.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sb6dt-iJWcqa",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResVolLocale4, dataSetTest[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTIQMwhREWsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mcResVolLocale4.to_csv(\"mcResVolLocale4.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z45IZtNnWfek"
      },
      "source": [
        "##### Hard constraint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2vCtUGRNWfel",
        "colab": {}
      },
      "source": [
        "mcResVolLocale5 = MonteCarloPricerVectorized(S0[0],\n",
        "                                             dataSetTest,\n",
        "                                             riskCurvespline,\n",
        "                                             divSpline,\n",
        "                                             nbPaths,\n",
        "                                             nbTimeStep,\n",
        "                                             nnVolLocale5)\n",
        "mcResVolLocale5.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1h3Ee3CcWfen",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResVolLocale5, dataSetTest[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFrj8iUVBVZx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mcResVolLocale5.to_csv(\"mcResVolLocale5.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XudXGmowWffz",
        "colab": {}
      },
      "source": [
        "mcResVolLocale6 = MonteCarloPricerVectorized(S0[0],\n",
        "                                             dataSetTest,\n",
        "                                             riskCurvespline,\n",
        "                                             divSpline,\n",
        "                                             nbPaths,\n",
        "                                             nbTimeStep,\n",
        "                                             nnVolLocale6)\n",
        "mcResVolLocale6.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oz5Rx-INWff1",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResVolLocale6, dataSetTest[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N0HxLb3apD4l",
        "colab": {}
      },
      "source": [
        "mcResVolLocale6.to_csv(\"mcResVolLocale6.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V4XXpLmjWff3",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39dhPnBxW-tI",
        "colab_type": "text"
      },
      "source": [
        "## Gatheral transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtubjBVY3vGh",
        "colab_type": "text"
      },
      "source": [
        "#### Select Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvG6upPF3yo0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainingDataSet = generateData(formattedTrainingData[\"ImpliedVol\"], S0, riskFreeIntegral, divSpreadIntegral, riskCurvespline, divSpline)\n",
        "trainingDataSet.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezNbsoYYP1II",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5mydcuC3-uA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainingDataSet[\"locvol\"] = interpolatedLocalVolatility(localVolatility, trainingDataSet[\"Price\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FVxFB8Y4Dma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataSet = trainingDataSet #Training set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC9cAGL14QjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = skl.preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
        "scaler.fit(dataSet)\n",
        "scaledDataSet = transformCustom(dataSet, scaler)\n",
        "scaledDataSetTest = transformCustom(dataSetTest, scaler)\n",
        "scaledDataSet.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wR4X6rz4TY1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Search strike for ATM option\n",
        "midS0 = dataSet[dataSet.index.get_level_values(\"Strike\") >= S0[0]].index.get_level_values(\"Strike\").min()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwG8y4zzVlLj",
        "colab_type": "text"
      },
      "source": [
        "#### Plot functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD9EbLyF0Eyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "  \n",
        "#Diagnose Price, theta, gamma and local volatility\n",
        "def modelSummaryGatheral(totalVariance,\n",
        "                         volLocale,\n",
        "                         delta_T,\n",
        "                         gamma_K,\n",
        "                         benchDataset,\n",
        "                         sigma=0.3,\n",
        "                         az=40,\n",
        "                         yMin = KMin,\n",
        "                         yMax = KMax,\n",
        "                         logMoneynessScale = False):\n",
        "  refDataset = benchDataset.loc[totalVariance.index]\n",
        "  if logMoneynessScale : \n",
        "    totalVariancePred = convertToLogMoneyness(totalVariance)\n",
        "    volLocalePred = convertToLogMoneyness(volLocale)\n",
        "    delta_TPred = convertToLogMoneyness(delta_T)\n",
        "    gKRefPred = convertToLogMoneyness(gamma_K)\n",
        "    benchDatasetScaled = convertToLogMoneyness(refDataset)\n",
        "    yMinScaled = np.log(S0[0]/yMax)\n",
        "    yMaxScaled = np.log(S0[0]/yMin)\n",
        "    azimutIncrement = 180\n",
        "  else : \n",
        "    totalVariancePred = totalVariance\n",
        "    volLocalePred = volLocale\n",
        "    delta_TPred = delta_T\n",
        "    gKRefPred = gamma_K\n",
        "    benchDatasetScaled = refDataset\n",
        "    yMinScaled = yMin\n",
        "    yMaxScaled = yMax\n",
        "    azimutIncrement = 0\n",
        "    \n",
        "  priceRef = benchDatasetScaled[\"impliedTotalVariance\"]\n",
        "  predictionDiagnosis(totalVariancePred, \n",
        "                      priceRef, \n",
        "                      \"Implied Variance\",\n",
        "                      az=320 + azimutIncrement,\n",
        "                      yMin = yMinScaled,\n",
        "                      yMax = yMaxScaled)\n",
        "  \n",
        "  volLocaleRef = benchDatasetScaled[\"locvol\"]\n",
        "  predictionDiagnosis(volLocalePred, \n",
        "                      volLocaleRef, \n",
        "                      \"Local volatility\",\n",
        "                      az=az + azimutIncrement,\n",
        "                      yMin = yMinScaled,\n",
        "                      yMax = yMaxScaled)\n",
        "  \n",
        "  impliedVolPred = np.sqrt(totalVariancePred) #np.sqrt(totalVariance / refDataset[\"Maturity\"])\n",
        "  predictionDiagnosis(impliedVolPred, \n",
        "                      benchDatasetScaled[\"ImpliedVol\"], \n",
        "                      \"Implied volatility\",\n",
        "                      az=az + azimutIncrement,\n",
        "                      yMin = yMinScaled,\n",
        "                      yMax = yMaxScaled)\n",
        "  \n",
        "  dTRef = benchDatasetScaled[\"Theta\"]\n",
        "  predictionDiagnosis(delta_TPred, \n",
        "                      dTRef, \n",
        "                      \"Theta\",\n",
        "                      az=340 + azimutIncrement,\n",
        "                      yMin = yMinScaled,\n",
        "                      yMax = yMaxScaled)\n",
        "  \n",
        "  gKRef = benchDatasetScaled[\"Gamma Strike\"]\n",
        "  predictionDiagnosis(gKRefPred, \n",
        "                      gKRef, \n",
        "                      \"Gamma Strike\",\n",
        "                      az=340 + azimutIncrement,\n",
        "                      yMin = yMinScaled,\n",
        "                      yMax = yMaxScaled)\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sie49ZXVqJA",
        "colab_type": "text"
      },
      "source": [
        "#### Execution functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDEyyFW0XEjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Train neural network with a decreasing rule for learning rate\n",
        "#NNFactory :  function creating the architecture\n",
        "#dataSet : training data\n",
        "#activateRegularization : boolean, if true add bound penalization to dupire variance\n",
        "#hyperparameters : dictionnary containing various hyperparameters\n",
        "#modelName : name under which tensorflow model is saved\n",
        "def create_train_model_gatheral(NNFactory, \n",
        "                                dataSet, \n",
        "                                activateRegularization, \n",
        "                                hyperparameters,\n",
        "                                modelName = \"bestModel\"):\n",
        "    hidden_nodes = hyperparameters[\"nbUnits\"] \n",
        "    nbEpoch = hyperparameters[\"maxEpoch\"] \n",
        "    fixedLearningRate = (None if hyperparameters[\"FixedLearningRate\"] else hyperparameters[\"LearningRateStart\"])\n",
        "    patience = hyperparameters[\"Patience\"]\n",
        "    \n",
        "    # Go through num_iters iterations (ignoring mini-batching)\n",
        "    activateLearningDecrease = (~ hyperparameters[\"FixedLearningRate\"])\n",
        "    learningRate = hyperparameters[\"LearningRateStart\"]\n",
        "    learningRateEpoch = 0\n",
        "    finalLearningRate = hyperparameters[\"FinalLearningRate\"]\n",
        "\n",
        "    batch_size = hyperparameters[\"batchSize\"]\n",
        "\n",
        "    start = time.time()\n",
        "    # Reset the graph\n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    # Placeholders for input and output data   \n",
        "    Moneyness = tf.placeholder(tf.float32,[None,1])\n",
        "    Maturity = tf.placeholder(tf.float32,[None,1])\n",
        "    y = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='y')\n",
        "    vegaRef = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='vegaRef')\n",
        "    learningRateTensor = tf.placeholder(tf.float32,[])\n",
        "    \n",
        "    #Get scaling for strike\n",
        "    colMoneynessIndex = dataSet.columns.get_loc(\"logMoneyness\")\n",
        "    maxColFunction = scaler.data_max_[colMoneynessIndex]\n",
        "    minColFunction = scaler.data_min_[colMoneynessIndex]\n",
        "    scF = (maxColFunction - minColFunction) \n",
        "    scaleTensor = tf.constant(scF, dtype=tf.float32)\n",
        "    moneynessMinTensor = tf.constant(minColFunction, dtype=tf.float32)\n",
        "\n",
        "    price_pred_tensor = None\n",
        "    TensorList = None\n",
        "    penalizationList = None \n",
        "    formattingFunction = None\n",
        "    if activateRegularization : #Add pseudo local volatility regularisation\n",
        "        vol_pred_tensor, TensorList, penalizationList, formattingFunction = addDupireRegularisation( *NNFactory(hidden_nodes,\n",
        "                                                                                                                Moneyness,\n",
        "                                                                                                                Maturity, \n",
        "                                                                                                                scaleTensor, \n",
        "                                                                                                                moneynessMinTensor, \n",
        "                                                                                                                vegaRef, \n",
        "                                                                                                                hyperparameters) ,\n",
        "                                                                                                    vegaRef, \n",
        "                                                                                                    hyperparameters)\n",
        "    else :\n",
        "        vol_pred_tensor, TensorList, penalizationList, formattingFunction = NNFactory(hidden_nodes,\n",
        "                                                                                      Moneyness, \n",
        "                                                                                      Maturity, \n",
        "                                                                                      scaleTensor, \n",
        "                                                                                      moneynessMinTensor, \n",
        "                                                                                      vegaRef, \n",
        "                                                                                      hyperparameters)\n",
        "\n",
        "    vol_pred_tensor_sc= vol_pred_tensor\n",
        "    TensorList[0] = vol_pred_tensor_sc\n",
        "    \n",
        "    # Define a loss function\n",
        "    pointwiseError = tf.reduce_mean(tf.abs(vol_pred_tensor_sc - y) / vegaRef)\n",
        "    errors = tf.add_n([pointwiseError] + penalizationList) \n",
        "    loss = tf.log(tf.reduce_mean(errors))\n",
        "\n",
        "\n",
        "\n",
        "    # Define a train operation to minimize the loss\n",
        "    lr = learningRate\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRateTensor)\n",
        "    train = optimizer.minimize(loss)\n",
        "\n",
        "    # Initialize variables and run session\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver()\n",
        "    sess = tf.Session()\n",
        "    sess.run(init)\n",
        "    n = dataSet.shape[0]\n",
        "    scaledInput = transformCustomMinMax(dataSet, scaler)\n",
        "\n",
        "    \n",
        "    maturity = dataSet[\"Maturity\"].values.reshape(n,1)\n",
        "    loss_serie = []\n",
        "\n",
        "    def createFeedDict(batch):\n",
        "        batchSize = batch.shape[0]\n",
        "        feedDict = {Moneyness : scaledInput[\"logMoneyness\"].loc[batch.index].values.reshape(batchSize,1),\n",
        "                    Maturity : batch[\"Maturity\"].values.reshape(batchSize,1), \n",
        "                    y : batch[\"impliedTotalVariance\"].values.reshape(batchSize,1),\n",
        "                    learningRateTensor : learningRate,\n",
        "                    vegaRef : np.ones_like(batch[\"VegaRef\"].values.reshape(batchSize,1))}\n",
        "        return feedDict\n",
        "\n",
        "    #Learning rate is divided by 10 if no imporvement is observed for training loss after \"patience\" epochs\n",
        "    def updateLearningRate(iterNumber, lr, lrEpoch):\n",
        "        if not activateLearningDecrease :\n",
        "            print(\"Constant learning rate, stop training\")\n",
        "            return False, lr, lrEpoch\n",
        "        if learningRate > finalLearningRate :\n",
        "            lr *= 0.1\n",
        "            lrEpoch = iterNumber\n",
        "            saver.restore(sess, modelName)\n",
        "            print(\"Iteration : \", lrEpoch, \"new learning rate : \", lr)\n",
        "        else :\n",
        "          print(\"Last Iteration : \", lrEpoch, \"final learning rate : \", lr)\n",
        "          return False, lr, lrEpoch\n",
        "        return True, lr, lrEpoch\n",
        "    \n",
        "    epochFeedDict = createFeedDict(dataSet)\n",
        "\n",
        "    def evalBestModel():\n",
        "        if not activateLearningDecrease :\n",
        "            print(\"Learning rate : \", learningRate, \" final loss : \", min(loss_serie))\n",
        "        currentBestLoss = sess.run(loss, feed_dict=epochFeedDict)\n",
        "        currentBestPenalizations = sess.run([pointwiseError, penalizationList], feed_dict=epochFeedDict)\n",
        "        print(\"Best loss (hidden nodes: %d, iterations: %d): %.2f\" % (hidden_nodes, len(loss_serie), currentBestLoss))\n",
        "        print(\"Best Penalization : \", currentBestPenalizations)\n",
        "        return\n",
        "    \n",
        "    for i in range(nbEpoch):\n",
        "        miniBatchList = [dataSet]\n",
        "        penalizationResult = sess.run(penalizationList, feed_dict=epochFeedDict)\n",
        "        lossResult = sess.run(pointwiseError, feed_dict=epochFeedDict)\n",
        "\n",
        "        #miniBatchList = selectMiniBatchWithoutReplacement(dataSet, batch_size)\n",
        "        for k in range(len(miniBatchList)) :\n",
        "            batchFeedDict = createFeedDict(miniBatchList[k])\n",
        "            sess.run(train, feed_dict=batchFeedDict)\n",
        "        \n",
        "        \n",
        "        loss_serie.append(sess.run(loss, feed_dict=epochFeedDict))\n",
        "\n",
        "        if (len(loss_serie) < 2) or (loss_serie[-1] <= min(loss_serie)):\n",
        "          #Save model as model is improved\n",
        "          saver.save(sess, modelName)\n",
        "        if (np.isnan(loss_serie[-1]) or  #Unstable model\n",
        "            ( (i-learningRateEpoch >= patience) and (min(loss_serie[-patience:]) > min(loss_serie)) ) ) : #No improvement for training loss during the latest 100 iterations\n",
        "          continueTraining, learningRate, learningRateEpoch = updateLearningRate(i, learningRate, learningRateEpoch)\n",
        "          if continueTraining :\n",
        "            evalBestModel()\n",
        "          else :\n",
        "            break\n",
        "    saver.restore(sess, modelName)  \n",
        "    \n",
        "    evalBestModel()\n",
        "\n",
        "    evalList  = sess.run(TensorList, feed_dict=epochFeedDict)\n",
        "    \n",
        "    sess.close()\n",
        "    end = time.time()\n",
        "    print(\"Training Time : \", end - start)\n",
        "    \n",
        "    return formattingFunction(*evalList, loss_serie, dataSet) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHytoxyBXEhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Evaluate neural network without training, it restores parameters obtained from a pretrained model \n",
        "#NNFactory :  function creating the neural architecture\n",
        "#dataSet : dataset on which neural network is evaluated \n",
        "#activateRegularization : boolean, if true add bound penalization for dupire variance\n",
        "#hyperparameters : dictionnary containing various hyperparameters\n",
        "#modelName : name of tensorflow model to restore\n",
        "def create_eval_model_gatheral(NNFactory, \n",
        "                               dataSet, \n",
        "                               activateRegularization, \n",
        "                               hyperparameters,\n",
        "                               modelName = \"bestModel\"):\n",
        "    hidden_nodes = hyperparameters[\"nbUnits\"] \n",
        "    \n",
        "    # Go through num_iters iterations (ignoring mini-batching)\n",
        "    activateLearningDecrease = (~ hyperparameters[\"FixedLearningRate\"])\n",
        "    learningRate = hyperparameters[\"LearningRateStart\"]\n",
        "\n",
        "    # Reset the graph\n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    # Placeholders for input and output data   \n",
        "    Moneyness = tf.placeholder(tf.float32,[None,1])\n",
        "    Maturity = tf.placeholder(tf.float32,[None,1])\n",
        "    y = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='y')\n",
        "    vegaRef = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='vegaRef')\n",
        "    learningRateTensor = tf.placeholder(tf.float32,[])\n",
        "    \n",
        "    #Get scaling for strike\n",
        "    colMoneynessIndex = dataSet.columns.get_loc(\"logMoneyness\")\n",
        "    maxColFunction = scaler.data_max_[colMoneynessIndex]\n",
        "    minColFunction = scaler.data_min_[colMoneynessIndex]\n",
        "    scF = (maxColFunction - minColFunction) \n",
        "    scaleTensor = tf.constant(scF, dtype=tf.float32)\n",
        "    moneynessMinTensor = tf.constant(minColFunction, dtype=tf.float32)\n",
        "\n",
        "    price_pred_tensor = None\n",
        "    TensorList = None\n",
        "    penalizationList = None \n",
        "    formattingFunction = None\n",
        "    if activateRegularization : #Add pseudo local volatility regularisation\n",
        "        vol_pred_tensor, TensorList, penalizationList, formattingFunction = addDupireRegularisation( *NNFactory(hidden_nodes,\n",
        "                                                                                                                Moneyness,\n",
        "                                                                                                                Maturity, \n",
        "                                                                                                                scaleTensor, \n",
        "                                                                                                                moneynessMinTensor, \n",
        "                                                                                                                vegaRef, \n",
        "                                                                                                                hyperparameters) ,\n",
        "                                                                                                    vegaRef, \n",
        "                                                                                                    hyperparameters)\n",
        "    else :\n",
        "        vol_pred_tensor, TensorList, penalizationList, formattingFunction = NNFactory(hidden_nodes,\n",
        "                                                                                      Moneyness, \n",
        "                                                                                      Maturity, \n",
        "                                                                                      scaleTensor, \n",
        "                                                                                      moneynessMinTensor, \n",
        "                                                                                      vegaRef, \n",
        "                                                                                      hyperparameters)\n",
        "\n",
        "    vol_pred_tensor_sc= vol_pred_tensor\n",
        "    TensorList[0] = vol_pred_tensor_sc\n",
        "    \n",
        "    # Define a loss function\n",
        "    pointwiseError = tf.reduce_mean(tf.abs(vol_pred_tensor_sc - y) / vegaRef)\n",
        "    errors = tf.add_n([pointwiseError] + penalizationList)\n",
        "    loss = tf.log(tf.reduce_mean(errors))\n",
        "\n",
        "\n",
        "    # Define a train operation to minimize the loss\n",
        "    lr = learningRate \n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRateTensor)\n",
        "    train = optimizer.minimize(loss)\n",
        "\n",
        "    # Initialize variables and run session\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver()\n",
        "    sess = tf.Session()\n",
        "    sess.run(init)\n",
        "    n = dataSet.shape[0]\n",
        "    scaledInput = transformCustomMinMax(dataSet, scaler)\n",
        "\n",
        "    \n",
        "    maturity = dataSet[\"Maturity\"].values.reshape(n,1)\n",
        "    loss_serie = []\n",
        "\n",
        "    def createFeedDict(batch):\n",
        "        batchSize = batch.shape[0]\n",
        "        feedDict = {Moneyness : scaledInput[\"logMoneyness\"].loc[batch.index].values.reshape(batchSize,1),\n",
        "                    Maturity : batch[\"Maturity\"].values.reshape(batchSize,1), \n",
        "                    y : batch[\"impliedTotalVariance\"].values.reshape(batchSize,1),\n",
        "                    learningRateTensor : learningRate,\n",
        "                    vegaRef : np.ones_like(batch[\"VegaRef\"].values.reshape(batchSize,1))}\n",
        "        return feedDict\n",
        "    \n",
        "    epochFeedDict = createFeedDict(dataSet)\n",
        "\n",
        "    def evalBestModel():\n",
        "        if not activateLearningDecrease :\n",
        "            print(\"Learning rate : \", learningRate, \" final loss : \", min(loss_serie))\n",
        "        currentBestLoss = sess.run(loss, feed_dict=epochFeedDict)\n",
        "        currentBestPenalizations = sess.run([pointwiseError, penalizationList], feed_dict=epochFeedDict)\n",
        "        print(\"Best loss (hidden nodes: %d, iterations: %d): %.2f\" % (hidden_nodes, len(loss_serie), currentBestLoss))\n",
        "        print(\"Best Penalization : \", currentBestPenalizations)\n",
        "        return\n",
        "    \n",
        "    saver.restore(sess, modelName)  \n",
        "    \n",
        "    evalBestModel()\n",
        "\n",
        "    evalList  = sess.run(TensorList, feed_dict=epochFeedDict)\n",
        "    \n",
        "    sess.close()\n",
        "    \n",
        "    return formattingFunction(*evalList, [0], dataSet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yz8_0VgRrXL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evalVolLocaleGatheral(NNFactory,\n",
        "                          strikes,\n",
        "                          maturities,\n",
        "                          dataSet,\n",
        "                          hyperParameters,\n",
        "                          modelName = \"bestModel\"):\n",
        "    \n",
        "    hidden_nodes = hyperParameters[\"nbUnits\"] \n",
        "\n",
        "    # Reset the graph\n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    # Placeholders for input and output data   \n",
        "    Moneyness = tf.placeholder(tf.float32,[None,1])\n",
        "    Maturity = tf.placeholder(tf.float32,[None,1])\n",
        "    y = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='y')\n",
        "    vegaRef = tf.placeholder(shape=(None, 1), dtype=tf.float32, name='vegaRef')\n",
        "    learningRateTensor = tf.placeholder(tf.float32,[])\n",
        "    \n",
        "    #Get scaling for strike\n",
        "    colMoneynessIndex = dataSet.columns.get_loc(\"logMoneyness\")\n",
        "    maxColFunction = scaler.data_max_[colMoneynessIndex]\n",
        "    minColFunction = scaler.data_min_[colMoneynessIndex]\n",
        "    scF = (maxColFunction - minColFunction) \n",
        "    scaleTensor = tf.constant(scF, dtype=tf.float32)\n",
        "    moneynessMinTensor = tf.constant(minColFunction, dtype=tf.float32)\n",
        "\n",
        "    price_pred_tensor = None\n",
        "    TensorList = None\n",
        "    penalizationList = None \n",
        "    formattingFunction = None\n",
        "    vol_pred_tensor, TensorList, penalizationList, formattingFunction = NNFactory(hidden_nodes,\n",
        "                                                                                  Moneyness,\n",
        "                                                                                  Maturity,\n",
        "                                                                                  scaleTensor,\n",
        "                                                                                  moneynessMinTensor,\n",
        "                                                                                  vegaRef,\n",
        "                                                                                  hyperparameters)\n",
        "\n",
        "    vol_pred_tensor_sc= vol_pred_tensor\n",
        "    TensorList[0] = vol_pred_tensor_sc\n",
        "    \n",
        "    # Define a loss function\n",
        "    pointwiseError = tf.reduce_mean(tf.abs(vol_pred_tensor_sc - y) / vegaRef)\n",
        "    errors = tf.add_n([pointwiseError] + penalizationList)\n",
        "    loss = tf.log(tf.reduce_mean(errors))\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=learningRateTensor)\n",
        "    train = optimizer.minimize(loss)\n",
        "\n",
        "    # Initialize variables and run session\n",
        "    init = tf.global_variables_initializer()\n",
        "    saver = tf.train.Saver()\n",
        "    sess = tf.Session()\n",
        "    sess.run(init)\n",
        "    n = strikes.shape[0]\n",
        "    changedVar = changeOfVariable(strikes, maturities)\n",
        "\n",
        "    moneyness = np.log(changedVar[0] / S0[0]) \n",
        "    scaledMoneyness = (moneyness-minColFunction)/scF\n",
        "\n",
        "    def createFeedDict(m, t):\n",
        "        batchSize = m.shape[0]\n",
        "        feedDict = {Moneyness : np.reshape(m, (batchSize,1)), \n",
        "                    Maturity : np.reshape(t, (batchSize,1)) ,  \n",
        "                    vegaRef : np.ones((batchSize,1))}\n",
        "        return feedDict\n",
        "    \n",
        "    epochFeedDict = createFeedDict(scaledMoneyness, maturities)\n",
        "    \n",
        "    saver.restore(sess, modelName)  \n",
        "\n",
        "    evalList = sess.run(TensorList, feed_dict=epochFeedDict)\n",
        "    \n",
        "    sess.close()\n",
        "    \n",
        "    return pd.Series(evalList[1].flatten(), index = pd.MultiIndex.from_arrays([strikes, maturities], names=('Strike', 'Maturity')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1raybsuVwOU",
        "colab_type": "text"
      },
      "source": [
        "#### Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9F79bBcXEfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Dupire formula from exact derivative computation\n",
        "def dupireFormulaGatheral(HessianMoneyness, \n",
        "                          GradMoneyness,\n",
        "                          GradMaturity, \n",
        "                          totalVariance,\n",
        "                          ScaledMoneyness,\n",
        "                          scaleTensor,\n",
        "                          MoneynessMinTensor,\n",
        "                          IsTraining=True):\n",
        "  twoConstant = tf.constant(2.0)\n",
        "  oneConstant = tf.constant(1.0)\n",
        "  quarterConstant = tf.constant(0.25)\n",
        "  halfConstant = tf.constant(0.5)\n",
        "\n",
        "  moneyness = ScaledMoneyness * scaleTensor + MoneynessMinTensor \n",
        "  \n",
        "  dT = GradMaturity\n",
        "\n",
        "  dMoneyness = GradMoneyness / scaleTensor\n",
        "  dMoneynessFactor = (moneyness/totalVariance)\n",
        "  dMoneynessSquaredFactor = quarterConstant * (-quarterConstant - oneConstant/totalVariance + tf.square(dMoneynessFactor))\n",
        "\n",
        "  gMoneyness =  HessianMoneyness / tf.square(scaleTensor)\n",
        "  gMoneynessFactor = halfConstant\n",
        "  denominator = oneConstant - dMoneynessFactor * (dMoneyness) + dMoneynessSquaredFactor * tf.square(dMoneyness) + gMoneynessFactor *  gMoneyness\n",
        "  \n",
        "  gatheralVar = dT / denominator\n",
        "  #Initial weights of neural network can be random which lead to negative dupireVar\n",
        "  gatheralVolTensor = tf.sqrt(gatheralVar) \n",
        "  return gatheralVolTensor, gatheralVar, gatheralDenominator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irkHQi3rXEcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dupire formula with derivative obtained from native tensorflow algorithmic differentiation\n",
        "def rawDupireFormulaGatheral(totalVarianceTensor, \n",
        "                             scaledMoneynessTensor, \n",
        "                             maturityTensor,\n",
        "                             scaleTensor,\n",
        "                             moneynessMinTensor,\n",
        "                             IsTraining=True):\n",
        "  batchSize = tf.shape(scaledMoneynessTensor)[0]\n",
        "  twoConstant = tf.constant(2.0)\n",
        "  oneConstant = tf.constant(1.0)\n",
        "  quarterConstant = tf.constant(0.25)\n",
        "  halfConstant = tf.constant(0.5)\n",
        "\n",
        "  moneyness = scaledMoneynessTensor * scaleTensor + moneynessMinTensor \n",
        "\n",
        "  dMoneyness = tf.reshape(tf.gradients(totalVarianceTensor, scaledMoneynessTensor, name=\"dK\")[0], shape=[batchSize,-1]) / scaleTensor\n",
        "  dMoneynessFactor = (moneyness/totalVarianceTensor)\n",
        "  dMoneynessSquaredFactor = quarterConstant * (-quarterConstant - oneConstant/totalVarianceTensor + tf.square(dMoneynessFactor))\n",
        "\n",
        "  gMoneyness = tf.reshape(tf.gradients(dMoneyness, scaledMoneynessTensor, name=\"hK\")[0], shape=[batchSize,-1]) / scaleTensor\n",
        "  gMoneynessFactor = halfConstant\n",
        "\n",
        "\n",
        "  gatheralDenominator = oneConstant - dMoneynessFactor * (dMoneyness) + dMoneynessSquaredFactor * tf.square(dMoneyness) + gMoneynessFactor *  gMoneyness\n",
        "\n",
        "  dT = tf.reshape(tf.gradients(totalVarianceTensor,maturityTensor,name=\"dT\")[0], shape=[batchSize,-1])\n",
        "\n",
        "  #Initial weights of neural network can be random which lead to negative dupireVar\n",
        "  gatheralVar = dT / gatheralDenominator\n",
        "  gatheralVol = tf.sqrt(gatheralVar) \n",
        "  return  gatheralVol, dT, gMoneyness, gatheralVar, gatheralDenominator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_YNWG3IbXEaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Soft constraints for strike convexity and strike/maturity monotonicity  \n",
        "def arbitragePenalties(dT, gatheralDenominator, vegaRef, hyperparameters):\n",
        "    \n",
        "    lambdas = hyperparameters[\"lambdaSoft\"]  / tf.reduce_mean(vegaRef) \n",
        "    lowerBoundTheta = tf.constant(hyperparameters[\"lowerBoundTheta\"])\n",
        "    lowerBoundGamma = tf.constant(hyperparameters[\"lowerBoundGamma\"])\n",
        "    calendar_penalty = lambdas * tf.reduce_mean(tf.nn.relu(-dT + lowerBoundTheta ))\n",
        "    butterfly_penalty = lambdas * hyperparameters[\"lowerBoundGamma\"] * tf.reduce_mean(tf.nn.relu(-gatheralDenominator + lowerBoundGamma ))\n",
        "    \n",
        "    return [calendar_penalty, butterfly_penalty]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShAR4NOqugHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def NNArchitectureVanillaSoftGatheralAckerer(n_units,\n",
        "                                             scaledMoneynessTensor,\n",
        "                                             maturityTensor,\n",
        "                                             scaleTensor,\n",
        "                                             moneynessMinTensor,\n",
        "                                             vegaRef,\n",
        "                                             hyperparameters,\n",
        "                                             IsTraining=True):\n",
        "  \n",
        "  inputLayer = tf.concat([scaledMoneynessTensor,maturityTensor], axis=-1)\n",
        "  #First layer\n",
        "  hidden1 = unconstrainedLayer(n_units = n_units,\n",
        "                               tensor = inputLayer,\n",
        "                               isTraining=IsTraining, \n",
        "                               name = \"Hidden1\")\n",
        "  #Second layer\n",
        "  hidden2 = unconstrainedLayer(n_units = n_units,\n",
        "                               tensor = hidden1,\n",
        "                               isTraining=IsTraining, \n",
        "                               name = \"Hidden2\")\n",
        "  #Third layer\n",
        "  hidden3 = unconstrainedLayer(n_units = n_units,\n",
        "                               tensor = hidden2,\n",
        "                               isTraining=IsTraining, \n",
        "                               name = \"Hidden3\")\n",
        "  #Output layer\n",
        "  out = unconstrainedLayer(n_units = 1,\n",
        "                           tensor = hidden3,\n",
        "                           isTraining=IsTraining, \n",
        "                           name = \"Output\",\n",
        "                           activation = None)\n",
        "  #Local volatility \n",
        "  gatheralVol, theta, hK, gatheralVar, gatheralDenominator = rawDupireFormulaGatheral(out * maturityTensor,\n",
        "                                                                                      scaledMoneynessTensor,\n",
        "                                                                                      maturityTensor,\n",
        "                                                                                      scaleTensor,\n",
        "                                                                                      moneynessMinTensor,\n",
        "                                                                                      IsTraining=IsTraining)\n",
        "  #Soft constraints for no arbitrage\n",
        "  penalties = arbitragePenalties(theta, gatheralDenominator, vegaRef, hyperparameters)\n",
        "  grad_penalty = penalties[0]\n",
        "  hessian_penalty = penalties[1]\n",
        "  \n",
        "  return out, [out, gatheralVol, theta, hK, gatheralVar], [grad_penalty, hessian_penalty], evalAndFormatDupireResult"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nndy4Ym6XEX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def NNArchitectureVanillaSoftGatheral(n_units, \n",
        "                                      scaledMoneynessTensor,\n",
        "                                      maturityTensor,\n",
        "                                      scaleTensor,\n",
        "                                      moneynessMinTensor,\n",
        "                                      vegaRef,\n",
        "                                      hyperparameters,\n",
        "                                      IsTraining=True):\n",
        "  \n",
        "  inputLayer = tf.concat([scaledMoneynessTensor,maturityTensor], axis=-1)\n",
        "  #First layer\n",
        "  hidden1 = unconstrainedLayer(n_units = n_units,\n",
        "                               tensor = inputLayer,\n",
        "                               isTraining=IsTraining, \n",
        "                               name = \"Hidden1\")\n",
        "  #Second layer\n",
        "  hidden2 = unconstrainedLayer(n_units = n_units,\n",
        "                               tensor = hidden1,\n",
        "                               isTraining=IsTraining, \n",
        "                               name = \"Hidden2\")\n",
        "  #Output layer\n",
        "  out = unconstrainedLayer(n_units = 1,\n",
        "                           tensor = hidden2,\n",
        "                           isTraining=IsTraining, \n",
        "                           name = \"Output\",\n",
        "                           activation = None)\n",
        "  #Local volatility \n",
        "  gatheralVol, theta, hK, gatheralVar, gatheralDenominator = rawDupireFormulaGatheral(out * maturityTensor,\n",
        "                                                                                      scaledMoneynessTensor,\n",
        "                                                                                      maturityTensor,\n",
        "                                                                                      scaleTensor,\n",
        "                                                                                      moneynessMinTensor,\n",
        "                                                                                      IsTraining=IsTraining)\n",
        "  #Soft constraints for no arbitrage\n",
        "  penalties = arbitragePenalties(theta, gatheralDenominator, vegaRef, hyperparameters)\n",
        "  grad_penalty = penalties[0]\n",
        "  hessian_penalty = penalties[1]\n",
        "  \n",
        "  return out, [out, gatheralVol, theta, hK, gatheralVar], [grad_penalty, hessian_penalty], evalAndFormatDupireResult"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DabeiqM_V0Cp",
        "colab_type": "text"
      },
      "source": [
        "#### Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSD_zWzaFeD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hyperparameters = {}\n",
        "#penalization coefficient\n",
        "hyperparameters[\"lambdaLocVol\"] = 0.01 #100\n",
        "hyperparameters[\"lambdaSoft\"] = 10#10 #100 \n",
        "hyperparameters[\"lambdaGamma\"] = 10#10 #10000\n",
        "\n",
        "#Derivative soft constraints parameters\n",
        "hyperparameters[\"lowerBoundTheta\"] = 0.01\n",
        "hyperparameters[\"lowerBoundGamma\"] = 0.00001\n",
        "\n",
        "#Local variance parameters\n",
        "hyperparameters[\"DupireVarCap\"] = 10\n",
        "hyperparameters[\"DupireVolLowerBound\"] = 0.05\n",
        "hyperparameters[\"DupireVolUpperBound\"] = 0.40\n",
        "\n",
        "#Learning scheduler coefficient\n",
        "hyperparameters[\"LearningRateStart\"] = 0.1\n",
        "hyperparameters[\"Patience\"] = 100\n",
        "hyperparameters[\"batchSize\"] = 50\n",
        "hyperparameters[\"FinalLearningRate\"] = 1e-6\n",
        "hyperparameters[\"FixedLearningRate\"] = False\n",
        "\n",
        "#Training parameters\n",
        "hyperparameters[\"nbUnits\"] = 200 #number of units for hidden layers\n",
        "hyperparameters[\"maxEpoch\"] = 10000 #maximum number of epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1g1xVliXEWM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred4G, volLocale4G, dNN_T4G, gNN_K4G, lossSerie4G = create_train_model_gatheral(NNArchitectureVanillaSoftGatheral,\n",
        "                                                                                   scaledDataSet[scaledDataSet.index.get_level_values(\"Maturity\") > 0.01],\n",
        "                                                                                   True,\n",
        "                                                                                   hyperparameters,\n",
        "                                                                                   modelName = \"convexSoftGatheralVolModel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIQ3RWuAXETq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerie4G)\n",
        "lossSerie4G.iloc[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrIfSfqNXERH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred4G, volLocale4G, dNN_T4G, gNN_K4G, lossSerie4G = create_eval_model(NNArchitectureVanillaSoftGatheral,\n",
        "                                                                         scaledDataSet[scaledDataSet.index.get_level_values(\"Maturity\") > 0.01],\n",
        "                                                                         True,\n",
        "                                                                         hyperparameters,\n",
        "                                                                         modelName = \"convexSoftGatheralVolModel\")\n",
        "modelSummaryGatheral(y_pred4G, volLocale4G, dNN_T4G, gNN_K4G, dataSet[dataSet.index.get_level_values(\"Maturity\") > 0.01])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjAFCm8IXEKH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "volLocale4G.loc[(midS0,slice(None))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAdEtgkxEaSZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred4TestG, volLocale4TestG, dNN_T4TestG, gNN_K4TestG, lossSerie4TestG = create_eval_model(NNArchitectureVanillaSoftGatheral,\n",
        "                                                                                             scaledDataSetTest[scaledDataSetTest.index.get_level_values(\"Maturity\") > 0.01],\n",
        "                                                                                             True,\n",
        "                                                                                             hyperparameters,\n",
        "                                                                                             modelName = \"convexSoftGatheralVolModel\")\n",
        "modelSummaryGatheral(y_pred4TestG, volLocale4TestG, dNN_T4TestG, gNN_K4TestG, dataSetTest[dataSetTest.index.get_level_values(\"Maturity\") > 0.01])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK-gPhr9EbEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaledDataSet.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iI9h12E849gF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhJjhSnk5BU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C2-5GSXEbBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelSummaryGatheral(y_pred4G[y_pred4G.index.get_level_values(\"Maturity\") >= 0.19], \n",
        "                     volLocale4G[volLocale4G.index.get_level_values(\"Maturity\") >= 0.19], \n",
        "                     dNN_T4G[dNN_T4G.index.get_level_values(\"Maturity\") >= 0.19], \n",
        "                     gNN_K4G[gNN_K4G.index.get_level_values(\"Maturity\") >= 0.19], \n",
        "                     dataSet[dataSet.index.get_level_values(\"Maturity\") > 0.19],\n",
        "                     logMoneynessScale = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I65cbFSSZQ4K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "modelSummaryGatheral(y_pred4TestG, \n",
        "                     volLocale4TestG, \n",
        "                     dNN_T4TestG, \n",
        "                     gNN_K4TestG, \n",
        "                     dataSetTest[dataSetTest.index.get_level_values(\"Maturity\") > 0.01],\n",
        "                     logMoneynessScale = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3a3HFfj9Oa6O"
      },
      "source": [
        "#### Monte Carlo backtest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_ft7IMrwOa6S",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CToRLckiOa6V",
        "colab": {}
      },
      "source": [
        "def neuralVolLocaleDugas(s,t):\n",
        "  vLoc = evalVolLocaleGatheral(NNArchitectureVanillaSoftGatheral,\n",
        "                               s, t,\n",
        "                               dataSetTest,\n",
        "                               hyperparameters,\n",
        "                               modelName = \"convexSoftGatheralVolModel\")\n",
        "  return vLoc.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V01XOkK-Oa6X",
        "colab": {}
      },
      "source": [
        "volLocalInterp7 = neuralVolLocaleDugas(volLocaleGrid[0].flatten(),\n",
        "                                       volLocaleGrid[1].flatten())\n",
        "volLocalInterp7.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VlLbCyxnOa6Z",
        "colab": {}
      },
      "source": [
        "volLocalInterp8 = neuralVolLocaleDugas(dataSetTest.index.get_level_values(\"Strike\").values.flatten(),\n",
        "                                       dataSetTest.index.get_level_values(\"Maturity\").values.flatten())\n",
        "volLocalInterp8.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KVPYoApyOa6b",
        "colab": {}
      },
      "source": [
        "nnVolLocale7 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp7, x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YxtwrupUOa6c",
        "colab": {}
      },
      "source": [
        "nnVolLocale8 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp8, x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hzjHMAcgOa6e",
        "colab": {}
      },
      "source": [
        "plotSerie(volLocalInterp7,\n",
        "          Title = 'Interpolated Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c5t5-3SFOa6g",
        "colab": {}
      },
      "source": [
        "plotSerie(volLocalInterp8,\n",
        "          Title = 'Interpolated Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WLm6CG5_Oa6i",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EOxtzoy5Oa6j",
        "colab": {}
      },
      "source": [
        "mcResVolLocale7 = MonteCarloPricerVectorized(S0[0],\n",
        "                                             dataSetTest,\n",
        "                                             riskCurvespline,\n",
        "                                             divSpline,\n",
        "                                             nbPaths,\n",
        "                                             nbTimeStep,\n",
        "                                             nnVolLocale7)\n",
        "mcResVolLocale7.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "u4P1yOyHOa6l",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResVolLocale7, dataSetTest[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXn4kO1sEFGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mcResVolLocale7.to_csv(\"mcResVolLocale7.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IxMQ_VNEOa6m",
        "colab": {}
      },
      "source": [
        "mcResVolLocale8 = MonteCarloPricerVectorized(S0[0],\n",
        "                                             dataSetTest,\n",
        "                                             riskCurvespline,\n",
        "                                             divSpline,\n",
        "                                             nbPaths,\n",
        "                                             nbTimeStep,\n",
        "                                             nnVolLocale8)\n",
        "mcResVolLocale8.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dQmDMVU8Oa6o",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResVolLocale8, dataSetTest[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c9sfLc6iOa6p",
        "colab": {}
      },
      "source": [
        "mcResVolLocale8.to_csv(\"mcResVolLocale8.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q-FhHmEUAaIj"
      },
      "source": [
        "## Hyperparameter selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CDIM91VNAk-s",
        "colab": {}
      },
      "source": [
        "def selectHyperparameters(hyperparameters, parameterOfInterest, modelFactory, modelName, activateDupireReg, logGrid = True):\n",
        "    oldValue = hyperparameters[parameterOfInterest]\n",
        "    gridValue = oldValue * ( np.exp( np.log(10) * np.array([-2,-1, 0, 1, 2])) if logGrid else np.array([0.2, 0.5, 1, 2, 5]) )\n",
        "    \n",
        "    oldNbEpochs = hyperparameters[\"maxEpoch\"]\n",
        "    hyperparameters[\"maxEpoch\"] = int(oldNbEpochs / 10)\n",
        "    trainLoss = {}\n",
        "    arbitrageViolation = {}\n",
        "    for v in gridValue :\n",
        "        hyperparameters[parameterOfInterest] = int(v)\n",
        "        pred, volLoc, theta, gammaK, loss = create_train_model(modelFactory,\n",
        "                                                               scaledDataSet,\n",
        "                                                               activateDupireReg,\n",
        "                                                               hyperparameters,\n",
        "                                                               modelName = modelName)\n",
        "        nbArbitrageViolation = np.sum((theta <= 0)) + np.sum((gammaK <= 0))\n",
        "        trainLoss[v] = min(loss)\n",
        "        arbitrageViolation[v] = nbArbitrageViolation\n",
        "        print()\n",
        "        print()\n",
        "\n",
        "    hyperparameters[\"maxEpoch\"] = oldNbEpochs\n",
        "    hyperparameters[parameterOfInterest] = oldValue\n",
        "    # Plot curves\n",
        "    \n",
        "    fig, ax1 = plt.subplots()\n",
        "    if logGrid :\n",
        "        plt.xscale('symlog')\n",
        "    \n",
        "    color = 'tab:red'\n",
        "    ax1.set_xlabel('Value')\n",
        "    ax1.set_ylabel('Loss', color=color)\n",
        "    ax1.plot(pd.Series(trainLoss), color=color)\n",
        "    ax1.tick_params(axis='y', labelcolor=color)\n",
        "    \n",
        "    ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
        "    \n",
        "    color = 'tab:blue'\n",
        "    ax2.set_ylabel('Arbitrage violation', color=color)  # we already handled the x-label with ax1\n",
        "    ax2.plot(pd.Series(arbitrageViolation), color=color)\n",
        "    ax2.tick_params(axis='y', labelcolor=color)\n",
        "    \n",
        "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
        "    plt.show()\n",
        "    \n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "REnyuzq2Ak5V",
        "colab": {}
      },
      "source": [
        "def selectHyperparametersRandom(hyperparameters, \n",
        "                                parametersOfInterest, \n",
        "                                modelFactory, \n",
        "                                modelName, \n",
        "                                activateDupireReg, \n",
        "                                nbAttempts,\n",
        "                                logGrid = True):\n",
        "    oldValue = {} \n",
        "    for k in parametersOfInterest :\n",
        "        oldValue[k] = hyperparameters[k]\n",
        "    \n",
        "    gridValue = np.exp( np.log(10) * np.array([-2,-1, 0, 1, 2])) if logGrid else np.array([0.2, 0.5, 1, 2, 5]) \n",
        "    \n",
        "    oldNbEpochs = hyperparameters[\"maxEpoch\"]\n",
        "    hyperparameters[\"maxEpoch\"] = int(oldNbEpochs / 10)\n",
        "    trainLoss = {}\n",
        "    arbitrageViolation = {}\n",
        "    nbTry = nbAttempts\n",
        "    for v in range(nbTry) :\n",
        "        combination = np.random.randint(5, size = len(parametersOfInterest) )\n",
        "        for p in range(len(parametersOfInterest)):\n",
        "            hyperparameters[parametersOfInterest[p]] = oldValue[parametersOfInterest[p]] * gridValue[int(combination[p])]\n",
        "            print(parametersOfInterest[p] , \" : \", hyperparameters[parametersOfInterest[p]])\n",
        "        pred, volLoc, theta, gammaK, loss = create_train_model(modelFactory,\n",
        "                                                               scaledDataSet,\n",
        "                                                               activateDupireReg,\n",
        "                                                               hyperparameters,\n",
        "                                                               modelName = modelName)\n",
        "        nbArbitrageViolation = np.sum((theta <= 0)) + np.sum((gammaK <= 0))\n",
        "        print(\"loss : \", min(loss))\n",
        "        print(\"nbArbitrageViolation : \", nbArbitrageViolation)\n",
        "        print()\n",
        "        print()\n",
        "        print()\n",
        "\n",
        "    hyperparameters[\"maxEpoch\"] = oldNbEpochs\n",
        "    for k in parametersOfInterest :\n",
        "        hyperparameters[k] = oldValue[k]\n",
        "    \n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0P-fv_wMAk2y",
        "colab": {}
      },
      "source": [
        "selectHyperparametersRandom(hyperparameters,\n",
        "                            [\"lambdaLocVol\",\"lambdaSoft\",\"lambdaGamma\"],\n",
        "                            NNArchitectureConstrainedRawDupire,\n",
        "                            \"hyperParameters\",\n",
        "                            True, \n",
        "                            100,\n",
        "                            logGrid = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eWz86KbKCqa-",
        "colab": {}
      },
      "source": [
        "\n",
        "hyperparameters[\"lambdaLocVol\"] = 100\n",
        "hyperparameters[\"lambdaSoft\"] = 100 \n",
        "hyperparameters[\"lambdaGamma\"] = 10000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1jSAmMeZ6V15",
        "colab": {}
      },
      "source": [
        "selectHyperparameters(hyperparameters, \n",
        "                      \"lambdaLocVol\", \n",
        "                      NNArchitectureVanillaSoftDupire, \n",
        "                      \"hyperParameters\", \n",
        "                      True, \n",
        "                      logGrid = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dANDwRU6DEom",
        "colab": {}
      },
      "source": [
        "selectHyperparameters(hyperparameters, \n",
        "                      \"DupireVarCap\", \n",
        "                      NNArchitectureConstrainedRawDupire, \n",
        "                      \"hyperParameters\", \n",
        "                      True, \n",
        "                      logGrid = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8-j6a1jpFt1_",
        "colab": {}
      },
      "source": [
        "selectHyperparameters(hyperparameters, \n",
        "                      \"lambdaLocVol\", \n",
        "                      NNArchitectureUnconstrainedDupire, \n",
        "                      \"hyperParameters\", \n",
        "                      True, \n",
        "                      logGrid = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_thp08oSGj6m",
        "colab": {}
      },
      "source": [
        "hyperparameters[\"lambdaLocVol\"] = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Smxq-gfiGGV8",
        "colab": {}
      },
      "source": [
        "selectHyperparameters(hyperparameters, \n",
        "                      \"lambdaLocVol\", \n",
        "                      NNArchitectureConstrainedRawDupire, \n",
        "                      \"hyperParameters\", \n",
        "                      True, \n",
        "                      logGrid = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GPSNSuGIGsMC",
        "colab": {}
      },
      "source": [
        "hyperparameters[\"nbUnits\"] = 40"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UQb2l-ZqIKyC",
        "colab": {}
      },
      "source": [
        "selectHyperparameters(hyperparameters, \n",
        "                      \"nbUnits\", \n",
        "                      NNArchitectureVanillaSoftDupire, \n",
        "                      \"hyperParameters\", \n",
        "                      True, \n",
        "                      logGrid = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_W3QYBDuMSu-",
        "colab": {}
      },
      "source": [
        "hyperparameters[\"nbUnits\"] = 200"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q-lYydHFB8G1",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPQe7i1bQys_",
        "colab_type": "text"
      },
      "source": [
        "## Dugas network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_vQgZNRQ2Bv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hyperparameters = {}\n",
        "#penalization coefficient\n",
        "hyperparameters[\"lambdaLocVol\"] = 1000\n",
        "hyperparameters[\"lambdaSoft\"] = 100 \n",
        "hyperparameters[\"lambdaGamma\"] = 10000\n",
        "\n",
        "#Derivative soft constraints parameters\n",
        "hyperparameters[\"lowerBoundTheta\"] = 0.01\n",
        "hyperparameters[\"lowerBoundGamma\"] = 0.00001\n",
        "\n",
        "#Local variance parameters\n",
        "hyperparameters[\"DupireVarCap\"] = 10\n",
        "hyperparameters[\"DupireVolLowerBound\"] = 0.05\n",
        "hyperparameters[\"DupireVolUpperBound\"] = 0.40\n",
        "\n",
        "#Learning scheduler coefficient\n",
        "hyperparameters[\"LearningRateStart\"] = 0.1\n",
        "hyperparameters[\"Patience\"] = 100\n",
        "hyperparameters[\"batchSize\"] = 50\n",
        "hyperparameters[\"FinalLearningRate\"] = 1e-6\n",
        "hyperparameters[\"FixedLearningRate\"] = False\n",
        "\n",
        "#Training parameters\n",
        "hyperparameters[\"nbUnits\"] = 200 #number of units for hidden layers\n",
        "hyperparameters[\"maxEpoch\"] = 10000 #maximum number of epochs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vdVwnXiqRDTM",
        "colab": {}
      },
      "source": [
        "def convexDugasLayer(n_units,  tensor, isTraining, name):\n",
        "  with tf.name_scope(name):\n",
        "    nbInputFeatures = tensor.get_shape().as_list()[1]\n",
        "    bias = tf.Variable(initial_value = tf.zeros_initializer()([n_units], dtype=tf.float32), \n",
        "                       trainable = True, \n",
        "                       shape = [n_units],\n",
        "                       dtype = tf.float32, \n",
        "                       name = name + \"Bias\")\n",
        "    weights = tf.exp(tf.Variable(initial_value = tf.keras.initializers.glorot_normal()([nbInputFeatures, n_units], dtype=tf.float32), \n",
        "                                 trainable = True, \n",
        "                                 shape = [nbInputFeatures, n_units],\n",
        "                                 dtype = tf.float32, \n",
        "                                 name = name + \"Weights\"))\n",
        "    layer = tf.matmul(tensor, weights) + bias\n",
        "    return K.softplus(layer)\n",
        "\n",
        "def monotonicDugasLayer(n_units,  tensor, isTraining, name):\n",
        "  with tf.name_scope(name):\n",
        "    nbInputFeatures = tensor.get_shape().as_list()[1]\n",
        "    bias = tf.Variable(initial_value = tf.zeros_initializer()([n_units], dtype=tf.float32), \n",
        "                       trainable = True, \n",
        "                       shape = [n_units],\n",
        "                       dtype = tf.float32, \n",
        "                       name = name + \"Bias\")\n",
        "    weights = tf.exp(tf.Variable(initial_value = tf.keras.initializers.glorot_normal()([nbInputFeatures, n_units], dtype=tf.float32), \n",
        "                                 trainable = True, \n",
        "                                 shape = [nbInputFeatures, n_units],\n",
        "                                 dtype = tf.float32, \n",
        "                                 name = name + \"Weights\"))\n",
        "    layer = tf.matmul(tensor, weights) + bias\n",
        "    return K.sigmoid(layer)\n",
        "\n",
        "def convexDugasOutputLayer(tensor, isTraining, name):\n",
        "  with tf.name_scope(name):\n",
        "    nbInputFeatures = tensor.get_shape().as_list()[1]\n",
        "    bias = tf.exp(tf.Variable(initial_value = tf.zeros_initializer()([], dtype=tf.float32), \n",
        "                              shape = [],\n",
        "                              trainable = True, \n",
        "                              dtype = tf.float32, \n",
        "                              name = name + \"Bias\"))\n",
        "    weights = tf.exp(tf.Variable(initial_value = tf.keras.initializers.glorot_normal()([nbInputFeatures, 1], dtype=tf.float32), \n",
        "                                 shape = [nbInputFeatures, 1],\n",
        "                                 trainable = True, \n",
        "                                 dtype = tf.float32, \n",
        "                                 name = name + \"Weights\"))\n",
        "    layer = tf.matmul(tensor, weights) + bias\n",
        "    return layer\n",
        "\n",
        "\n",
        "\n",
        "def NNArchitectureHardConstrainedDugas(n_units, strikeTensor, \n",
        "                                       maturityTensor,\n",
        "                                       scaleTensor,\n",
        "                                       strikeMinTensor, \n",
        "                                       vegaRef,\n",
        "                                       hyperparameters,\n",
        "                                       IsTraining=True):\n",
        "  #First layer\n",
        "  hidden1S = convexDugasLayer(n_units = n_units,\n",
        "                              tensor = strikeTensor,\n",
        "                              isTraining=IsTraining,\n",
        "                              name = \"Hidden1S\")\n",
        "  \n",
        "  hidden1M = monotonicDugasLayer(n_units = n_units,\n",
        "                                 tensor = maturityTensor,\n",
        "                                 isTraining = IsTraining,\n",
        "                                 name = \"Hidden1M\")\n",
        "  \n",
        "  hidden1 = hidden1S * hidden1M\n",
        "  \n",
        "  #Second layer and output layer\n",
        "  out= convexDugasOutputLayer(tensor = hidden1,\n",
        "                              isTraining = IsTraining,\n",
        "                              name = \"Output\")\n",
        "  #Local volatility\n",
        "  dupireVol, theta, hK, dupireVar = rawDupireFormula(out, strikeTensor,\n",
        "                                                     maturityTensor,\n",
        "                                                     scaleTensor,\n",
        "                                                     strikeMinTensor,\n",
        "                                                     IsTraining=IsTraining)\n",
        "  \n",
        "  return out, [out, dupireVol, theta, hK, dupireVar], [], evalAndFormatDupireResult"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uD6MWTroRDTO",
        "colab": {}
      },
      "source": [
        "y_predDugas, volLocaleDugas, dNN_TDugas, gNN_KDugas, lossSerieDugas = create_train_model(NNArchitectureHardConstrainedDugas,\n",
        "                                                                                         scaledDataSet,\n",
        "                                                                                         True,\n",
        "                                                                                         hyperparameters,\n",
        "                                                                                         modelName = \"convexHardDugasVolModel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WyLhpBzXRDTQ",
        "colab": {}
      },
      "source": [
        "plotEpochLoss(lossSerieDugas)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_wyRPp5kRDTS",
        "colab": {}
      },
      "source": [
        "lossSerieDugas.iloc[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RlUkuCZKRDTT",
        "colab": {}
      },
      "source": [
        "y_predDugas, volLocaleDugas, dNN_TDugas, gNN_KDugas, lossSerieDugas = create_eval_model(NNArchitectureHardConstrainedDugas,\n",
        "                                                                                        scaledDataSet,\n",
        "                                                                                        True,\n",
        "                                                                                        hyperparameters,\n",
        "                                                                                        modelName = \"convexHardDugasVolModel\")\n",
        "modelSummary(y_predDugas, volLocaleDugas, dNN_TDugas, gNN_KDugas, dataSet)\n",
        "impVDugas = plotImpliedVol(y_predDugas, dataSet[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gSGNfIEuRDTV",
        "colab": {}
      },
      "source": [
        "volLocaleDugas.loc[(midS0,slice(None))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SbAyorEaRDTW",
        "colab": {}
      },
      "source": [
        "y_predDugasTest, volLocaleDugasTest, dNN_TDugasTest, gNN_KDugasTest, lossSerie6Test = create_eval_model(NNArchitectureHardConstrainedDugas, \n",
        "                                                                                        scaledDataSetTest, \n",
        "                                                                                        True, \n",
        "                                                                                        hyperparameters,\n",
        "                                                                                        modelName = \"convexHardDugasVolModel\")\n",
        "modelSummary(y_predDugasTest, volLocaleDugasTest, dNN_TDugasTest, gNN_KDugasTest, dataSetTest)\n",
        "impVDugasTest = plotImpliedVol(y_predDugasTest, dataSetTest[\"ImpliedVol\"], rIntegralSpline=riskFreeIntegral, qIntegralSpline=divSpreadIntegral)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_YGqIPnFRDTY",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-rXuxceMS-M",
        "colab_type": "text"
      },
      "source": [
        "#### Monte Carlo backtest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t01R9oTaMk81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0vGg9QuXMlVT",
        "colab": {}
      },
      "source": [
        "def neuralVolLocaleDugas(s,t):\n",
        "  vLoc = evalVolLocale(NNArchitectureHardConstrainedDugas,\n",
        "                       s, t,\n",
        "                       dataSet,\n",
        "                       hyperparameters,\n",
        "                       modelName = \"convexHardDugasVolModel\")\n",
        "  return vLoc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZZNSCOHhMlVX",
        "colab": {}
      },
      "source": [
        "volLocalInterp9 = neuralVolLocaleDugas(volLocaleGrid[0].flatten(),\n",
        "                                       volLocaleGrid[1].flatten())\n",
        "volLocalInterp9.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HrCtvaDmMlVa",
        "colab": {}
      },
      "source": [
        "volLocalInterp10 = neuralVolLocaleDugas(dataSetTest.index.get_level_values(\"Strike\").values.flatten(),\n",
        "                                        dataSetTest.index.get_level_values(\"Maturity\").values.flatten())\n",
        "volLocalInterp10.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4cUpV8aqMlVd",
        "colab": {}
      },
      "source": [
        "nnVolLocale9 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp9, x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gXcE78sEMlVf",
        "colab": {}
      },
      "source": [
        "nnVolLocale10 = lambda x,y : interpolatedMCLocalVolatility(volLocalInterp10, x, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-16ZM3CHMlVi",
        "colab": {}
      },
      "source": [
        "plotSerie(volLocalInterp9,\n",
        "          Title = 'Interpolated Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4bQyPsd0MlVk",
        "colab": {}
      },
      "source": [
        "plotSerie(volLocalInterp10,\n",
        "          Title = 'Interpolated Local Volatility Surface',\n",
        "          az=30,\n",
        "          yMin=0.0*S0,\n",
        "          yMax=2.0*S0, \n",
        "          zAsPercent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "d3TuWcvHMlVm",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-QauSPwZICoJ",
        "colab": {}
      },
      "source": [
        "mcResVolLocale9 = MonteCarloPricerVectorized(S0[0],\n",
        "                                             dataSet,\n",
        "                                             riskCurvespline,\n",
        "                                             divSpline,\n",
        "                                             nbPaths,\n",
        "                                             nbTimeStep,\n",
        "                                             nnVolLocale9)\n",
        "mcResVolLocale9.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bbcC3XNXICoN",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResVolLocale9, dataSet[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YvbIoQXWICoP",
        "colab": {}
      },
      "source": [
        "mcResVolLocale10 = MonteCarloPricerVectorized(S0[0],\n",
        "                                             dataSet,\n",
        "                                             riskCurvespline,\n",
        "                                             divSpline,\n",
        "                                             nbPaths,\n",
        "                                             nbTimeStep,\n",
        "                                             nnVolLocale10)\n",
        "mcResVolLocale10.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "S6QHWuSuICoS",
        "colab": {}
      },
      "source": [
        "predictionDiagnosis(mcResVolLocale10, dataSet[\"Price\"], \" Price \", yMin=4100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O9RboM1EICoU",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQmQJZjxuYD7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BfNVpgxc51kJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDsTnEfiorBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}